{"title":"[P] EvoTorch 0.4.0 dropped with GPU-accelerated implementations of CMA-ES, MAP-Elites and NSGA-II.","description":"Find the release notes here:\n\n[https://github.com/nnaisense/evotorch/releases/tag/v0.4.0](https://github.com/nnaisense/evotorch/releases/tag/v0.4.0)\n\nA big highlight is how fast these implementations are! I genuinely believe GPU-acceleration is the future of Evolutionary algorithms, and EvoTorch and its integration into the PyTorch ecosystem is a fantastic enabler for this.   \n\nTo demonstrate the raw speed provided by the new release, I compared EvoTorch's CMA-ES implementation to that provided by the popular pycma package on the 80-dimensional Rastrigin problem and tracked the run-time:\n\n[Performance was measured over 50 runs on the 80-dimensional Rastrigin problem](https://preview.redd.it/w3qwefgr6dea1.jpg?width=458&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e056e6aa42e07b050ea2a187ae3b07de2b789f6f)\n\nThe crazy thing to note is that when we switch to GPU (Tesla V100), we can efficiently run CMA-ES with population sizes going into 100k+!","link":"https://www.reddit.com/r/MachineLearning/comments/10lot3v/p_evotorch_040_dropped_with_gpuaccelerated/","created":"2023-01-26","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":1}}
{"title":"[R] Blogpost on comparing Chatbots like ChatGPT, LaMDA, Sparrow, BlenderBot 3, and Claude","description":"[https://huggingface.co/blog/dialog-agents](https://huggingface.co/blog/dialog-agents) breaks down the techniques behind ChatGPT -- instruction fine-tuning, supervised fine-tuning, chain-of-thought, read teaming, and more.\n\nhttps://preview.redd.it/fv16fsemd9ea1.png?width=889&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=bc723c4cc71ec0457bb1c2ac07f5fa6e4a3a4ccf","link":"https://www.reddit.com/r/MachineLearning/comments/10l9tet/r_blogpost_on_comparing_chatbots_like_chatgpt/","created":"2023-01-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":4}}
{"title":"Few questions about scalability of chatGPT [D]","description":"I have two questions about chatGPT. I don't come from a machine learning background. I am just a programmer. So bear with me if they sound a bit dumb.\n\nI was checking about chatGPT a bit the last week. I went through their papers and also tried out a fine tuning by myself by creating some fictional world and giving it some examples. \n\nThe first thing I wondered is what is very special about the model than the large data and parameter set it has, that other competitors can't do. I ask this because I have seen a lot of \"google killer\" discussions in some places. From what I understood from their papers I thought it is something another company with the computing power and the filtered data can have up and running in few months. I see their advantage in rolling out to the public because with feedbacks from actual users all over the world it can potentially be retrained.\n\nThe second thing I wondered is its scalability. It feels to me that it is a very big challenge to keep it scalable in the future. Currently getting a long text out of it is kind of painful because it has to continuously generate. I think it is continuously calculating with the huge parameter set it has. I wonder also about new trends, if it needs to be retrained. I also used it for a fine tuning, where I created a fictional world with its own law and rules and the fine tuning took hours in the queue - so is it creating separate parameters for my case? that would be a lot considering how much parameter set they have.","link":"https://www.reddit.com/r/MachineLearning/comments/10lp3g4/few_questions_about_scalability_of_chatgpt_d/","created":"2023-01-26","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":11}}
{"title":"Are there any projects working at an open source version of Constitutional AI? [D]","description":"I'm looking into projects which augment the RLHF training approach of chatGPT with explicit rules, such as in [https://paperswithcode.com/paper/constitutional-ai-harmlessness-from-ai](https://paperswithcode.com/paper/constitutional-ai-harmlessness-from-ai). \n\nIdeally there would be both rules and priority levels between the rules, similarly to the Asimov laws of robotics. \n\nThe Open-Assistant project ([https://github.com/LAION-AI/Open-Assistant](https://github.com/LAION-AI/Open-Assistant)) captures the spirit, but it is looking to replicate chatGPT at the moment.","link":"https://www.reddit.com/r/MachineLearning/comments/10lui3i/are_there_any_projects_working_at_an_open_source/","created":"2023-01-26","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[D] Quantitative measure for smoothness of NLP autoencoder latent space","description":"I would like to measure the smoothness of an NLP-autoencoder's latent space. The idea is to sample two Gaussian vectors v1 and v2 in the latent space of the AE, and generate N-1 points between them like so:\n\nvi = v1 + (v2 - v1) / (N * i)\n\nMy idea is to then decode these vectors and measure the BLEU score between d(vi) and d(vi+1) for all N-2 comparisons.\n\nIs this idea reasonable, do you have a better one? Is there a technique from AEs with images that can be useful here?","link":"https://www.reddit.com/r/MachineLearning/comments/10ltyki/d_quantitative_measure_for_smoothness_of_nlp/","created":"2023-01-26","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[D] What are some of your favorite ML research posters?","description":"And what are your own best practices when creating one (e.g. adding a QR code that links to the GitHub project or paper PDF)?","link":"https://www.reddit.com/r/MachineLearning/comments/10lsirk/d_what_are_some_of_your_favorite_ml_research/","created":"2023-01-26","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[P] Diffusion models best practices","description":"I'm about to start an experimental project that involves training a denoising diffusion model on the medical data (small dataset).\n\nCould you please share useful resources, tips, tricks and heuristics for dealing with diffusion models?","link":"https://www.reddit.com/r/MachineLearning/comments/10leaq9/p_diffusion_models_best_practices/","created":"2023-01-26","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":12}}
{"title":"[D] Fastest and most accurate model for casing","description":"What is the state of the art regarding freely available casing models, i.e. DNNs, that try to restore the original casing of a text with uniform (either lowercase or capital letters) casing? I value both speed and accuracy, as I have to process a large corpus of text.","link":"https://www.reddit.com/r/MachineLearning/comments/10lqd34/d_fastest_and_most_accurate_model_for_casing/","created":"2023-01-26","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[D] Self-Supervised Contrastive Approaches that don\u2019t use large batch size.","description":"This thread is dedicated to exploring the various techniques used in self-supervised contrastive learning that utilize standard batch sizes. I am seeking information on the current methods in this field, specifically those that do not rely on large batch sizes.\n\nI am familiar with the SimSiam paper published by META research, which utilizes 256 batch size for 8-GPUs. However, for individuals with limited resources such as myself, access to a large number of GPUs may not be feasible. As a result, I am interested in learning about other methods that can be used with smaller batch sizes and a single GPU, such as those that would be suitable for training on 1024x1024 input images.\n\nAdditionally, I am curious about any more efficient architectures that have been developed in this field. This includes, but is not limited to, techniques used in natural language processing that may have applications in other areas of artificial intelligence.\n\n\\*\\*\\*posted the same question in PyTorch forums, reposting here for wider reach.","link":"https://www.reddit.com/r/MachineLearning/comments/10ky2oh/d_selfsupervised_contrastive_approaches_that_dont/","created":"2023-01-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":25}}
{"title":"Machine learning and black box numerical solver[D]","description":"Anybody know some methods and techniques for integrating a numerical solver with the neural network .. how do you calculate the gradients of the solver when you don\u2019t know the details of such solver- black box solver.","link":"https://www.reddit.com/r/MachineLearning/comments/10lka00/machine_learning_and_black_box_numerical_solverd/","created":"2023-01-26","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":5}}
{"title":"[R] Tsetlin Machine in Medical Research - Striking Differences Between Tsetlin Machine Interpretability and Deep Learning Attention","description":"&amp;#x200B;\n\n[Tsetlin machine interpretability vs deep learning attention.](https://preview.redd.it/vgcfhj7x86ea1.png?width=2074&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=07eae3a8ae5f4be6aef020b82fb28dedb4016cc5)\n\nResearchers at West China Hospital, Sichuan University, NORCE, and UiA have developed a Tsetlin machine-based architecture for premature ventricular contraction identification by analyzing long-term ECG signals. The experiments show that the Tsetlin machine is capable of producing human-interpretable rules, consistent with the clinical standard and medical knowledge. Simultaneously, the accuracy was comparable with deep CNN-based models.\n\nPaper: [https://arxiv.org/abs/2301.10181](https://arxiv.org/abs/2301.10181)","link":"https://www.reddit.com/r/MachineLearning/comments/10kw6ob/r_tsetlin_machine_in_medical_research_striking/","created":"2023-01-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":6}}
{"title":"[D] Efficient retrieval of research information for graduate research","description":"I have lot of notes about research papers in a particular directory and the number of files has started to become larger than what I can remember off the top of my head. It will continue to keep growing and I have begun to wonder the most efficient way to retrieve the information. I could use ripgrep and regular expressions to find the notes efficiently, but I imagine that if the database is very huge and I don't have the correct regular expression in use, then I might not retrieve the correct files.\n\nInspired by chatGPT, I was impressed at how it presents info from the internet and speeds up my time for finding information even when I do not know the correct keywords. I figured a NLP model primarily trained on my database would be an easier task and I was wondering if someone had already created something like this as open source or how would they go about it?","link":"https://www.reddit.com/r/MachineLearning/comments/10l1a5s/d_efficient_retrieval_of_research_information_for/","created":"2023-01-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":9}}
{"title":"[D] Publication Resume","description":"If we submit a publication to ICML and it is under anonymous review, can I list the title and authors on my resume which will be on my personal webpage?","link":"https://www.reddit.com/r/MachineLearning/comments/10l9zly/d_publication_resume/","created":"2023-01-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":6}}
{"title":"[D] Alphatensor benchmark code in Colab","description":"Hello everybody\n\nI was wondering if anybody tried to run the main factorisation code [https://github.com/deepmind/alphatensor/blob/main/benchmarking/factorizations.py](https://github.com/deepmind/alphatensor/blob/main/benchmarking/factorizations.py) from alpha tensor on Google Colab, with Colab's GPUs ( Tesla T4).\n\nI know that Tesla T4 is not as the same as the V100 used in Deep Mind's paper, however, I can see that the tensor formulation for the matrix multiplication is highly inefficient, compared to standard JAX matrix multiplication.\n\nAny suggestion where am I wrong?","link":"https://www.reddit.com/r/MachineLearning/comments/10lc538/d_alphatensor_benchmark_code_in_colab/","created":"2023-01-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[R] Best service for scientific paper correction","description":"Hello,\nAnyone ever used a paper revision service and can recommend one ?\n\nI\u2019m publishing my first paper next month and I want to have feedback from an expert on this domain.\n\nThanks !","link":"https://www.reddit.com/r/MachineLearning/comments/10kzfwm/r_best_service_for_scientific_paper_correction/","created":"2023-01-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":7}}
{"title":"[R] INSTRUCTOR One Embedder , Any Task: Instruction-Finetuned Text Embeddings Paper Explanation and Collab Demo","description":"In this video  I   explain about INSTRUCTOR, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor achieves sota on 70 diverse embedding tasks! I also show a google collab demo of instructor\n\nhttps://youtu.be/vg38cq3KJ6M","link":"https://www.reddit.com/r/MachineLearning/comments/10ksetd/r_instructor_one_embedder_any_task/","created":"2023-01-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[D] ICLR now has a track with race-based (and more) acceptance criteria","description":"ICLR introduced a [Tiny Paper Track](https://iclr.cc/Conferences/2023/CallForTinyPapers) for shorter contributions, up to 2 pages. Sounds like a nice idea, right?\n\nBut to keep things interesting, since it's organized by the DEI initiative, there are restrictions as to who can author the submitted papers. \n\nAccording to the official guidelines:\n&gt; Each Tiny Paper needs its first or last author to qualify as an underrepresented minority (URM). Authors don't have to reveal how they qualify, and may just self-identify that they qualify.\n\n&gt; Our working definition of an URM is someone whose age, gender, sexual orientation, racial or ethnic makeup is from one or more of the following: \n\n&gt; Age: outside the range of 30-50 years\n\n&gt; Gender: does not identify as male\n\n&gt; Sexual orientation: does not identify as heterosexual\n\n&gt; Geographical: not located in North America, Western Europe and UK, or East Asia\n\n&gt; Race: non-White\n\n&gt; In addition, underprivileged researchers and first-time submitters also qualify:\n\n&gt; Underprivileged: not affiliated with a funded organization or team whose primary goal is research\n&gt; First-time submitters: have never submitted to ICLR or similar conferences\n\n\nSo effectively, someone could submit a paper, and literally have it rejected because they're e.g. white or male. \n\nIs this really the way the field should go? I feel like this is something that should never have passed any ethics board, but clearly the organizers disagree.","link":"https://www.reddit.com/r/MachineLearning/comments/10k31w3/d_iclr_now_has_a_track_with_racebased_and_more/","created":"2023-01-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":294}}
{"title":"[D] CVPR Reviews are out","description":"Don't post about your cool papers or you'll get rejected lol","link":"https://www.reddit.com/r/MachineLearning/comments/10kbey9/d_cvpr_reviews_are_out/","created":"2023-01-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":63}}
{"title":"[D] Accurate data or more data?","description":"If you are building a model and had the choice, would you prefer more accurate (~99%) but less data or a lot more data but less accurate (~90%)?","link":"https://www.reddit.com/r/MachineLearning/comments/10l0nya/d_accurate_data_or_more_data/","created":"2023-01-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":4}}
{"title":"Can an AI model licensed under the BigScience RAIL License v1.0 such as BLOOM be used in a program that is useful for any domain? [D]","description":"Example: the AI model [BLOOM](https://en.wikipedia.org/wiki/BLOOM_(language_model)) is licensed under the [BigScience RAIL License v1.0](https://huggingface.co/spaces/bigscience/license). The BigScience RAIL License v1.0 forbids that some types of usages:\n\n&gt; You agree not to use the Model or Derivatives of the Model:\n&gt;\n&gt;  [...]\n&gt;\n&gt; - To provide medical advice and medical results interpretation;\n&gt; - To generate or disseminate information for the purpose to be used for administration of justice, law enforcement, immigration or asylum processes, such as predicting an individual will commit fraud/crime commitment (e.g. by text profiling, drawing causal relationships between assertions made in documents, indiscriminate and arbitrarily-targeted use).\n\nAm I allowed to use BLOOM in a program that is useful for any domain (e.g., a program to summarize or paraphrase some text, or perform question-answer on a text, or generate questions and their answers based on the text)? \n\nSince people could use the program for any domain, they could technically, for example, use the program to summarize a medical report or generate questions and their answers based on some asylum process to distribute to potential applicants.","link":"https://www.reddit.com/r/MachineLearning/comments/10kl8y9/can_an_ai_model_licensed_under_the_bigscience/","created":"2023-01-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":3}}
{"title":"[R] Easiest way to train RNN's in MATLAB or Julia?","description":" I work as as a researcher and am kind of new to neural networks. I have an RNN (1e4 x 1e4 network) that I would like to train in either MATLAB or Julia.\n\nOne option I considered is writing my own code for Hessian-free optimization, but the implementational details are really, really hard to figure out.\n\nI am aware there is a Theano or TF implementation of HFO but I I am primarily interested in having the code in MATLAB/Julia.\n\nAlso, are there better/alternative techniques than Hessian-free optimization for training RNN's ?","link":"https://www.reddit.com/r/MachineLearning/comments/10kmc7n/r_easiest_way_to_train_rnns_in_matlab_or_julia/","created":"2023-01-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":13}}
{"title":"[P] tsdownsample: extremely fast time series downsampling for visualization","description":"tsdownsample brings highly optimized time series downsampling to Python! The downsampling algorithms are written and optimized in Rust, which are made available in Python through the use of PyO3 bindings.\n\nCode: [https://github.com/predict-idlab/tsdownsample](https://github.com/predict-idlab/tsdownsample)\n\n# Features\n\n* **Fast**: leverages the optimized [argminmax crate](https://github.com/jvdd/argminmax) which is SIMD accelerated with runtime feature detection (matches or even outperforms numpy's speed)\n* **Efficient**: operates on views of the data, eliminating the need for unnecessary data copies and avoiding the creation of intermediate data structures\n* **Flexible**: supports a wide range of datatypes, including [f16 which is 200-300x faster than numpy's implementation](https://github.com/jvdd/argminmax/pull/1).\n* **Easy to use**: simple and flexible API\n\n# Installation\n\n    pip install tsdownsample\n\n# Example\n\nWhen using multi-threading, tsdownsample can downsample 500 MILLION datapoints (f32) in 0.05s! \u2b07\ufe0f\n\nhttps://preview.redd.it/frqh8o2bezda1.png?width=1650&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=0674cd6b681210d70d8f2e7a81b12415c880ea50\n\n&amp;#x200B;\n\nI would love to hear your feedback on this!","link":"https://www.reddit.com/r/MachineLearning/comments/10k48bz/p_tsdownsample_extremely_fast_time_series/","created":"2023-01-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":3}}
{"title":"[P] New textbook: Understanding Deep Learning","description":"I've been writing a new textbook on deep learning for publication by MIT Press late this year.  The current draft is at:\n\n[https://udlbook.github.io/udlbook/](https://udlbook.github.io/udlbook/)\n\nIt contains a lot more detail than most similar textbooks and will likely be useful for all practitioners, people learning about this subject, and anyone teaching it.  It's (supposed to be) fairly easy to read and has hundreds of new visualizations.\n\nMost recently, I've added a section on generative models, including chapters on GANs, VAEs, normalizing flows, and diffusion models.\n\nLooking for feedback from the community.\n\n* If you are an expert, then what is missing?\n* If you are a beginner, then what did you find hard to understand?\n* If you are teaching this, then what can I add to support your course better?\n\nPlus of course any typos or mistakes.  It's kind of hard to proof your own 500 page book!","link":"https://www.reddit.com/r/MachineLearning/comments/10jlq1q/p_new_textbook_understanding_deep_learning/","created":"2023-01-23","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":53}}
