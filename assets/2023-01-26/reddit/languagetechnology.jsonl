{"title":"How would you approach this kind of Info/Entity extraction problem?","description":"Training dataset is as follows:\nThree columns:\nText (unstructured)\nLeads counts (integer)\nConversion counts (integer)\n(That\u2019s not an actual case, it\u2019s an example.)\n\nFor a given article, either could be zero, or not present.\n\nLooking to train a model that, when given a new text, extracts Leads count and Conversion count.\n\nThose are not counted one by one, they should be extracted from sentences.\n\nFor instance, text may be:\n\n\n```\nOn Tuesday, there was a large industry convention. We spoke with 12 people who were interested in the product. Out of them, three people decided to buy it. All in all, it was a ten out of ten event. There is another one on February 23 that I\u2019m looking forward to.\n```\n\nModel should return leads=12, conversions=3.\n\n\nI think how I want it to function is something like:\nWhen encountered a number \nCheck that sentence and sentence before and after\nClassify as leads number, conversions number or neither.\n\nI started on doing fine tuned BERT similar to named entity recognition, but that feels like an overkill, feels like I should be able to go a little lighter.\n\nWhat do you think?","link":"https://www.reddit.com/r/LanguageTechnology/comments/10ldqyp/how_would_you_approach_this_kind_of_infoentity/","created":"2023-01-26","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":2}}
{"title":"Targeted Summarization - A tool for information extraction","description":"&amp;#x200B;\n\n[Visual of the Algorithm](https://reddit.com/link/10kys25/video/dci8r3ovz6ea1/player)\n\nHere's the GitHub repo: [https://github.com/helliun/targetedSummarization](https://github.com/helliun/targetedSummarization)\n\nTextReducer is a tool for summarization and information extraction powered by the SentenceTransformer library. Unlike many techniques for extractive summaries, TextReducer has the option for a \"target\" around which the summary will be focused. This target can be any text prompt, meaning that a user can specify the type of information that they would like to find or summarize, and ignore everything else.\n\nAnother key benefits of TextReducer is that rather than extracting the sentences for the summary, it carves away at the original text, removing unnecessary sentences. This leads to more fluent summarizations, and preserves grammatical features like coreference that are often lost in traditional extractive summarization.\n\nFor instance, in the sentences \"In his free time, John enjoyed playing golf and traveling with his family. He was married with two children, and lived in a suburban area with his wife and kids.\", it is imporant that these sentences stay linked together. Otherwise, the coreferent of the word \"He\" in the second sentence is lost. TextReducer is much better at preserving such related sentences, and is thus a valuable tool for fast, but fluent summarizations of large texts.","link":"https://www.reddit.com/r/LanguageTechnology/comments/10kys25/targeted_summarization_a_tool_for_information/","created":"2023-01-25","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":2}}
{"title":"What is the largest model that can be feasibly trained on a RTX 4090 24GB?","description":"I am interested to hear what the largest model is that I can feasibly train on a single RTX 4090 24GB card. For sure the upper bound is a model of 24GB max. If we additionally take into account the additional RAM memory needed to do inference, loss back propagation etc, how large can we go? I understand this will also depend on the batch size, so let's fix that to 16 to have an explicit example. Does anyone have experience with training such a model and this card? What is the largest model you reached?\n\nAdditionally, if I have two RTX 4090 24GB cards, is it feasible to split the model over these two cards? Would this allow me to fit a model roughly twice as big as on one card, or is there significant overhead?\n\nI would appreciate any insight you have.","link":"https://www.reddit.com/r/LanguageTechnology/comments/10kxc0k/what_is_the_largest_model_that_can_be_feasibly/","created":"2023-01-25","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":6}}
{"title":"Bi-Encoder with BERT does not learn","description":"My data consists of 15k question and answer pairs. I am using a bi-encoder with a pre-trained BERT model, to obtain the most fitting answer for a new question. Each question/answer pair has a category name, which I added to the beginning of each question and answer. I'm using a qrels file as well, which has the relevancy = 1 for all the question/answer pairs, and that's about it.\n\nSame dataset gave me acceptable mean metrics on BM25 (0.4 recall). But the bi-encoder fails to learn anything meaningful, all metrics are nearly zero after training for &gt;10 epochs, batch size being 16. \n\nWhat could be the possible causes? Where should I start looking at?","link":"https://www.reddit.com/r/LanguageTechnology/comments/10lci6h/biencoder_with_bert_does_not_learn/","created":"2023-01-26","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":5}}
{"title":"INSTRUCTOR instruction fine-tuned text embeddings","description":"In this video  I   explain about INSTRUCTOR, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor achieves sota on 70 diverse embedding tasks! I also show a google collab demo of instructor\n\nhttps://youtu.be/vg38cq3KJ6M","link":"https://www.reddit.com/r/LanguageTechnology/comments/10ksfhg/instructor_instruction_finetuned_text_embeddings/","created":"2023-01-25","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"high-performance computing (HPC) for language technology in EU","description":"I am looking to compile list of HPC open to researchers in the field of language technology.\n\nCan you please point me to the HPC that are available to researchers and small and medium enterprises.\n\nFor example,\n\n\\- LEONARDO\n\n\\- [https://www.lumi-supercomputer.eu/](https://www.lumi-supercomputer.eu/)","link":"https://www.reddit.com/r/LanguageTechnology/comments/10ky556/highperformance_computing_hpc_for_language/","created":"2023-01-25","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"How to create a caption from the question-answer pair?","description":"I was wondering if it is possible to create a caption or a sentence from the given question-answer pair.\n\nSo given any question-answer pair, I want a caption. Is there any existing work on this? How can I achieve this?\n\nFor example :\n\n1)\n\nQ: What is on the table?\n\nA: A bottle\n\nI want to get something like: \"A bottle is on the table.\"\n\n&amp;#x200B;\n\n2) \n\nQ: What is the man doing?\n\nA: jumping\n\nI want to get something like: \"The man is jumping.\"\n\n&amp;#x200B;","link":"https://www.reddit.com/r/LanguageTechnology/comments/10kvb72/how_to_create_a_caption_from_the_questionanswer/","created":"2023-01-25","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"The ChatGPT Cheat Sheet","description":"\ud83d\ude01 Happy to introduce one of the most comprehesive ChatGPT cheat sheets: a 30 pg. paper highlighting various prompts to manage ChatGPT for generating text. The document not only highlights what ChatGPT can generate but also how it can generate it! Here is the TOC:\n\n1. NLP Tasks\n2. Code\n3. Structured Output Styles\n4. Unstructured Output Styles\n5. Media Types\n6. Meta ChatGPT\n7. Expert Prompting\n\nGoogle Doc: [https://drive.google.com/file/d/1OcHn2NWWnLGBCBLYsHg7xdOMVsehiuBK/view?usp=share\\_link](https://drive.google.com/file/d/1OcHn2NWWnLGBCBLYsHg7xdOMVsehiuBK/view?usp=share_link)","link":"https://www.reddit.com/r/LanguageTechnology/comments/10k67l1/the_chatgpt_cheat_sheet/","created":"2023-01-24","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":8}}
{"title":"Data preparation for embedding","description":"I need to improve the quality of embeddings for a specific task. \nI have a huge Text corpus but it doesn\u2019t have any labels or similarity indicators attached to it \n Can somebody point we into the right direction where to get started ?","link":"https://www.reddit.com/r/LanguageTechnology/comments/10kusq2/data_preparation_for_embedding/","created":"2023-01-25","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":2}}
{"title":"What tests and validations do you perform on your models and data?","description":"I'm currently developing (at deepchecks), an **open source** python package for validation of NLP data and models (that is of course meant for free use for the entire NLP community). \n\nSo I want to ask you - what common mistakes did you encounter in developing your model? How did you find them? How did you solve them? How did you think they can be solved but never had the time to do it? :)","link":"https://www.reddit.com/r/LanguageTechnology/comments/10khdbc/what_tests_and_validations_do_you_perform_on_your/","created":"2023-01-24","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Debiasing GPT-3 model output: any idea as to what that process looks like at OpenAI?","description":"Last year's inverse scaling contest revealed some interesting trends in de-biasing / bias mitigation attempts by OpenAI in its GPT-3 models (`ada` ---&gt; `babbage`---&gt; `curie`---&gt; `davinci`). Prompts in which inverse scaling phenomena were most apparent included topics such as race, gender, ethnicity, class, religion, etc. \n\nDoes anyone have any idea as to what OpenAI's bias mitigation process looks like for the various sizes of GPT-3? I'd imagine that GPT-3 was trained on the large dataset (bigger than the Pile by quite a lot) and then, after the fact, the various models were put through the 'de-bias' fine-tuning wringer. And then those models were deployed as Models as a Service: `ada`, `babbage`, `curie`, `davinci`, etc. \n\nI'm wondering if anyone knows what the distillation process looked / looks like? Or if anyone knows anything about the debiasing process at OpenAI for the GPT-3 variants?","link":"https://www.reddit.com/r/LanguageTechnology/comments/10kgbpl/debiasing_gpt3_model_output_any_idea_as_to_what/","created":"2023-01-24","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"How to create chat bot similar to character.ai but for erotic fantasies?","description":"In playing around with [character.ai](https://character.ai) and ChatGPT I realize that it doesn't handle erotic content as it's against guidelines. I'm curious if there's a way to create a chatbot similar where you design the characters and their personalities. This seems like something lots of people would pay for. I'm a full-stack developer that recently transitioned into product management so I have all the skills to do this except the actual AI knowledge. If anyone with a good resume/track record is interested in helping as a possible co-founder I'd love to chat. Also, I'm open with my ideas so if anyone has suggestions or questions here please let me know! Thank you in advance :)","link":"https://www.reddit.com/r/LanguageTechnology/comments/10kf6io/how_to_create_chat_bot_similar_to_characterai_but/","created":"2023-01-24","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Need help with a project.","description":"I have a text and a reason and I need predict if **text** satisfies the **reason**. But the catch here is the training dataset only has positive examples, where reason satisfies the text. Can someone help me how to train a model?","link":"https://www.reddit.com/r/LanguageTechnology/comments/10k71ad/need_help_with_a_project/","created":"2023-01-24","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"How to fine-tune T5 for multiple tasks?","description":"I am fine-tuning T5 for multiple tasks so that they work together. I want the models to work together. For example, summarization and translation should work together. The summarized text is the input to the translation model and gets translated. \n\nQ1. Can we train T5 with different datasets and prefixes and expect it to work this way?\n\nQ2. Is it possible to concatenate the models and make them one single model?","link":"https://www.reddit.com/r/LanguageTechnology/comments/10jyrgm/how_to_finetune_t5_for_multiple_tasks/","created":"2023-01-24","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Word2Vec for code analyzation","description":"So recently me and a long-time partner of mine has gotten into NLPs and we wanted to try and use a model to pretty much find similarity between functions and package names. \n\nThe goal would be to create a program which through the NLP models can see if the code structure makes sense. For example if given a program with the file with functions car, audi, bmw and cat, with the package names car, it should see that cat doesn\u2019t belong there and tell the user to move it to a new package, maybe even give it a hint on package name. It would be used to test code structure logic, to aid in maintainability and readability of code. \n\nWe\u2019re very early in our development and we\u2019re still not sure if word2vec is the best choice, or even how we\u2019re supposed to represent their similarities. Anyone got an idea if there are improvements to our idea or does it sound fair?","link":"https://www.reddit.com/r/LanguageTechnology/comments/10jsjsd/word2vec_for_code_analyzation/","created":"2023-01-24","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":1}}
