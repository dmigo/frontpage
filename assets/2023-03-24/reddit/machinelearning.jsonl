{"title":"Reminder: Use the report button and read the rules!","description":"","link":"https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/","created":"2023-03-24","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[N] ChatGPT plugins","description":"[https://openai.com/blog/chatgpt-plugins](https://openai.com/blog/chatgpt-plugins)\n\n&gt;We\u2019ve implemented initial support for plugins in ChatGPT. Plugins are tools designed specifically for language models with safety as a core principle, and help ChatGPT access up-to-date information, run  computations, or use third-party services.","link":"https://www.reddit.com/r/MachineLearning/comments/11zsdwv/n_chatgpt_plugins/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":101}}
{"title":"[D] \"Sparks of Artificial General Intelligence: Early experiments with GPT-4\" contained unredacted comments","description":"Microsoft's research paper exploring the capabilities, limitations and implications of an early version of GPT-4 was [found to contain unredacted comments by an anonymous twitter user.](https://twitter.com/DV2559106965076/status/1638769434763608064) ([threadreader](https://threadreaderapp.com/thread/1638769434763608064.html), [nitter](https://nitter.lacontrevoie.fr/DV2559106965076/status/1638769434763608064), [archive.is](https://archive.is/1icMv), [archive.org](https://web.archive.org/web/20230323192314/https://twitter.com/DV2559106965076/status/1638769434763608064)) \n\n- Commented section titled \"Toxic Content\": https://i.imgur.com/s8iNXr7.jpg\n- [`dv3` (the interval name for GPT-4)](https://pastebin.com/ZGMzgfqd)\n- [`varun`](https://pastebin.com/i9KMFcy5) \n- [commented lines](https://pastebin.com/Aa1uqbh1)\n\n[arxiv](https://arxiv.org/abs/2303.12712), [original /r/MachineLearning thread](https://www.reddit.com/r/MachineLearning/comments/11z3ymj/r_sparks_of_artificial_general_intelligence_early), [hacker news](https://twitter.com/DV2559106965076/status/1638769434763608064)","link":"https://www.reddit.com/r/MachineLearning/comments/1200lgr/d_sparks_of_artificial_general_intelligence_early/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":18}}
{"title":"[P] ChatGPT with GPT-2: A minimum example of aligning language models with RLHF similar to ChatGPT","description":"hey folks, happy Friday! I wish to get some feedback for my recent project of a minimum example of using RLHF on language models to improve human alignment. \n\nThe goal is to compare with vanilla GPT-2 and supervised fine-tuned GPT-2 to see how much RLHF can benefit small models. Also I hope this project can show an example of the minimum requirements to build a RLHF training pipeline for LLMs.\n\nGithub: https://github.com/ethanyanjiali/minChatGPT\nDemo: https://colab.research.google.com/drive/1LR1sbWTyaNAmTZ1g1M2tpmU_pFw1lyEX?usp=sharing\n\nThanks a lot for any suggestions and feedback!","link":"https://www.reddit.com/r/MachineLearning/comments/120csub/p_chatgpt_with_gpt2_a_minimum_example_of_aligning/","created":"2023-03-24","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[D] What is the best open source chatbot AI to do transfer learning on?","description":"Let's say I have some proprietary text data. I want to train a chatbot to absorb said knowledge and be able to answer questions about it. \n\nWhat are the best open source frameworks for getting started with such a project? \n\nIdeally I'd want to be able to build out human feedback as well for sample prompts, to better help train.","link":"https://www.reddit.com/r/MachineLearning/comments/11zzgzc/d_what_is_the_best_open_source_chatbot_ai_to_do/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":11}}
{"title":"[R] Sparks of Artificial General Intelligence: Early experiments with GPT-4","description":"[New paper](https://arxiv.org/abs/2303.12712) by MSR researchers analyzing an early (and less constrained) version of GPT-4. Spicy quote from the abstract:\n\n\"Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"\n\nWhat are everyone's thoughts?","link":"https://www.reddit.com/r/MachineLearning/comments/11z3ymj/r_sparks_of_artificial_general_intelligence_early/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":317}}
{"title":"[P] The noisy sentences dataset: 550K sentences in 5 European languages augmented with noise for training and evaluating spell correction tools or machine learning models.","description":"GitHub: https://github.com/radi-cho/noisy-sentences-dataset\n\nWe have constructed our dataset to cover representatives from the language families used across Europe.\n\nGermanic - English, German;\nRomance - French;\nSlavic - Bulgarian;\nTurkic - Turkish;\n\nUse case example: Apply language models or other techniques to compare the sentence pairs and reconstruct the original sentences from the augmented ones. You can use a single multilingual solution to solve the challenge or employ multiple models/techniques for the separate languages. Per-word dictionary lookup is also an option.","link":"https://www.reddit.com/r/MachineLearning/comments/11zyi1s/p_the_noisy_sentences_dataset_550k_sentences_in_5/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[D] [R] GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models","description":"A paper was released by OpenAI, OpenResearch &amp; UPenn titled \"GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models.\"Link: [https://arxiv.org/abs/2303.10130](https://arxiv.org/abs/2303.10130)\n\nAbstract: We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that these models could have notable economic, social, and policy implications.\n\nWhat do you think about the societal and economic impacts of LLMs?\n\nAlso, I've started an open-source repository to track projects and research papers about GPT-4: [https://github.com/radi-cho/awesome-gpt4](https://github.com/radi-cho/awesome-gpt4). There are some related papers listed already. I would greatly appreciate your contributions.","link":"https://www.reddit.com/r/MachineLearning/comments/11zi0km/d_r_gpts_are_gpts_an_early_look_at_the_labor/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":31}}
{"title":"[P] Attention all drivers! \ud83d\ude97\ud83d\udca8 Want to predict driving behavior using Machine Learning? Check out this project!","description":"Are you concerned about road safety and want to make a positive impact? Look no further than \"Driving Behavior Prediction with Machine Learning\"! This project uses data from vehicles and an ML algorithm to predict and mitigate driving behavior, which could ultimately lead to fewer accidents on the road.\n\nWe invite you to take a closer look at the Git repository for this project [https://github.com/YashRevannavar/DrivingBehavior] , where you'll find the code and documentation for the ML algorithm. The results are promising, and we believe that this project has the potential to make a real difference in the world.\n\nSo if you're interested in learning more about this cutting-edge project, join us in exploring the possibilities of using Machine Learning to predict driving behavior. Let's work together to create a safer future for all drivers!\n\n#MachineLearning #DrivingBehaviorPrediction #RoadSafety #DataScience #AI #MLAlgorithm","link":"https://www.reddit.com/r/MachineLearning/comments/120d8hi/p_attention_all_drivers_want_to_predict_driving/","created":"2023-03-24","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[D] Are there any methods to deal with false-negatives in a binary classification problem?","description":"I'm interested in a binary classification problem. However I know my dataset contains false-negative labeled data (and no false-positive). Is there any literature or good approach for a problem like this? Maybe label smoothing or something?","link":"https://www.reddit.com/r/MachineLearning/comments/120cy4r/d_are_there_any_methods_to_deal_with/","created":"2023-03-24","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[P] GPT-4 powered full stack web development with no manual coding","description":"[https://www.youtube.com/watch?v=lZj63vjueeU](https://www.youtube.com/watch?v=lZj63vjueeU)\n\nWhat do you all think of this approach to full stack gpt-assisted web development? In a sense its no code because the human user does not write or even edit the code - but in a sense its the opposite, because only an experienced web developer or at least a product manager would know how to instruct GPT in a useful manner.\n\n\\*\\*\\* We are seeking donations to ensure this project continues and, quite literally, keep the lights on. Cryptos, cash, cards, openai access tokens with free credits, hardware, cloud GPUs, etc... all is appreciated. Please DM to support this really cool open source project \\*\\*\\*\n\nPS. I'm the injured engineer who made this thing out of necessity, because i injured my wrist building an AI platform that's become way too big for one engineer to maintain. So AMA :)","link":"https://www.reddit.com/r/MachineLearning/comments/11z7r4c/p_gpt4_powered_full_stack_web_development_with_no/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":42}}
{"title":"[D] is it possible to use encodings from the vggface2 for face swap","description":"i\u2019m currently doing a project with the vggface2 resnet model. i had an idea to do a face swap with getting the encodings of the source and target faces, manipulating them. passing this new one into a decoder to get the face and blending it onto the original image. \n\nis this possible? i tried a version but the image was just noise and i think it was the decoder. i wasn\u2019t too sure how to go about it","link":"https://www.reddit.com/r/MachineLearning/comments/1205ij6/d_is_it_possible_to_use_encodings_from_the/","created":"2023-03-24","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[N] PyG 2.3.0 released: PyTorch 2.0 support, native sparse tensor support, explainability and accelerations","description":"PyG (PyTorch Geometric) is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.\n\nToday version 2.3 got released: https://github.com/pyg-team/pytorch_geometric/releases/tag/2.3.0","link":"https://www.reddit.com/r/MachineLearning/comments/11zgl87/n_pyg_230_released_pytorch_20_support_native/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[D] Ben Eysenbach, CMU: On designing simpler and more principled RL algorithms","description":"Listen to the [podcast episode](https://generallyintelligent.com/podcast/2023-03-22-podcast-episode-30-ben-eysenbach/) with Ben Eysenbach from CMU where we discuss about designing simpler and more principled RL algorithms!","link":"https://www.reddit.com/r/MachineLearning/comments/12000z1/d_ben_eysenbach_cmu_on_designing_simpler_and_more/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[R] Question about Selection of Machine Learning Type for a Neuroscience/Biomedical Engineering Problem","description":"All:\n\nThank you for reading this. I have the following problem and goals:\n\nLet's say that I have measurements of neuron spiking activity from a particular location of the brain of a rat. This rat is also involved in a behavioral task in which the rat has to press a button in response to some visual cue. Suppose we have the following neuron spike activity time series with overlaid instances of button presses:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/t2tdfywr6hpa1.png?width=1890&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=6f005593bb4749b1c7935ab667a8c2569b9a8a7b\n\nI want to identify feature of the time series (for example, in the frequency domain) to then use to make a model that can make predictions on button presses based on neuron spike activity. I'm under the impression that I can then arrive at confidence intervals for button presses (in terms of the time period window when the model 'thinks' a button press has occurred).\n\nI'm lost when it comes to types of machine learning models I can use for my particular goal. Any input is appreciated. If I need to provide more information, please let me know. Thank you again.","link":"https://www.reddit.com/r/MachineLearning/comments/11zhnzb/r_question_about_selection_of_machine_learning/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":2}}
{"title":"[R] Zero-shot Sign Pose Embedding model","description":"We built a model that converts sign language videos into embeddings. It takes body and hand pose keypoints from a video and converts this into an embedding for use in downstream tasks. We show how classification can be done on an unseen dataset.\n\nYou can check out the repo at [https://github.com/xmartlabs/spoter-embeddings](https://github.com/xmartlabs/spoter-embeddings) and the accompanying blog post [here](https://blog.xmartlabs.com/blog/machine-learning-sign-language-recognition/).","link":"https://www.reddit.com/r/MachineLearning/comments/11zlu03/r_zeroshot_sign_pose_embedding_model/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[P] Serge, a self-hosted app for running LLaMa models (Alpaca) entirely locally, no remote API needed.","description":"Hello there!\n\n[Serge chat UI, with conversations on the left](https://preview.redd.it/rayrn7m4ncpa1.png?width=1922&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2bea149c499f4f0b9ad0b6aceb9dc21404c6e9d5)\n\nI've recently been working on Serge, a self-hosted dockerized way of running LLaMa models with a decent UI &amp; stored conversations. It currently supports Alpaca 7B, 13B and 30B and we're working on integrating it with LangChain and the ReAct chain agent.\n\nI've tried my best at making the instructions dead easy, so it's all dockerized with a download manager for weights and it can be run with almost zero configuration required.\n\nI think being able to run those models locally will be key to expanding their ability, and so I hope this can contribute to that.\n\nLet me know if you have any feedback or suggestions on how to extend its capabilities!\n\n&amp;#x200B;\n\nGitHub: [https://github.com/nsarrazin/serge](https://github.com/nsarrazin/serge)","link":"https://www.reddit.com/r/MachineLearning/comments/11yvbzc/p_serge_a_selfhosted_app_for_running_llama_models/","created":"2023-03-22","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":40}}
{"title":"[R] Introducing SIFT: A New Family of Sparse Iso-FLOP Transformations to Improve the Accuracy of Computer Vision and Language Models","description":"**Note**: Thank you r/MachineLearning for providing so many awesome naming alternatives! We'll revise the name accordingly. We look forward to hearing any additional feedback you have on the research.\n\nWe are excited to announce the availability of our [paper on arxiv](https://arxiv.org/abs/2303.11525) on Sparse Iso-FLOP Transformations (SIFT), which increases accuracy and maintains the same FLOPs as the dense model using sparsity. In this research, we replace dense layers with SIFT and significantly improve computer vision and natural language processing tasks without modifying training hyperparameters\n\nSome of the highlights of this work include ResNet-18 on ImageNet achieving a 3.5% accuracy improvement and GPT-3 Small on WikiText-103 reducing perplexity by 0.4, both matching larger dense model variants that have 2x or more FLOPs.\n\nThe SIFT transformations are simple to use, provide a larger search space to find optimal sparse masks, and are parameterized by a single hyperparameter - the sparsity level.\n\nThis is independent of the research we [posted](https://www.reddit.com/r/MachineLearning/comments/11xskuk/r_spdf_sparse_pretraining_and_dense_finetuning/) yesterday, which demonstrates the ability to reduce pre-training FLOPs while maintaining accuracy on downstream tasks.\n\nThis is the first work (that we know of!) to demonstrate the use of sparsity for improving the accuracy of models via a set of sparse transformations.\n\nhttps://preview.redd.it/7y8cgaisddpa1.png?width=3536&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9c7123463516291acc495b47625c0dd874fd9c43","link":"https://www.reddit.com/r/MachineLearning/comments/11yzsz6/r_introducing_sift_a_new_family_of_sparse_isoflop/","created":"2023-03-22","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":32}}
{"title":"[N] Prompt-to-voice (Dall-E for Voice)","description":"Blogpost: [Introducing Prompt-to-Voice - Describe It to Hear It / Blog / Coqui](https://coqui.ai/blog/tts/prompt-to-voice)  \n\n\nThere is still space for improvement, but that is an exciting take on voice creation. \n\nI wonder if it'd be open-sourced alongside [TTS.](https://github.com/coqui-ai/TTS)","link":"https://www.reddit.com/r/MachineLearning/comments/11zgmb2/n_prompttovoice_dalle_for_voice/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[P] Open-source GPT4 &amp; LangChain Chatbot for large PDF docs","description":"GitHub: [https://github.com/mayooear/gpt4-pdf-chatbot-langchain](https://github.com/mayooear/gpt4-pdf-chatbot-langchain)  \nDemo video: [https://www.youtube.com/watch?v=ih9PBGVVOO4](https://www.youtube.com/watch?v=ih9PBGVVOO4)","link":"https://www.reddit.com/r/MachineLearning/comments/11z9s3g/p_opensource_gpt4_langchain_chatbot_for_large_pdf/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":13}}
{"title":"[P][R][D] Feature Subset Selection (NP-Hard)","description":"Hey guys, for my project this semester I am to tackle the problem of Feature Subset Selection.\n\nMy original approach to this problem was to find a pure categoric dataset to run classification tasks, a pure numeric one for both classification and regression and lastly multiple multivariate ones for both tasks.\n\nI will be using ITMO FS and ASU libraries and multiple of their different algorithms to find feature subsets from these databases.\n\nThe candidate features outputted from these algorithms will be used to train multiple ML models. The goal is not to rank the ML models but to rank or discuss the FS algorithms but ML is needed all throughout the way. An ensemble final prediction (or a score) will be taken from the ML predictors.\n\nThan comes the fitness measuring part where I will be ranking the selected feature subsets, trying to find common selected features accross the FS algorithms or any correlations, rules, anything interesting as the findings part tbh. I am planning on using this post as an update board a discussion board and a helpline.\n\nFor the start I need to find the datasets. I think that different datasets with both high and low number of features should be used. There comes the first part I need help with. Finding the candidate datasets. I am open to any recommendations of datasets to use, their numbers and everything really.\n\nAny help all throughout this campaign is highly appreciated.\n\nThanks in advance you awesome redditors","link":"https://www.reddit.com/r/MachineLearning/comments/11zqosr/prd_feature_subset_selection_nphard/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":2}}
{"title":"[D] Best decoder only Language model under 400M parameters ?","description":"Hello,\nI\u2019m looking for a decent GPT-like Language model which is relatively small in size.\n\n Thanks in advance !","link":"https://www.reddit.com/r/MachineLearning/comments/11zq93r/d_best_decoder_only_language_model_under_400m/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":3}}
{"title":"[D] LLaMA or Alpaca Weights","description":"Was anyone able to download the LLaMA or Alpaca weights for the 7B, 13B and or 30B models? If yes please share, not looking for HF weights","link":"https://www.reddit.com/r/MachineLearning/comments/11zcog6/d_llama_or_alpaca_weights/","created":"2023-03-23","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":10}}
{"title":"[D] Overwhelmed by fast advances in recent weeks","description":"I was watching the GTC keynote and became entirely overwhelmed by the amount of progress achieved from last year.  I'm wondering how everyone else feels.\n\n&amp;#x200B;\n\nFirstly, the entire ChatGPT, GPT-3/GPT-4 chaos has been going on for a few weeks, with everyone scrambling left and right to integrate chatbots into their apps, products, websites. Twitter is flooded with new product ideas, how to speed up the process from idea to product, countless promp engineering blogs, tips, tricks, paid courses.\n\n&amp;#x200B;\n\nNot only was ChatGPT disruptive, but a few days later, Microsoft and Google also released their models and integrated them into their search engines. Microsoft also integrated its LLM into its Office suite. It all happenned overnight. I understand that they've started integrating them along the way, but still, it seems like it hapenned way too fast. This tweet encompases the past few weeks perfectly [https://twitter.com/AlphaSignalAI/status/1638235815137386508](https://twitter.com/AlphaSignalAI/status/1638235815137386508) , on a random Tuesday countless products are released that seem revolutionary.\n\n&amp;#x200B;\n\nIn addition to the language models, there are also the generative art models that have been slowly rising in mainstream recognition. Now Midjourney AI is known by a lot of people who are not even remotely connected to the AI space.\n\n&amp;#x200B;\n\nFor the past few weeks, reading Twitter, I've felt completely overwhelmed, as if the entire AI space is moving beyond at lightning speed, whilst around me we're just slowly training models, adding some data, and not seeing much improvement, being stuck on coming up with \"new ideas, that set us apart\".\n\n&amp;#x200B;\n\nWatching the GTC keynote from NVIDIA I was again, completely overwhelmed by how much is being developed throughout all the different domains. The ASML EUV (microchip making system) was incredible, I have no idea how it does lithography and to me it still seems like magic. The Grace CPU with 2 dies (although I think Apple was the first to do it?) and 100 GB RAM, all in a small form factor. There were a lot more different hardware servers that I just blanked out at some point. The omniverse sim engine looks incredible, almost real life (I wonder how much of a domain shift there is between real and sim considering how real the sim looks). Beyond it being cool and usable to train on synthetic data, the car manufacturers use it to optimize their pipelines. This change in perspective, of using these tools for other goals than those they were designed for I find the most interesting.\n\n&amp;#x200B;\n\nThe hardware part may be old news, as I don't really follow it, however the software part is just as incredible. NVIDIA AI foundations (language, image, biology models), just packaging everything together like a sandwich. Getty, Shutterstock and Adobe will use the generative models to create images. Again, already these huge juggernauts are already integrated.\n\n&amp;#x200B;\n\nI can't believe the point where we're at. We can use AI to write code, create art, create audiobooks using Britney Spear's voice, create an interactive chatbot to converse with books, create 3D real-time avatars, generate new proteins (?i'm lost on this one), create an anime and countless other scenarios. Sure, they're not perfect, but the fact that we can do all that in the first place is amazing.\n\n&amp;#x200B;\n\nAs Huang said in his keynote, companies want to develop \"disruptive products and business models\". I feel like this is what I've seen lately. Everyone wants to be the one that does something first, just throwing anything and everything at the wall and seeing what sticks.\n\n&amp;#x200B;\n\nIn conclusion, I'm feeling like the world is moving so fast around me whilst I'm standing still. I want to not read anything anymore and just wait until everything dies down abit, just so I can get my bearings. However, I think this is unfeasible. I fear we'll keep going in a frenzy until we just burn ourselves at some point.\n\n&amp;#x200B;\n\nHow are you all fairing? How do you feel about this frenzy in the AI space? What are you the most excited about?","link":"https://www.reddit.com/r/MachineLearning/comments/11ybjsi/d_overwhelmed_by_fast_advances_in_recent_weeks/","created":"2023-03-22","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":325}}
