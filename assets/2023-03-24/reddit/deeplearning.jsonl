{"title":"I deployed a Deep-Learning model as a REST-API to detect Pneumonia using AWS tools","description":"Link to proj: [https://github.com/akkik04/PulmoLens](https://github.com/akkik04/PulmoLens)\n\nPulmoLens is a deep learning model that uses AWS SageMaker and associated tools to detect pneumonia in X-ray images. The project leverages the power of machine learning fundamentals to create an accurate model (validation accuracy of 85%), which has been extensively tested using PostMan-API to confirm its efficacy. The model has been deployed using a serverless architecture, which includes AWS Lambda, API Gateway, S3, IAM, and CloudWatch. The model's endpoint is currently not active to avoid incurring unnecessary costs. To use the model, you will need to deploy it yourself (instructions will be provided below soon).","link":"https://www.reddit.com/r/deeplearning/comments/12035gm/i_deployed_a_deeplearning_model_as_a_restapi_to/","created":"2023-03-24","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":2}}
{"title":"LTSM to improve autonomous navigation","description":"I'm using DRL to train a model that allows a mobile robot to navigate its environment autonomously (without using a map).\n\nIn the DRL's model, the neural network is completely feedforward. Nothing fancy, just basic layers above layers and the performance is good. It is even good in the case of dynamic obstacles but in the same time, dynamic obstacles are usually humans that already navigate well but when two robots running the same model, a weird behavior happens and a collision is not out of the question. \n\nAnother problem I'm facing is the fact that sometimes the robot stucks at a local optimum, depending on the reward function I'm using.\n\nI've been reading around that having LTSM layers can improve the behavior by 'adding memory'. That memory should make it recognize a local from a global optimum and make it behave better if a dynamic obstacle is around its local proximity. But why? I'm not an expert in DL and all the articles on LTSM seems blunt and mathematical. \n\nCan somebody explain the core idea of LTSMs in an easy way?","link":"https://www.reddit.com/r/deeplearning/comments/120fbfl/ltsm_to_improve_autonomous_navigation/","created":"2023-03-24","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":0}}
{"title":"Why We Divide by N-1 in the Sample Variance Formula","description":"Hi guys,\n\nI have made a video [here](https://youtu.be/E3_408q1mjo) where I explain why and when we divide by n-1 instead of n in the sample variance.\n\nI hope it may be of use to some of you out there. Feedback is more than welcomed! :)","link":"https://www.reddit.com/r/deeplearning/comments/11zuwd7/why_we_divide_by_n1_in_the_sample_variance_formula/","created":"2023-03-23","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":3}}
{"title":"[Tutorial] Custom Object Detection using PyTorch Faster RCNN","description":"Custom Object Detection using PyTorch Faster RCNN\n\n[https://debuggercafe.com/custom-object-detection-using-pytorch-faster-rcnn/](https://debuggercafe.com/custom-object-detection-using-pytorch-faster-rcnn/)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/xckzf2tqzkpa1.png?width=1000&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=30866e63a00b423bc544656024d37e2fdefe9d83","link":"https://www.reddit.com/r/deeplearning/comments/12031yc/tutorial_custom_object_detection_using_pytorch/","created":"2023-03-24","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":0}}
{"title":"Best Way Alpaca GPU Inference","description":"What is currently the best model/code to run Alpaca inference on GPU? I saw there is a model with 4 bit quantization, but the code accompanying the model seems to be written for CPU inference (https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml/blob/main/ggml-alpaca-7b-q4.bin).","link":"https://www.reddit.com/r/deeplearning/comments/1200n9b/best_way_alpaca_gpu_inference/","created":"2023-03-23","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":1}}
{"title":"Cheshire Cat - Open source layer on top of any language model (extendible via plugins)","description":"&amp;#x200B;\n\n \\^.\\_.\\^\n\n&amp;#x200B;\n\nThe Cheshire Cat is an open source, customizable AI architecture:\n\n&amp;#x200B;\n\n\\- language model agnosatic (works with OpenAI, Cohere, HuggingFace models, custom)\n\n\\- long term memory\n\n\\- can use external tools (APIs, other models)\n\n\\- can ingest documents (.pdf, .txt)\n\n\\- 100% dockerized\n\n\\- extendible via plugins\n\n&amp;#x200B;\n\nWaiting for you to try it out and contribute with tutorials, code, and whatever makes you happy\n\n&amp;#x200B;\n\n\\#opensource #artificialintelligence #cognitivecomputing #deeplearning #cheshirecat\n\n&amp;#x200B;\n\nTutorial:\n\n&amp;#x200B;\n\n[https://www.youtube.com/watch?v=srsaYy0xmkc](https://www.youtube.com/watch?v=srsaYy0xmkc)\n\n&amp;#x200B;\n\nRepo:\n\n&amp;#x200B;\n\n[https://github.com/pieroit/cheshire-cat](https://github.com/pieroit/cheshire-cat)","link":"https://www.reddit.com/r/deeplearning/comments/11zk42o/cheshire_cat_open_source_layer_on_top_of_any/","created":"2023-03-23","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":0}}
{"title":"MAC M1 error pls help","description":"GPU:0Metal device set to: Apple M1  systemMemory: 16.00 GB maxCacheSize: 5.33 GB   2023-03-23 23:22:57.840298: I tensorflow/core/common\\_runtime/pluggable\\_device/pluggable\\_device\\_factory.cc:305\\] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support. 2023-03-23 23:22:57.840398: I tensorflow/core/common\\_runtime/pluggable\\_device/pluggable\\_device\\_factory.cc:271\\] Created TensorFlow device (/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)","link":"https://www.reddit.com/r/deeplearning/comments/11zs9t2/mac_m1_error_pls_help/","created":"2023-03-23","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":0}}
{"title":"Remove Person From Photo","description":" Remove Person From Photo in canva using magic eraser \n\n[Tutorial link](https://youtu.be/IkXfXgHTng8) \n\n&amp;#x200B;\n\nhttps://preview.redd.it/beqkhw4nuipa1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8b1ee763c485a3b06b04a961d8fe534fd2a9188a","link":"https://www.reddit.com/r/deeplearning/comments/11zqt8i/remove_person_from_photo/","created":"2023-03-23","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":0}}
{"title":"Question for use of ML in adaptive authentication","description":"Hi all, I'm looking for advice for using ML for Adaptive Authentication.\n\nThe use case is that I want to generate a unique identifier key from user bahavior. eg: Sam uses my app and I want to generate key 1234, Mel uses the app, her key is 2351, etc\n\nTo generate this key I thought I could use an ML model that takes as input user behavior data and outputs this key or something I can use to derive a key.\n\nTaking typing on a smartphone as an example: a user types 10 words on their keyboard, we take data from that and feed it to the model to generate the key for this user. The data we take might be something like speed of typing a letter, time fingers were pressed on keys, number of times they used backspace, etc...\n\nIs this possible? I'm not an ML specialist so my knowledge is limited, but I was thinking we could do something like using a classifier with 10 categories, and use some statistical value from the output equivalent to prediction accuracy or prediction certainty for each category to generate numbers out of the classifications... but that seems like a hack and there may be something more precise and standard","link":"https://www.reddit.com/r/deeplearning/comments/11zcc1a/question_for_use_of_ml_in_adaptive_authentication/","created":"2023-03-23","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":20}}
{"title":"Deep Learning Gone Wrong: The Rise of Filter Queens and the Fall of Natural Beauty","description":"I was feeling pretty good about myself for learning about artificial intelligence and thinking about all the cool things I could do with it. But then I realized that some girls are using it for a whole other level of fakeness - I mean, have you seen these filters? They're like deep learning gone wrong. They change everything from eye color to face shape to skin tone. I don't know about you, but I'm starting to wonder if the real AI apocalypse isn't the robots taking over, but the filters taking over our faces.  \nsome girls look like they belong in a Disney movie . Well, it's all thanks to the magic of filters and deep learning. These girls have figured out how to create a whole new face without ever leaving the comfort of their phones. Meanwhile, I can barely figure out how to turn on my computer. I guess I'll just have to accept that I'm no match for the filter queens. But hey, at least I can still recognize myself in the mirror!","link":"https://www.reddit.com/r/deeplearning/comments/11zmceo/deep_learning_gone_wrong_the_rise_of_filter/","created":"2023-03-23","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":4}}
{"title":"Comic Text Effect","description":"Comic Cartoon Text Effect in Canva \n\n[Tutorial link](https://youtu.be/ijVu0cnJbh0)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/h3noa0yeecpa1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3151dc891e906988cc81b77e2f26c9dd53d18c13","link":"https://www.reddit.com/r/deeplearning/comments/11ytv4g/comic_text_effect/","created":"2023-03-22","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":0}}
{"title":"Anyone have any good alternatives to Paperspace? My account got closed for unauthorized access.","description":"I've been using Paperspace Gradient Growth recently and really liked it. Their shutdown after 6 hours was annoying, but bearable. And the access to A5000s, A6000s, and A100s for $40 per month was pretty fantastic.\n\nUnfortunately, my account was closed. I was using it to train a Chess engine and I installed Stockfish, which is apparently not allowed. I contacted them because I was having issues logging in and they told me that they \"detected unauthorized activities\" and explained that I needed to upgrade my account to Paperspace Core to use it.\n\nThe 2 most popular services I've seen here are probably vast.ai and podrun, but I don't want to pay hourly. I'd prefer something with persistent storage so I don't need to be constantly processing or uploading large amounts of data.\n\nThere's Google Colab Pro, of course. But I don't like their lack of transparency in regards to pricing with their credit system.\n\nEdit: Title should say \"unauthorized activities.\"","link":"https://www.reddit.com/r/deeplearning/comments/11ys19h/anyone_have_any_good_alternatives_to_paperspace/","created":"2023-03-22","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":9}}
{"title":"Using CharBERT for text similarity","description":"Hi all!\n\nI'm looking into [CharBERT](https://arxiv.org/abs/2011.01513) for an university project, and I noticed that it was finetuned on many tasks like sentiment analysis, NER, and so on. I tried to use it to do text similarity by using only the pretrained version the authors give + a cosine similarity algorithm between word embeddings: is this the way to go? Should I treat the embeddings the model gives in some way before calculating the cosine distance?","link":"https://www.reddit.com/r/deeplearning/comments/11ycciy/using_charbert_for_text_similarity/","created":"2023-03-22","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":1}}
{"title":"[Pytorch] How do you efficiently keep in memory the attention weights in an autoregressive transformer","description":"Hi when I do an inference (not training) of my autoregressive transformer I do it substantially this way (I removed few lines to not affect readibility):\n\n    for i in range(max_batch_sequence_len):\n        for layer in self.layers:\n            y[:, i] = layer(x, keep_mask, y)[:, i]\n\nwhere my layers \"forward' are:\n\n    def forward(self, x: torch.Tensor, keep_mask: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n            attn_mask = (~keep_mask).unsqueeze(2) &amp; (~keep_mask).unsqueeze(2)\n            attn_mask = attn_mask.repeat_interleave(self.num_heads, dim=0)\n                \n            y_normed = self.layer_norm(y)\n            y = y + self.self_attn(y_normed) #A causal mask is applied\n    \n            x_normed = self.layer_norm(x)\n            y_normed = self.layer_norm(y)\n            y = y + self.cross_attn(y_normed, x_normed, attn_mask)\n            \n            y_normed = self.layer_norm(y)\n            y = y + self.ffn(y_normed)\n            return y\n    \n    def self_attn(self, y):\n            out, _ = self.attn1(\n                query=y, key=y, value=y, need_weights=False, is_causal=True,\n            )\n            return out\n    \n    def cross_attn(self, y, x, attn_mask):\n    \n        out, _ = self.attn2(\n            query=y, key=x, value=x, need_weights=False, attn_mask=attn_mask\n        )\n        return out\n\nI can see that using the attention weights and re-inputing them in a certain way I can manage to reduce the computation, especially at step i+1 the attention weights for j&lt;=i have all been already computed.  \n\n\nHas someone here have ever dealt with that and can suggest me a modification of my code?","link":"https://www.reddit.com/r/deeplearning/comments/11ypnr0/pytorch_how_do_you_efficiently_keep_in_memory_the/","created":"2023-03-22","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":0}}
{"title":"How to handle multiple languages in a sentence?","description":"Hi everyone I am having a task where I have to use product\\_title for making recommendations. But the challenge is that some product\\_titles are present in multiple languages. Now how to get embeddings for such product titles.  \n\nThe product title are mostly like:     \n\n   1. English only    \n\n   2. Japanese+ english     \n\n   3. German + english","link":"https://www.reddit.com/r/deeplearning/comments/11yp9g2/how_to_handle_multiple_languages_in_a_sentence/","created":"2023-03-22","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":1}}
{"title":"Is a GAN being able to generate realistic data analogous to it learning the underlying data generation mechanism of the input?","description":"If a specific GAN can be proven to have learned the underlying data distribution, can it be said that it has learned the mechanisms that generate the input data? I'm trying to find sources on this but am struggling so any help would be great","link":"https://www.reddit.com/r/deeplearning/comments/11yjmwk/is_a_gan_being_able_to_generate_realistic_data/","created":"2023-03-22","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":1}}
