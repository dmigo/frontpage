{"title":"How to interpret actions","description":"Hey guys, I would like to be able to extract actions along with their objects. For example, in the sentence \"Paint all the walls red and hide all the doors and windows.\", I would like to extract the verbs \"paint\" and \"hide\", the objects \"walls, doors, windows\", the relationships \"paint-&gt;walls\", \"hide-&gt;doors, windows\", and the adverb relationship \"paint-&gt;red\".\n\nWhat tools/techniques would you suggest? Is deep learning the way to go?\n\n[Spacy](https://spacy.io/usage/rule-based-matching#dependencymatcher) and [Stanza](https://stanfordnlp.github.io/stanza/available\\_models.html) look promising, but I am not sure.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11nozbv/how_to_interpret_actions/","created":"2023-03-10","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Training Transformer Networks in Scikit-Learn?!","description":"Have you ever wanted to use handy scikit-learn functionalities with your neural networks, but couldn\u2019t because TensorFlow models are not compatible with the scikit-learn API?\n\nI\u2019m excited to introduce one-line wrappers for TensorFlow/Keras models that enable you to use TensorFlow models within scikit-learn workflows with features like Pipeline, GridSearch, and more.\n\nTransformers are extremely popular for modeling text nowadays with GPT3, ChatGPT, Bard, PaLM, FLAN excelling for conversational AI and other Transformers like T5 &amp; BERT excelling for text classification. Scikit-learn offers a broadly useful suite of features for classifier models, but these are hard to use with Transformers. However not if you use these wrappers we developed, which only require changing one line of code to make your existing Tensorflow/Keras model compatible with scikit-learn\u2019s rich ecosystem!\n\nAll you have to do is swap `keras.Model` \u2192 `KerasWrapperModel`, or `keras.Sequential` \u2192 `KerasSequentialWrapper`. The wrapper objects have all the same methods as their keras counterparts, plus you can use them with tons of awesome scikit-learn methods.\n\nYou can find a demo jupyter notebook and read more about the wrappers here: [https://cleanlab.ai/blog/transformer-sklearn/](https://cleanlab.ai/blog/transformer-sklearn/)","link":"https://www.reddit.com/r/LanguageTechnology/comments/11mzctf/training_transformer_networks_in_scikitlearn/","created":"2023-03-09","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Can NLP be used to categorize individuals based on responses?","description":"Hi all,\n\nNew to the field of machine learning and have a dataset with survey responses. I was wondering if NLP can be utilized to categorize individuals based on their responses (approximately 2-3 categories), or is this something better for another domain of machine learning?\n\nIt seems like NLP is more for language generation and interaction. I haven't found much with a couple of quick Google searches around categorization, which makes me think it likely isn't role but just want to check.\n\nThank you!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11n56np/can_nlp_be_used_to_categorize_individuals_based/","created":"2023-03-09","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":10}}
{"title":"Computational Linguist looking to expand","description":"Hello,\n\nI\u2019m in between jobs right now and looking to expand my career. I\u2019ve held about 4-5 jobs as a computational linguist. It remains my strong suit but I\u2019m also realizing that there are very few jobs for compling. Last role I interviewed for was for an nlp engineer and I realized I\u2019m falling short for anything after building a prototype. I\u2019m looking to get back into \u201cstudying\u201d and considering MLOps or Data Science or MBA as I have held two roles as a product manager too (of language technologies) so may be time to explore that area too. My preference is definitely engineering over product management but I wanted to hear people\u2019s opinion on what/ how to stay relevant to the language technology domain.\n\nThanks for reading!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11mvjhs/computational_linguist_looking_to_expand/","created":"2023-03-09","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":2}}
{"title":"Wanna Get Training Datasets For Social media spam classifier","description":"I am planning to build social media's spam classifier with Multinomial Naive Bayes model with python, using \\`sklearn\\` and \\`spacy\\` library. And the text feature extraction technique I will use is tf-idf vectorizer.\n\nHowever, I am having problem to find social media datasets with labelled data as SPAM or NOT SPAM. Another criteria with the datasets is that I need the datasets to be balanced (with roughly equal number of SPAM and NOT SPAM data).\n\nDo suggest me some links or source that I could get the data from?\n\nHope for help. Thanks in advance.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11muxkl/wanna_get_training_datasets_for_social_media_spam/","created":"2023-03-09","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"[Beginner] Any tips/resources on where should I start?","description":"I would like to create a simple chatbot where user would ask a school-related question (e.g., when is the enrollment) and the response will be based on the answer column on the dataset.\n\nWhat I had in mind is to use Question Answering but without need to input the context.  The problem is most of the tutorials I found (HuggingFace) uses with the *'with context'* approach and my Dataset consist only question and answer columns.\n\nAny help or tutorials would greatly help.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11mims7/beginner_any_tipsresources_on_where_should_i_start/","created":"2023-03-09","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":2}}
{"title":"Semantic Search: With Exclusions","description":"I am making a semantic search engine in Python that takes a user input and returns the 5 most similar results from a list of sentences. \n\nThe list of sentences features a list of things not included in the category at the end of a sentence e.g.   \u201cThis category includes: lions, tigers. This category excludes: birds, bees\u201d  \n\nCurrently if I search \u201cbirds\u201d the above example would be returned as strong similarity due to the word matching with \u201ccategory excludes: birds\u201d \n\nDoes anyone know any way to prevent this?\nAny help appreciated!!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11m4niv/semantic_search_with_exclusions/","created":"2023-03-08","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":9}}
{"title":"Encoder-decoder architecture for POS tagging","description":"I understand following about encoder and decoder:\n\n&gt; An encoder is a network that takes the input, and output a feature map/vector/tensor. These feature vector hold the information, the features, that represents the input. The decoder is again a network that takes the feature vector from the encoder, and gives the best closest match to the actual input or intended output.\n\nI want to implement POS tagging with encoder and decoder. I can guess that we can use \"encoder-only\" model to do POS tagging. Can we use \"encoder-decoder\" architecture for POS tagging task? If yes, then how should I design it. Most importantly I am not able to get what input will the decoder get from the encoder.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11m5rzs/encoderdecoder_architecture_for_pos_tagging/","created":"2023-03-08","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":3}}
{"title":"Testing Viterbi Algorithm for Hidden markov model pos tagger","description":"I am implementing the HMM model pos tagger using viterbi algorithm on the brown dataset from nltk. I have separated the data into train and test datasets, now for train dataset, I have calculated the emission and transition probability matrix. I have a few questions though.\n\n1. To calculate the accuracy, do we count the no. of correct tags on words or the no. of correctly tagged sentences? (my guess is it should be words)\n2. For testing data, I have some words which are not in the emission probability matrix, and hence for those sentences viterbi algorithm gives me an error. how do i handle this?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11luju0/testing_viterbi_algorithm_for_hidden_markov_model/","created":"2023-03-08","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":4}}
{"title":"How to get a Phd in NLP for protein/gene design ?","description":"I have a background in Biotechnology and am currently doing a MS in Bioinformatics. My research consists on natural language models like BERT and protein design I'm also working on data/text mining projects with Biomedical data.  I want to do a PHD  with a focus on NLP but I'm worried if I have enough knowhow to apply for them. Any suggestions how I should approach this?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11m5je0/how_to_get_a_phd_in_nlp_for_proteingene_design/","created":"2023-03-08","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":1}}
{"title":"Question about density plots for dimensionality reduced embeddings","description":"I have long form documents that each talk about a variety of topics (10+). The documents are split into paragraph, which is the unit (sub-) topics are talked about. For each paragraph an embedding is created via OpenAI (text-embedding-ada-002). Since the embeddings contain 1536 dimensions, I use UMAP to reduce it to two. \n\nWhat I would like to do is then use bivatiate kde plots via [seaborn](https://seaborn.pydata.org/tutorial/distributions.html) to compare the focus of the documents (each representing an organization) showing differences and commonalities. I don\u2018t have a strong background in mathematics but this part of the [documentation](https://umap-learn.readthedocs.io/en/latest/clustering.html) threw me a little of. While I am not directly clustering, the underlying idea seems similar enough to warrant caution. \n\nDoes anybody know if my idea (umap reduced embedding-&gt; kde plot) reasonably sound or have any pointers to fintune the approach?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11m3jgo/question_about_density_plots_for_dimensionality/","created":"2023-03-08","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Worth learning Python just for NLP if I have good grasp of R?","description":"Wanted to survey what people thought. I have a good amount of experience and feel comfortable with R having used it on various data analytic projects. \n\nApproaching my first NLP project. Do you all feel it is worth learning Python to do this specifically? I know there may be general arguments for learning Python (flexibility, etc.) but was wondering how different it is for NLP application.\n\nThanks!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11l6zu1/worth_learning_python_just_for_nlp_if_i_have_good/","created":"2023-03-07","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":13}}
{"title":"Options for BERT in Python vs. Pyspark","description":"Hi all,  I'm working on a project to improve the **selection of web pages where ads will be placed**. (ex: If the ad is for supplements for women place it on a page about... women's health and wellness. Pretty simple.)\n\nPreviously, this has been done using very basic keyword matching and/or the site's membership in a category that was pre-chosen by the customer. (External service provides categorization of site, customer chooses keywords/category they want to advertise on.) Very basic, context and word sense not considered.\n\nNow I'm trying to bring the system up to a modern approach. \n\n# My approach so far has been the following:\n\n* **Make Corpus Embeddings**\n   * Get the text of a bunch of the pages where an ad can be shown and **do TF-IDF** to find most relevant words.\n   * **Get embeddings** of all the page's words **from bert-base-uncased**\n   * Pull out **just those that are top 10 TD-IDF** and **average** to create a general embedding for that page (Two notes about this: This is actually done a little more efficiently than this but I'm trying to make it clear conceptually that I'm getting the embedding for the word in its original context. I'm adding the extra TF-IDF step because it seems to keep size/computation low and not sacrifice quality.)\n* **Make Example Site Embedding**\n   * **Get an example site from the customer** that they consider ideal to advertise on. Do the above on this site's text also.\n* **Find Pages Similar to Example**\n   * Do **cosine similarity** across the pages in the corpus to **find near neighbors to the example site** and advertise on those highest ranking pages where possible.\n\n# How to get this into pySpark?\n\nSo, this has all been great so far. The results look like we want them to look. But it's just been done on 70k rows of corpus sites, totally in Python. We're going to need to deal with a corpus of \\~10mil sites. That's not going to work in Python. There is a Hadoop cluster available that is accessible by PySpark, though.  \n\nSo we have **options**.\n\n* Put everything in a **UDF**, run same BERT package in UDF (not so efficient and coincidentally also not working at all due to a platform issue I won't explain here but basically **this won't work** so it's ruled out)\n* Switch the **TF-IDF to SparkML**, do the BERT **embeddings in SparkNLP** (this is how we're going about this now but it's still slow, not sure the cause yet)\n* **Forget the TF-IDF** efficiency step and just do BERT embeddings in SparkNLP, go eat cake and watch television!\n* **SOMETHING ELSE MUCH BETTER**\n\n# Can I do this better? How?\n\nThat brings me to my question. What would you do to approach this problem better? What's best for **storage efficiency, computational efficiency**? Would you go about it a totally different way entirely? How can I improve this approach?\n\nThanks for your advice!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11l7vuu/options_for_bert_in_python_vs_pyspark/","created":"2023-03-07","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"[HELP]","description":" I'm trying to build a pos tagger model for my native language and I found out that I need an annotated corpus for my model. my question is should I label each word with its POS as two columns one for the word and the second for the tag or what should I do?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11lgqzf/help/","created":"2023-03-08","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Swahili Translation Tool","description":"Hello all! I am a teacher and I am looking for an app or a website or something you all suggest to translate my assignments and letters home into Swahili and Arabic. If you have anything you would like to suggest, I would really appreciate it! Thank you all.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11l5mlt/swahili_translation_tool/","created":"2023-03-07","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":1}}
{"title":"We tracked mentions of OpenAI, Bing, and Bard across social media to find out who's the most talked about in Silicon Valley","description":"Have you been following the news on the conversational AI race? We used social media data and [geolocation models](https://github.com/1712n/yachay-public/tree/master/conf_geotagging_model) to find posts about OpenAI, Bing, and Bard in the Silicon Valley and San Francisco Bay Area for the last two weeks to see which one received the most mentions.\n\nFirst, we filtered social media data with the keywords \"openai,\" \"bing,\" \"bard,\" and then we predicted coordinates for the social media posts by using our text-based geolocation models. After selecting texts which received a confidence score higher than 0.8, we plotted their coordinates as company logos on a leaflet [map](https://1712n.github.io/yachay-public/maps/chatbots/) using Python and the folium library, restricting the map to the bounding box of the San Francisco Bay Area and Silicon Valley.\n\nWe analyzed over 300 social media posts and found that roughly 54.5% of the time, OpenAI was the most talked about. Bing made second place with around 27.2%, and then Bard came in last with 18.3%.\n\nOpenAI may be winning the AI race at the moment, but it's not the end yet. Let us know what other AI projects you're following, and we'll check them out.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11l3k5x/we_tracked_mentions_of_openai_bing_and_bard/","created":"2023-03-07","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
