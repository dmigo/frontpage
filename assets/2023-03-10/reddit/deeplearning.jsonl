{"title":"[POC] ChatGPT Audio Bot (like Google Assistant and Alexa) - Opensource","description":"Hey guys, just wanna share this bot that I quickly built using ChatGPT API.\n\nGitHub page: [https://github.com/LanyTek/ChatGPT\\_Audio\\_Bot](https://github.com/LanyTek/ChatGPT_Audio_Bot)\n\nThis service allows you to keep talking to the bot using your voice like you often do with Google Assistant or Alexa. It can be used in a lot of scenarios like teaching, gossiping, or quickly retrieving information without the need of typing. \n\nI have a quick video demo here if you wanna check [https://www.youtube.com/watch?v=e9n0BJfMyKw](https://www.youtube.com/watch?v=e9n0BJfMyKw)\n\nIt's open source so feel free to use it for anything you like :)","link":"https://www.reddit.com/r/deeplearning/comments/11nfrhw/poc_chatgpt_audio_bot_like_google_assistant_and/","created":"2023-03-10","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":2}}
{"title":"Approximately how long will it take to finish Transfer Learning?","description":"Hi there,\n\nI  have a multi-task transformer model that I would like to apply transfer  learning to. It is a multi-task model that takes offers \\~5,000 multi  task outputs. I am planning to add one linear layer to the end and  having it offer 50 multi-task outputs after transfer learning. If it  took \\~3 days to train the first model, and I have 800x additional training data for transfer learning, is there an easy way to tell how  long this should take? I suppose I am specifically wondering whether I  should expect it to take 838x as long if I use the same batch sizes  while training, or if decreasing the amount of tasks from \\~5000 to 50  helps decrease training time at all.\n\n&amp;#x200B;\n\nThanks in advance for helping a beginner!","link":"https://www.reddit.com/r/deeplearning/comments/11ne0nm/approximately_how_long_will_it_take_to_finish/","created":"2023-03-10","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":0}}
{"title":"[Tutorial] Image Classification using TensorFlow on Custom Dataset","description":"Image Classification using TensorFlow on Custom Dataset\n\n[https://debuggercafe.com/image-classification-using-tensorflow-on-custom-dataset/](https://debuggercafe.com/image-classification-using-tensorflow-on-custom-dataset/)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/g4b652622tma1.png?width=1000&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=a0053f6a050a64da6cd7250c5126b9e556f3dc28","link":"https://www.reddit.com/r/deeplearning/comments/11n8xhq/tutorial_image_classification_using_tensorflow_on/","created":"2023-03-10","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":0}}
{"title":"Stanford Webinar - The Frontier of Deep Learning for Robotics","description":"Join Professor Chelsea Finn in this discussion on modern deep reinforcement learning algorithms, and learn more about their usefulness towards solving ambitious challenges in [\\#Robotics](https://twitter.com/hashtag/Robotics?src=hashtag_click).   \n[Register Now](https://learn.stanford.edu/DeepLearningRobotics-2023-Registration.html?utm_source=reddit&amp;utm_medium=oa&amp;utm_campaign=webinar)\n\nhttps://preview.redd.it/m73tnq832rma1.jpg?width=720&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8622319607af5d5f2890323a909d6eadd70a7ded","link":"https://www.reddit.com/r/deeplearning/comments/11myc6e/stanford_webinar_the_frontier_of_deep_learning/","created":"2023-03-09","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":0}}
{"title":"AskReddit: which MBP Pro is future proof for AI apps development?","description":"Specifically to play with diffusion or gpt models, just generation and fine tuning, not large training.\n\nI gear towards MBP due to portability and build quality.\n\nAny advice is appreciated.\n\nPD: for my work, I have a 14\u201d M1 Pro but do GPU stuff (in-house vision models) on the cloud, so I don\u2019t know how practical it is for diffusion/gpt models.","link":"https://www.reddit.com/r/deeplearning/comments/11n8xrc/askreddit_which_mbp_pro_is_future_proof_for_ai/","created":"2023-03-10","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":6}}
{"title":"Image denoising using deep learning survey","description":"Hi everyone!\n\nI am a final year undergraduate following a BSc (Hons) Computer Science degree offered by the Informatics Institute of Technology, affiliated with the University of Westminster.\u00a0\n\nThis survey will be used to collect information for my final-year research project. The project's main goal is to develop **an image-denoising system that can remove noise from noisy images**.\n\n**\\*This survey is anonymous and confidential, and no personal information will be collected. By filling out the survey, you agree to let the data provided via answers be used for academic purposes.\\***\n\nI would appreciate it if you could complete the survey.\n\nI want to thank you in advance for your participation. If you have any questions or suggestions, please don't hesitate to contact me.  \n\n\n[https://forms.gle/TDbcEqUfYi8XL3hu8](https://forms.gle/TDbcEqUfYi8XL3hu8)","link":"https://www.reddit.com/r/deeplearning/comments/11mrz59/image_denoising_using_deep_learning_survey/","created":"2023-03-09","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":1}}
{"title":"Can feature engineering avoid overfitting?","description":" Can feature engineering avoid overfitting? If yes, are there any relevant papers that state this?","link":"https://www.reddit.com/r/deeplearning/comments/11mokqu/can_feature_engineering_avoid_overfitting/","created":"2023-03-09","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":14}}
{"title":"PyTorch Faster RCNN Library - Support for transformer detection models.","description":"[https://github.com/sovit-123/fasterrcnn-pytorch-training-pipeline](https://github.com/sovit-123/fasterrcnn-pytorch-training-pipeline)\n\nNow, the library supports Faster RCNN ViTDet and Faster RCNN MobileViT\\_XXS also.\n\nWould love to get feedback/contributions/suggestions.","link":"https://www.reddit.com/r/deeplearning/comments/11mhkpd/pytorch_faster_rcnn_library_support_for/","created":"2023-03-09","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":0}}
{"title":"Do you use synthetic data in your projects?","description":"&amp;#x200B;\n\nhttps://preview.redd.it/vl7j0i04zpma1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=30a37b6c563173e47ac5d9d9b5fd6c74fc7348e9\n\nHi all!\n\nMy name is Vadim, I work in [OpenCV.ai](https://OpenCV.ai). We provide consulting services in the field of Computer Vision and AI. Now we work on a new tool for creating photorealistic synthetic data. \n\nWe eager to know what problems you most usually face while using it or why you don't use it. Your experience is extremely valuable for us. If you are open to discuss it, please write a private message to gleb.tuzov@opencv.ai or leave a comment. \n\nThank you!","link":"https://www.reddit.com/r/deeplearning/comments/11mswj6/do_you_use_synthetic_data_in_your_projects/","created":"2023-03-09","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":0}}
{"title":"Build the BEST Data Science Resume with Quadruple Kaggle Grandmaster","description":"Here's an interview with Chris Deotte, Quadruple Kaggle Grandmaster at NVIDIA. \n\nIn this episode, Chris shares valuable insights on topics such as crafting a strong data science resume, achieving grandmaster status on Kaggle (even quadruple), working at NVIDIA, and how to approach current data science challenges. Learn more about Kaggle, the data science world, and NVIDIA through the fascinating story of Chris Deotte. (and win an RTX 4080 thanks to NVIDIA GTC collaboration!)\n\nListen to this week's episode on your favorite platform: \n\n[https://youtu.be/NjGnnG3evmE](https://youtu.be/NjGnnG3evmE)\n\n[https://podcasts.apple.com/us/podcast/whats-ai-by-louis-bouchard/id1675099708?i=1000603382690](https://podcasts.apple.com/us/podcast/whats-ai-by-louis-bouchard/id1675099708?i=1000603382690)\n\n[https://podcasters.spotify.com/pod/show/louis-bouchard/episodes/How-to-Build-a-strong-Data-Science-Resume--With-Chris-Deotte--Quadruple-Kaggle-Grandmaster-at-NVIDIA-and-win-an-RTX-4080-e203a7t/a-a9f73dt](https://podcasters.spotify.com/pod/show/louis-bouchard/episodes/How-to-Build-a-strong-Data-Science-Resume--With-Chris-Deotte--Quadruple-Kaggle-Grandmaster-at-NVIDIA-and-win-an-RTX-4080-e203a7t/a-a9f73dt)","link":"https://www.reddit.com/r/deeplearning/comments/11mhur7/build_the_best_data_science_resume_with_quadruple/","created":"2023-03-09","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":0}}
{"title":"Parameter values of diffusion models","description":"Hi everyone!\n\nI have a question about diffusion models from the paper \"Denoising Diffusion Probabilistic Models\" by Ho. et al. ([https://arxiv.org/pdf/2006.11239.pdf](https://arxiv.org/pdf/2006.11239.pdf)). They chose not to train \u03a3\\_\u03b8(x\\_t, t) but setting it equal to  \u03c3\\_t\\^2 I, and then they experiment with two different values on \u03c3\\_t\\^2, namely \u03b2\\_t and \\\\tilde{\u03b2}\\_t. The first choice is optimal for x\\_0 \u223c N(0, I), and the second is optimal for x\\_0 deterministically set to one point, why is that? Does anyone have a good explanation and/or derivation of that.","link":"https://www.reddit.com/r/deeplearning/comments/11m0kvf/parameter_values_of_diffusion_models/","created":"2023-03-08","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":1}}
{"title":"AI that plays a video game","description":"How would I make an AI that gathers resources in a game like ark? Thank you, I am new to this","link":"https://www.reddit.com/r/deeplearning/comments/11majq4/ai_that_plays_a_video_game/","created":"2023-03-08","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":3}}
{"title":"Weaviate Vector DB adds support for Product Quantization, Bitmap Filters, Filtered Hybrid Search, Tunable Consistency, and more in the v1.18 release.","description":"Ever since Chat-GPT has hit the masses, the interest in vector search has gone through the roof. Weaviate takes an end2end approach to vector search because it also stores the data object, and builds inverted indexes besides the vector indexes.  \n\n\nYesterday, version `v1.18.0` was released, with the following features that were in high demand by the community:\n\n# Product Quantization\n\nWeaviate v1.18 allows compressing vector embeddings using Product Quantization in combination with HNSW vector indexing (HNSW-PQ). This allows for a lower memory footprint while keeping low latency and high recall\n\n# Bitmap Filtering\n\nWeaviate's inverted index is now built natively on top of roaring bitmaps. This allows for very fast filtered vector search even at the 100M or billion scale. In some extreme cases, search latencies went down from 5s to 5ms.\n\n# Filtered Hybrid Search\n\nWeaviate v1.17 added support for Hybrid (BM25 sparse + Vector Dense) search. However, it did not (yet) allow for setting filters on Hybrid Search queries. This is now possible with v1.18\n\n# BM25 WAND Scoring\n\nWeak-AND (\"WAND\") is a BM25 scoring algorithm that avoids scoring documents that cannot reach a high enough score to be contained in the result set. This speeds up BM25 \u2013\u00a0and in turn \u2013 hybrid search\n\n# Tunable Consistency and Automatic Repairs\n\nA previous Weaviate release added support for High-Availability through Replication. However, the desired level of consistency when reading and writing was set by Weaviate. Now, the user can set these settings according to their preferences. In addition, if Weaviate detects an inconsistency (e.g. after a temporary node failure) it can now be repaired automatically when reading the \"corrupt\" object.\n\n# Cursor API\n\nIn previous Weaviate releases, it was impossible to export all objects from Weaviate because of the increasing cost of each page on pagination. The new cursor API provides a constant-cost way to extract all objects (and their vector embeddings) from Weaviate.\n\n# Azure Backup Module\n\nIn addition to Google Cloud Storage, and Amazon S3, Weaviate now supports Azure Blob storage for seamless backups and restores.\n\n\\---\n\nMore information:\n\n* [Release blog post](https://weaviate.io/blog/weaviate-1-18-release)\n* [Release on GitHub](https://github.com/weaviate/weaviate/releases/tag/v1.18.0)\n\nDisclaimer: I am a co-founder of Weaviate.","link":"https://www.reddit.com/r/deeplearning/comments/11lsal6/weaviate_vector_db_adds_support_for_product/","created":"2023-03-08","tags":["reddit","deeplearning","ml"],"meta":{"num_comments":2}}
