{"title":"Detect cracks and scratches on microchips..","description":"Hello guys,\n\ni need to classify images of microchips, which have cracks and scratches on them. I want to classify them in good and bad.\n\nThe dataset consist 4 classes and about 3000 images. The Classes are microchip with cooler good/bad and microchip without cooler good/bad.\n\nThe Images are all black/white and have such a structure like in the image i posted. Is it possible to classify this with a CNN and furthermore how could i achieve to highlight the scratched.. like paint the scratch in blue or red or something? Is that possible to achieve? This entire task is for my bachelorthesis and i search ideas on how to solve this with neural networks..\n\n[https://imgur.com/a/v0GkcAZ](https://imgur.com/a/v0GkcAZ)\n\n&amp;#x200B;","link":"https://www.reddit.com/r/deeplearning/comments/11sp5ga/detect_cracks_and_scratches_on_microchips/","created":"2023-03-16","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"How To Fine-tune LLaMA Models, Smaller Models With Performance Of GPT3","description":"Recently, the LLaMA models by Meta were released. What makes these models so exciting, is that despite being small enough to run on consumer hardware, popular metrics show that the models perform as well or better than GPT3 despite being over 10X smaller!\n\nThe reason for this increased performance seems to be due to a larger number of tokens being used for training.\n\nNow, following along with the video tutorial and open-source code, you can now fine-tune these powerful models on your own dataset to further increase the ability of these models!\n\n[https://youtu.be/d4Cnv\\_g3GiI](https://youtu.be/d4Cnv_g3GiI)","link":"https://www.reddit.com/r/deeplearning/comments/11ryc3s/how_to_finetune_llama_models_smaller_models_with/","created":"2023-03-15","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":7}}
{"title":"How do I prepare for the Microsoft Exams?","description":"","link":"https://www.reddit.com/r/deeplearning/comments/11snji6/how_do_i_prepare_for_the_microsoft_exams/","created":"2023-03-16","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"[P] We are building a curated list of awesome curated list closely related to machine learning, looking for contributions.","description":"Hey r/MachineLearning,\n\nWe are collecting a hand-crafted curated list of awesome curated lists closely related to machine learning.\n\nHere is the link to the Github repo: [https://github.com/zhimin-z/awesome-awesome-machine-learning](https://github.com/zhimin-z/awesome-awesome-machine-learning)\n\nDo any lists need to be included from your perspective? Please let me know, or feel free to submit a pull request.\n\nThe motivation underlying this project is that so many awesome lists regarding machine learning exist on GitHub. But, gradually, it adds a mental burden to memorize where to look for when the ML world is progressing faster and faster these days.\n\nThus, there the project comes, as a unification to sew together all awesome lists closely related to machine learning.","link":"https://www.reddit.com/r/deeplearning/comments/11schoa/p_we_are_building_a_curated_list_of_awesome/","created":"2023-03-15","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"How to Use mpirun to Launch a LLaMA Inference Job Across Multiple Cloud Instances","description":"[How to Use mpirun to Launch a LLaMA Inference Job Across Multiple Cloud Instances](https://lambdalabs.com/blog/how-to-use-mpirun-to-launch-a-llama-inference-job-across-multiple-cloud-instances?utm_source=reddit&amp;utm_medium=organic-social&amp;utm_campaign=2023-03-llama-github-repo&amp;utm_content=blog)\n\nGuide on how to use mpirun to launch a LLaMA inference job across multiple Lambda Cloud instances, including a cost analysis for running various LLaMA models on different GPU instances. Notable updates include:\n\n* A script to easily set up a \"cluster\" of cloud instances that is ready to run LLaMA inference (all models from 7B to 65B).\n* mpirun compatible, so you can launch the job directly from the head node without the need of typing in the torchrun command on the worker nodes.\n* Interactive inference mode across multiple nodes.\n* eos\\_w: controls how \"lengthy\" the results are likely to be by scaling the probability of eos\\_token\n* Inference speed profiling (\"tokens/sec\").","link":"https://www.reddit.com/r/deeplearning/comments/11s4u0b/how_to_use_mpirun_to_launch_a_llama_inference_job/","created":"2023-03-15","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"Sliding Window on time serie create too big dataset","description":"Hello, \n\nI have a time serie dataset and when splitting it using sliding windows it generates me over 13 millions samples, which takes too long to train.  \n\n  \nDo I absolutely need to use sliding windows or can I simply split each sequence into multiple non-overlapping samples ?  (I'm using LSTM bidirectional layers)  \nDo you have any advice apart from changing sliding stride ? \n\nMany thanks, this is my first time serie project :)","link":"https://www.reddit.com/r/deeplearning/comments/11say4l/sliding_window_on_time_serie_create_too_big/","created":"2023-03-15","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":5}}
{"title":"Building a deep learning model for bug severity prediction","description":"Can a deep learning model be built to classify bugs as blocker, minor, major, critical, trivial, or normal? \n\nor only binary classification as **severe** \\[if the bug is blocker, major or critical\\] and **non-severe** \\[if the bug is minor, trivial or normal\\]. \n\nas I tried many models like gpt-2 but didn't get an accuracy greater than 0.2, but when making it binary classification task I got accuracy &gt; 0.65. \n\n&amp;#x200B;\n\nPlease advise. is it possible to do the multiclass classification?","link":"https://www.reddit.com/r/deeplearning/comments/11s90au/building_a_deep_learning_model_for_bug_severity/","created":"2023-03-15","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"Transformer models: if token embeddings are trainable params, why doesn't training cause every token to be mapped to the same vector?","description":"Wouldn't the model have incredibly low loss if every token was the same? What stops this from happening?","link":"https://www.reddit.com/r/deeplearning/comments/11rqtpm/transformer_models_if_token_embeddings_are/","created":"2023-03-15","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":5}}
{"title":"image to image","description":"Hi!\n\nI'm a bachelor student on my second year in AI. \n\nI'm planning to implement an image to image model that takes and image and outputs the same image in a different style (specifically in the style of a painter).\n\nI was wondering if anyone had some suggesting of where to start research and pointing me in the right direction.\n\nBest regards\n\nMarius","link":"https://www.reddit.com/r/deeplearning/comments/11rs817/image_to_image/","created":"2023-03-15","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":6}}
{"title":"How does Donut extract precise text without OCR?","description":"I've stumbled upon [this paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880493.pdf) and a couple of others that basically discuss an alternative approach (Donut) for Visual Document Understanding (VDU).\n\nThe conventional and common approach (like what's done by LayoutLM) is to first perform OCR on the input image (with potential text block recognition beforehand), then post-process the output text. Donut's premise is to basically cut out the OCR step and process end-to-end in one pass.\n\nMy question is simply how does the text extraction happen in that case? how can text be extracted with such precision without OCR or some other form of optical text recognition?\n\nI went through the paper and a handful of articles explaining it, but the concept as a whole is still quite baffling to me and it all sounds like \"you can see without your eyes\" at this point x)","link":"https://www.reddit.com/r/deeplearning/comments/11rc2oh/how_does_donut_extract_precise_text_without_ocr/","created":"2023-03-14","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":1}}
{"title":"Hilarious mistake by a friend","description":"I thought this conversation may spread some laughs. A friend is just starting to learn about CNNs in a course and comes to me for help sometimes. Hilarity ensued this morning when he tried to add some new functionality to his CNN on the Caltech dataset and mistook weight decay for learning rate decay.\n\nhttps://preview.redd.it/5hvcc817gqna1.png?width=455&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=30eaba5feea732dd60b5f426b1dcd1cae9efd6d8\n\nhttps://preview.redd.it/xej27laagqna1.png?width=438&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ce46be4bef86e6ed7e56c49a46db396d67ca5755\n\n&amp;#x200B;\n\nhttps://preview.redd.it/nv4m4hqagqna1.png?width=449&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=da8f9d0981c6a1d07fad04db40d81516cfa6bd21\n\nhttps://preview.redd.it/ouphexgbgqna1.png?width=445&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c4f1e2d71e19420f9b10b5c4f9df22323c885bdd","link":"https://www.reddit.com/r/deeplearning/comments/11rb5eo/hilarious_mistake_by_a_friend/","created":"2023-03-14","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":2}}
{"title":"Research opportunity","description":"Hey all, I came across Fatima Fellowship on LinkedIn (not sure if links are allowed here so I won't post it but you can just google it up). They provide research opportunities for Machine Learning and I guess related areas. It says that it's free and works as a non-profit. Thought I would share here incase anyone is looking for research chances.","link":"https://www.reddit.com/r/deeplearning/comments/11rfapy/research_opportunity/","created":"2023-03-14","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"Question on study options","description":"I started my career as a quant then programmer, then data scientist and now work for Bloomberg.\n\nI've been using ML for years but have not really worked with NLP and with the recent advances in LLMs the penny dropped that our working world is about to start changing very quickly.\n\nAre there any AI MSc degrees that are aligned to this space that are open to part time study?\n\nOr, should I just dive into the books as the MSc would not be specific enough.\n\nI did an MSc in quant mathematics a few years ago after a break of 20 years from my Physics BSc and found it pretty broad and tbh not all that useful.\n\nAnyway. Just seeing what people's thoughts are \n\nCheers","link":"https://www.reddit.com/r/deeplearning/comments/11r0l52/question_on_study_options/","created":"2023-03-14","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":3}}
{"title":"What are some ways to teach myself new skills?","description":"","link":"https://www.reddit.com/r/deeplearning/comments/11r65oj/what_are_some_ways_to_teach_myself_new_skills/","created":"2023-03-14","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":4}}
