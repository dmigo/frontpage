{"title":"Question for use of ML in adaptive authentication","description":"Hi all, I'm looking for advice for using ML for Adaptive Authentication.\n\nThe use case is that I want to generate a unique identifier key from user bahavior. eg: Sam uses my app and I want to generate key 1234, Mel uses the app, her key is 2351, etc\n\nTo generate this key I thought I could use an ML model that takes as input user behavior data and outputs this key or something I can use to derive a key.\n\nTaking typing on a smartphone as an example: a user types 10 words on their keyboard, we take data from that and feed it to the model to generate the key for this user. The data we take might be something like speed of typing a letter, time fingers were pressed on keys, number of times they used backspace, etc...\n\nIs this possible? I'm not an ML specialist so my knowledge is limited, but I was thinking we could do something like using a classifier with 10 categories, and use some statistical value from the output equivalent to prediction accuracy or prediction certainty for each category to generate numbers out of the classifications... but that seems like a hack and there may be something more precise and standard","link":"https://www.reddit.com/r/deeplearning/comments/11zcc1a/question_for_use_of_ml_in_adaptive_authentication/","created":"2023-03-23","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":18}}
{"title":"Comic Text Effect","description":"Comic Cartoon Text Effect in Canva \n\n[Tutorial link](https://youtu.be/ijVu0cnJbh0)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/h3noa0yeecpa1.png?width=1280&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3151dc891e906988cc81b77e2f26c9dd53d18c13","link":"https://www.reddit.com/r/deeplearning/comments/11ytv4g/comic_text_effect/","created":"2023-03-22","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"Anyone have any good alternatives to Paperspace? My account got closed for unauthorized access.","description":"I've been using Paperspace Gradient Growth recently and really liked it. Their shutdown after 6 hours was annoying, but bearable. And the access to A5000s, A6000s, and A100s for $40 per month was pretty fantastic.\n\nUnfortunately, my account was closed. I was using it to train a Chess engine and I installed Stockfish, which is apparently not allowed. I contacted them because I was having issues logging in and they told me that they \"detected unauthorized activities\" and explained that I needed to upgrade my account to Paperspace Core to use it.\n\nThe 2 most popular services I've seen here are probably vast.ai and podrun, but I don't want to pay hourly. I'd prefer something with persistent storage so I don't need to be constantly processing or uploading large amounts of data.\n\nThere's Google Colab Pro, of course. But I don't like their lack of transparency in regards to pricing with their credit system.\n\nEdit: Title should say \"unauthorized activities.\"","link":"https://www.reddit.com/r/deeplearning/comments/11ys19h/anyone_have_any_good_alternatives_to_paperspace/","created":"2023-03-22","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":2}}
{"title":"Using CharBERT for text similarity","description":"Hi all!\n\nI'm looking into [CharBERT](https://arxiv.org/abs/2011.01513) for an university project, and I noticed that it was finetuned on many tasks like sentiment analysis, NER, and so on. I tried to use it to do text similarity by using only the pretrained version the authors give + a cosine similarity algorithm between word embeddings: is this the way to go? Should I treat the embeddings the model gives in some way before calculating the cosine distance?","link":"https://www.reddit.com/r/deeplearning/comments/11ycciy/using_charbert_for_text_similarity/","created":"2023-03-22","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":1}}
{"title":"[Pytorch] How do you efficiently keep in memory the attention weights in an autoregressive transformer","description":"Hi when I do an inference (not training) of my autoregressive transformer I do it substantially this way (I removed few lines to not affect readibility):\n\n    for i in range(max_batch_sequence_len):\n        for layer in self.layers:\n            y[:, i] = layer(x, keep_mask, y)[:, i]\n\nwhere my layers \"forward' are:\n\n    def forward(self, x: torch.Tensor, keep_mask: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n            attn_mask = (~keep_mask).unsqueeze(2) &amp; (~keep_mask).unsqueeze(2)\n            attn_mask = attn_mask.repeat_interleave(self.num_heads, dim=0)\n                \n            y_normed = self.layer_norm(y)\n            y = y + self.self_attn(y_normed) #A causal mask is applied\n    \n            x_normed = self.layer_norm(x)\n            y_normed = self.layer_norm(y)\n            y = y + self.cross_attn(y_normed, x_normed, attn_mask)\n            \n            y_normed = self.layer_norm(y)\n            y = y + self.ffn(y_normed)\n            return y\n    \n    def self_attn(self, y):\n            out, _ = self.attn1(\n                query=y, key=y, value=y, need_weights=False, is_causal=True,\n            )\n            return out\n    \n    def cross_attn(self, y, x, attn_mask):\n    \n        out, _ = self.attn2(\n            query=y, key=x, value=x, need_weights=False, attn_mask=attn_mask\n        )\n        return out\n\nI can see that using the attention weights and re-inputing them in a certain way I can manage to reduce the computation, especially at step i+1 the attention weights for j&lt;=i have all been already computed.  \n\n\nHas someone here have ever dealt with that and can suggest me a modification of my code?","link":"https://www.reddit.com/r/deeplearning/comments/11ypnr0/pytorch_how_do_you_efficiently_keep_in_memory_the/","created":"2023-03-22","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"How to handle multiple languages in a sentence?","description":"Hi everyone I am having a task where I have to use product\\_title for making recommendations. But the challenge is that some product\\_titles are present in multiple languages. Now how to get embeddings for such product titles.  \n\nThe product title are mostly like:     \n\n   1. English only    \n\n   2. Japanese+ english     \n\n   3. German + english","link":"https://www.reddit.com/r/deeplearning/comments/11yp9g2/how_to_handle_multiple_languages_in_a_sentence/","created":"2023-03-22","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":1}}
{"title":"Is a GAN being able to generate realistic data analogous to it learning the underlying data generation mechanism of the input?","description":"If a specific GAN can be proven to have learned the underlying data distribution, can it be said that it has learned the mechanisms that generate the input data? I'm trying to find sources on this but am struggling so any help would be great","link":"https://www.reddit.com/r/deeplearning/comments/11yjmwk/is_a_gan_being_able_to_generate_realistic_data/","created":"2023-03-22","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":1}}
{"title":"Realism of GAN-Generated Terrains - Survey","description":"Hi there!\n\nI am currently conducting research as part of my Master's program in Game Technology, with a focus on terrain generation using GANs. Specifically, I aim to enhance the methods employed in previous studies.\n\nAs part of my research, I have created a survey that presents participants with a side-by-side comparison of a terrain generated by the GAN used in my study and one generated by a GAN from an earlier study. The survey requires participants to choose which terrain appears more realistic to them. Please note that the survey takes approximately 15 minutes to complete. You can access the survey via this link: [https://buas.eu.qualtrics.com/jfe/form/SV\\_cSiYjJi08Oot2bY](https://buas.eu.qualtrics.com/jfe/form/SV_cSiYjJi08Oot2bY)\n\nI would be grateful if members of the Reddit Deep Learning community could take time to complete the survey. Please feel free to reach out to me with any questions you may have about my research or the survey. Thank you for your valuable time and consideration.","link":"https://www.reddit.com/r/deeplearning/comments/11yiive/realism_of_gangenerated_terrains_survey/","created":"2023-03-22","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"Training on distributed system/ own cluster","description":"Hi Reddit,\nIs there a way to increase training speed of a own model by putting it on several consumer computers / laptops?\nOr in other words can i set up an own sort of cluster for LLM training/finetuning?\nAnyone give me some hints?","link":"https://www.reddit.com/r/deeplearning/comments/11ybkl6/training_on_distributed_system_own_cluster/","created":"2023-03-22","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":3}}
{"title":"Minimizing model training stability","description":"Hello good people of reddit,  \n\n\ni've been developing a neural net for some time and im currently reaching sufficiently low RMSE in some cases. What im having an issue is the deviation of rmse during multiple training runs. This is showing to be very problematic since in some cases i get three times the desirable RMSE.  I applied most of the techniques i can imagine could help with this but non of it really works. I normalize the input data, use dropout, batch normalization as well as decaying learning rate and weight initializers.  \n\n\nIs there any other technique that im missing that could help with it ?","link":"https://www.reddit.com/r/deeplearning/comments/11ye16q/minimizing_model_training_stability/","created":"2023-03-22","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"Reduced Memory Usage with Burn: A Deep Learning Framework written in Rust","description":"I announced last year on the Rust subreddit Burn, the deep learning framework I'm building in Rust.\n\nWhile building machine learning tools in a language other than Python goes against the trend, I humbly believe it is a promising avenue. There has been a lot of work since the last release, and now we're starting to see some benefits. Burn uses less memory, especially on the CPU during both inference and training than PyTorch with a similar computational graph. I wrote a technical blog post about it, describing how Burn allows for the reuse of tensor-allocated memory ([**https://burn-rs.github.io/blog/burn-rusty-approach-to-tensor-handling**](https://burn-rs.github.io/blog/burn-rusty-approach-to-tensor-handling)).\n\nThere is still a lot more work to be done before being really competitive with other frameworks, notably properly supporting operation fusion. But Burn is still usable today, and you can even run inference in the browser using WebAssembly ([**https://burn-rs.github.io/demo**](https://burn-rs.github.io/demo)).  \n\n\nIf you have any questions regarding the blog, Rust, or Burn, I'm happy to answer them below.","link":"https://www.reddit.com/r/deeplearning/comments/11xtmnf/reduced_memory_usage_with_burn_a_deep_learning/","created":"2023-03-21","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"Alpaca Turbo : A chat interface to interact with alpaca models with history and context","description":"So I made this chat UI that will help you use alpaca models to have coherent conversations with history and the bot will remember your previous questions  \n\n\nHere is the demo  \n\n\nyou can get the interface from [https://github.com/ViperX7/Alpaca-Turbo](https://github.com/ViperX7/Alpaca-Turbo)  \n \n\nhttps://reddit.com/link/11xdx3w/video/o7jpmysvt2pa1/player","link":"https://www.reddit.com/r/deeplearning/comments/11xdx3w/alpaca_turbo_a_chat_interface_to_interact_with/","created":"2023-03-21","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":19}}
{"title":"It would be cool if there was a machine learning Nes Emulator, that the ai could learn to play automatically and you just run it on your pc till it finds the optimum root.","description":"","link":"https://www.reddit.com/r/deeplearning/comments/11xx3r6/it_would_be_cool_if_there_was_a_machine_learning/","created":"2023-03-21","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":3}}
{"title":"Anyone knows what is the minimum requirement to run the dalai alpaca 7B? Does the CPU matter or GPU?","description":"Can I run it using a machine i7 from 2014 with 16gb ram?","link":"https://www.reddit.com/r/deeplearning/comments/11xpyrh/anyone_knows_what_is_the_minimum_requirement_to/","created":"2023-03-21","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":2}}
