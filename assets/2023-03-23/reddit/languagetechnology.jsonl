{"title":"I need guidance on the feasibility or scoping of a project","description":"Hello everyone, for my work I'm assigned to make a system, which, for a given job ad will produce the necessary skills and responsibilities needed to do the job.  \nFor example, let's say, I'm giving the content of a  job ad to chatGPT and ask \" what skills are needed to do this job? \". And it replied:\n\n1. Good understanding of Oracle/Sybase/MSSQL database architecture\n2. Hands-on experience in database administration, including backup and recovery using RMAN\n3. Experience with Oracle GRID Control, ASM, Recovery Manager, Import / Export, Datapump, SQL Server administration, and clustering management\n4. Working knowledge of Control\\_M jobs\n5. Good understanding of High Availability (HA) concepts such as Always On and Clustering.  \n\n\n.... The list contains a couple more instances.  \n\n\n  \nNow, I want to phrase it as a QA system to make just this. To train it, I will have jobs\\_ad... and the fixed\\_question \"What are the skills/responsibilities to do this job\" and the context answer will be manually labeled. I'm thinking of fine-tuning \"distilbert-base-cased-distilled-squad\" with randomly sampled 5000 labeled instances.  \n\n\nI need suggestions on the feasibility of this. Has anyone built this kind of system? Any suggestion on phrasing the solution differently? Any feedback is really appreciated.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11zblqp/i_need_guidance_on_the_feasibility_or_scoping_of/","created":"2023-03-23","tags":["reddit","ml","languagetechnology"],"meta":{"num_comments":0}}
{"title":"Rare/unusual words extraction","description":"I want to get a list of the rare words (probably these that are not encountered on a normal basis, speaking in language-acquisition terms, words that are known for C2 speakers of the language or native speakers) from some text.\n\nWhat i thought about so far is just going through some frequency lists (like this one [http://corpus.leeds.ac.uk/serge/kelly/](http://corpus.leeds.ac.uk/serge/kelly/) or wikipedia frequency lists, or even everything combined), but this sounds like brute-forcing and something that would not entirely accurate. Are there any good pre-trained models classifying the rarity of words?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11z0hyk/rareunusual_words_extraction/","created":"2023-03-22","tags":["reddit","ml","languagetechnology"],"meta":{"num_comments":2}}
{"title":"best way to do Topic modeling for short texts?","description":"Hello, \n\nwhat's the best topic modeling technique to identify topics in short texts? I have a data set where textes are composed of around 10 to 60 words ! I tried LDA, TopicBERT and GDSMM. the results were all bad. My texts are in french by the way.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11ykwu1/best_way_to_do_topic_modeling_for_short_texts/","created":"2023-03-22","tags":["reddit","ml","languagetechnology"],"meta":{"num_comments":3}}
{"title":"How do we find Values in Attention, or do we need them at all?","description":"Hi everyone, I'm a bit confused about value(v) parameter in attention. At some posts after calculating attention scores they don't introduce any value(v) parameter. Sometimes they just copy and use the same values as key parameter.\n\n&amp;#x200B;\n\nI'm watching the lecture: [https://www.youtube.com/watch?v=0PPzD4mxpuM&amp;list=PL8PYTP1V4I8D0UkqW2fEhgLrnlDW9QK7z&amp;index=7](https://www.youtube.com/watch?v=0PPzD4mxpuM&amp;list=PL8PYTP1V4I8D0UkqW2fEhgLrnlDW9QK7z&amp;index=7)\n\n&amp;#x200B;\n\n1st part: We calculate attention scores. For this Seq2Seq model we take each encoder state as query vector. We take each decoder state as key vector. We choose a attention score function and calculate attention.\n\n[https://imgur.com/a/HcameRc](https://imgur.com/a/HcameRc)\n\n&amp;#x200B;\n\n2nd part: Point I get confused. We treat the value vectors same as key vectors and we do a multiplication with the attention scores if I understood correctly.\n\n[https://imgur.com/a/S7lYDGl](https://imgur.com/a/S7lYDGl)\n\n&amp;#x200B;\n\nI understand different attention papers implement differently. **But if we're going to use the value vectors same as key vectors why do we need it in the first place?** \n\nI've been trying to understand this key, query, value triplet. Read papers, posts, implementations. The database analogies but I couldn't get the intuition behind. I would be appreciated to any insights.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11ybmdd/how_do_we_find_values_in_attention_or_do_we_need/","created":"2023-03-22","tags":["reddit","ml","languagetechnology"],"meta":{"num_comments":1}}
{"title":"Txtai RuntimeError: failed to import","description":"I made a semantic search engine using txtai and it has stopped working. So I\u2019m wondering if it is to do with package versions. Any advice would be brilliant\n\n[error and line where it failed](https://imgur.com/a/1ulj2KS)","link":"https://www.reddit.com/r/LanguageTechnology/comments/11yf8be/txtai_runtimeerror_failed_to_import/","created":"2023-03-22","tags":["reddit","ml","languagetechnology"],"meta":{"num_comments":5}}
{"title":"Is there any value in continuing research on LSTM behavior?","description":"I've been doing research on the generalization capacity of LSTMs (e.g. stratifying training data based on different factors and seeing effects on performance) and although I've observed some interesting results, I can't help but just feel defeated at this point since now Transformers Are All We Need. My advisor has said that there is still value in LSTM research that might be of interest to the cognitive science community if we treat LSTMs as a cognitive model of language learning, but honestly the remarkable capacity of LLMs for humanlike language generation has me doubting even that. I want to believe there are reasons to keep going but it's been difficult to make a case for it, and it's hard for me to transfer the work I've been doing to transformers because with a large enough model we don't even observe these kinds of effects at all, or they just pattern with human behavior.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11xwk4p/is_there_any_value_in_continuing_research_on_lstm/","created":"2023-03-21","tags":["reddit","ml","languagetechnology"],"meta":{"num_comments":6}}
{"title":"CU Boulder or Brandeis for compling MS?","description":"I was admitted to both CU Boulder and Brandeis for their computational linguistics masters programs. I\u2019m leaning quite heavily toward CU for a few reasons, but just from an academic and professional standpoint, does anyone have any insight of which of those might be a more solid choice and program if all else were equal?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11y0wqq/cu_boulder_or_brandeis_for_compling_ms/","created":"2023-03-22","tags":["reddit","ml","languagetechnology"],"meta":{"num_comments":2}}
{"title":"Whisper Open AI: How to not include silences in the timestamps returned?","description":"Hello\n\n&amp;#x200B;\n\nI'm using Whisper, the timestamps includes the silence.\n\nWhen having a video with a speaker starting his speech at sec 10, I'm getting the first timestamp to be at sec 1. instead of sec 10.\n\nHere is my config:\n\nPOST/ v1/audio/transcriptions\n\nConfig\n\n\\`\\`\\`\n\n{\n\nmodel:\"whisper-1\"\n\nfile:\"...mp3\"\n\nresponse\\_format:\"srt\",\n\nprompt:\"Hello, welcome to my lecture\"\n\n}\n\n&amp;#x200B;\n\n\\`\\`\\`\n\n&amp;#x200B;\n\nOutput:\n\n\\`\\`\\`\n\n1\n\n00:00:01,000 --&gt; 00:00:14,000\n\nWhy are there both successful and struggling entrepreneurs?\n\n&amp;#x200B;\n\n2\n\n00:00:15,000 --&gt; 00:00:23,000\n\nMany customers prefer to watch videos to enjoy online content.\n\n&amp;#x200B;\n\n3\n\n00:00:24,000 --&gt; 00:00:32,000\n\nan other sentences.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n\\`\\`\\`\n\n&amp;#x200B;\n\n\\* I believe  \\`1\\` it should be \\`00:00:10,000 --&gt; 00:00:14,000\\`, since there is no one talking at all for 10 sec.\n\n\\* Also, the \\`3\\`, the speakers starts again talking at sec 28, but I'm getting the timestamp to be at sec 24. The silence is simply included in the timestamp with Whisper\n\n&amp;#x200B;\n\nAny idea how I could fix that, maybe using a prompt?\n\n&amp;#x200B;\n\nThanks!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11xdnvd/whisper_open_ai_how_to_not_include_silences_in/","created":"2023-03-21","tags":["reddit","ml","languagetechnology"],"meta":{"num_comments":3}}
{"title":"Is there any literature or courses on how to build datasets from scratch to train language models?","description":"Hi, everyone! I'm looking to get better at creating datasets to train/fine-tune different language models (mostly Transformers) for very specific tasks. I'm currently putting together a dataset from different social media sources and tagging it manually, and throughout the process my team and I had to make a lot of choices that were more guided by instinct than by theory.\n\nTherefore, I would be interested in any book/course that covers one or more of the following (or other relevant) topics for dataset creation:\n\n&amp;#x200B;\n\n\\- How to determine which data sources to use.\n\n\\- How to access the data I need (and automation if possible).\n\n\\- How to check for biases.\n\n\\- How to balance the dataset for different tasks.\n\n\\- Tagging techniques/tools.\n\n\\- Good practices/industry standards.\n\n\\- Any other topic you consider important or key for this task.\n\n&amp;#x200B;\n\nThanks in advance to all! Looking forward to reading from all of you.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11xhzq8/is_there_any_literature_or_courses_on_how_to/","created":"2023-03-21","tags":["reddit","ml","languagetechnology"],"meta":{"num_comments":0}}
{"title":"Are there any pretrained sentiment analysis models that grade sentiment along a gradient?","description":"I'm not sure if I've phrased my title correctly, but what I mean is, the majority of the models I've found tend towards the extremes. For example,\n\n&gt;You're an asshole\n\nmight be a -0.99 and\n\n&gt;Fuck you, I hope you fucking die you piece of shit\n\nis a -1, despite there being a significant difference in the intensity of the negative sentiment. Are there any pretrained models where the score that the model outputs doesn't just show the sentiment, but also the intensity of the sentiment?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11xlnrq/are_there_any_pretrained_sentiment_analysis/","created":"2023-03-21","tags":["reddit","ml","languagetechnology"],"meta":{"num_comments":3}}
{"title":"Cant get the word out of a vector embeding...","description":"I want to train a transformer, Im using fast text for word embedding, i trained the model and everyting was fine, but at the end, when I wanted to convert a the output vector to a word, i find out that fast text doesent have this functionality, is there any alteranative?\nor maybe someone can explain how to do that with fast text?\nthe task I want to do is the following: I get a string as input that may or not contain a date range, for example \"the holiday was from the first of jul to the third, at 02.07 we had dinner together\", and output the date range in the string: \"01.07.2023-03.07.2023\" with that format...","link":"https://www.reddit.com/r/LanguageTechnology/comments/11xdi64/cant_get_the_word_out_of_a_vector_embeding/","created":"2023-03-21","tags":["reddit","ml","languagetechnology"],"meta":{"num_comments":2}}
{"title":"Where does the input sentence length is dealt with in a transformer?","description":"Someone understands why the vector size still depends on the sequence length after the positional encoding in a transformer? I thought it needed to be independent from the sequence length at one point?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11wuo4y/where_does_the_input_sentence_length_is_dealt/","created":"2023-03-20","tags":["reddit","ml","languagetechnology"],"meta":{"num_comments":11}}
{"title":"Translate a meeting","description":"Hi Everyone,\n\nI need to translate a recording of a meeting of 2 hours where Dutch and French are spoken. I speak Dutch but I don't speak French. What is the best voice translator for longer spoken texts? Does anyone have tips?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11wh2zm/translate_a_meeting/","created":"2023-03-20","tags":["reddit","ml","languagetechnology"],"meta":{"num_comments":1}}
