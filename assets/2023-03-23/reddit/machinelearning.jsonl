{"title":"[R] Sparks of Artificial General Intelligence: Early experiments with GPT-4","description":"[New paper](https://arxiv.org/abs/2303.12712) by MSR researchers analyzing an early (and less constrained) version of GPT-4. Spicy quote from the abstract:\n\n\"Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"\n\nWhat are everyone's thoughts?","link":"https://www.reddit.com/r/MachineLearning/comments/11z3ymj/r_sparks_of_artificial_general_intelligence_early/","created":"2023-03-23","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":107}}
{"title":"[P] GPT-4 powered full stack web development with no manual coding","description":"[https://www.youtube.com/watch?v=lZj63vjueeU](https://www.youtube.com/watch?v=lZj63vjueeU)\n\nWhat do you all think of this approach to full stack gpt-assisted web development? In a sense its no code because the human user does not write or even edit the code - but in a sense its the opposite, because only an experienced web developer or at least a product manager would know how to instruct GPT in a useful manner.\n\nPS. I'm the injured engineer who made this thing out of necessity, because i injured my wrist building an AI platform that's become way too big for one engineer to maintain. So AMA :)","link":"https://www.reddit.com/r/MachineLearning/comments/11z7r4c/p_gpt4_powered_full_stack_web_development_with_no/","created":"2023-03-23","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":10}}
{"title":"[P] Serge, a self-hosted app for running LLaMa models (Alpaca) entirely locally, no remote API needed.","description":"Hello there!\n\n[Serge chat UI, with conversations on the left](https://preview.redd.it/rayrn7m4ncpa1.png?width=1922&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2bea149c499f4f0b9ad0b6aceb9dc21404c6e9d5)\n\nI've recently been working on Serge, a self-hosted dockerized way of running LLaMa models with a decent UI &amp; stored conversations. It currently supports Alpaca 7B, 13B and 30B and we're working on integrating it with LangChain and the ReAct chain agent.\n\nI've tried my best at making the instructions dead easy, so it's all dockerized with a download manager for weights and it can be run with almost zero configuration required.\n\nI think being able to run those models locally will be key to expanding their ability, and so I hope this can contribute to that.\n\nLet me know if you have any feedback or suggestions on how to extend its capabilities!\n\n&amp;#x200B;\n\nGitHub: [https://github.com/nsarrazin/serge](https://github.com/nsarrazin/serge)","link":"https://www.reddit.com/r/MachineLearning/comments/11yvbzc/p_serge_a_selfhosted_app_for_running_llama_models/","created":"2023-03-22","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":25}}
{"title":"[D] Overwhelmed by fast advances in recent weeks","description":"I was watching the GTC keynote and became entirely overwhelmed by the amount of progress achieved from last year.  I'm wondering how everyone else feels.\n\n&amp;#x200B;\n\nFirstly, the entire ChatGPT, GPT-3/GPT-4 chaos has been going on for a few weeks, with everyone scrambling left and right to integrate chatbots into their apps, products, websites. Twitter is flooded with new product ideas, how to speed up the process from idea to product, countless promp engineering blogs, tips, tricks, paid courses.\n\n&amp;#x200B;\n\nNot only was ChatGPT disruptive, but a few days later, Microsoft and Google also released their models and integrated them into their search engines. Microsoft also integrated its LLM into its Office suite. It all happenned overnight. I understand that they've started integrating them along the way, but still, it seems like it hapenned way too fast. This tweet encompases the past few weeks perfectly [https://twitter.com/AlphaSignalAI/status/1638235815137386508](https://twitter.com/AlphaSignalAI/status/1638235815137386508) , on a random Tuesday countless products are released that seem revolutionary.\n\n&amp;#x200B;\n\nIn addition to the language models, there are also the generative art models that have been slowly rising in mainstream recognition. Now Midjourney AI is known by a lot of people who are not even remotely connected to the AI space.\n\n&amp;#x200B;\n\nFor the past few weeks, reading Twitter, I've felt completely overwhelmed, as if the entire AI space is moving beyond at lightning speed, whilst around me we're just slowly training models, adding some data, and not seeing much improvement, being stuck on coming up with \"new ideas, that set us apart\".\n\n&amp;#x200B;\n\nWatching the GTC keynote from NVIDIA I was again, completely overwhelmed by how much is being developed throughout all the different domains. The ASML EUV (microchip making system) was incredible, I have no idea how it does lithography and to me it still seems like magic. The Grace CPU with 2 dies (although I think Apple was the first to do it?) and 100 GB RAM, all in a small form factor. There were a lot more different hardware servers that I just blanked out at some point. The omniverse sim engine looks incredible, almost real life (I wonder how much of a domain shift there is between real and sim considering how real the sim looks). Beyond it being cool and usable to train on synthetic data, the car manufacturers use it to optimize their pipelines. This change in perspective, of using these tools for other goals than those they were designed for I find the most interesting.\n\n&amp;#x200B;\n\nThe hardware part may be old news, as I don't really follow it, however the software part is just as incredible. NVIDIA AI foundations (language, image, biology models), just packaging everything together like a sandwich. Getty, Shutterstock and Adobe will use the generative models to create images. Again, already these huge juggernauts are already integrated.\n\n&amp;#x200B;\n\nI can't believe the point where we're at. We can use AI to write code, create art, create audiobooks using Britney Spear's voice, create an interactive chatbot to converse with books, create 3D real-time avatars, generate new proteins (?i'm lost on this one), create an anime and countless other scenarios. Sure, they're not perfect, but the fact that we can do all that in the first place is amazing.\n\n&amp;#x200B;\n\nAs Huang said in his keynote, companies want to develop \"disruptive products and business models\". I feel like this is what I've seen lately. Everyone wants to be the one that does something first, just throwing anything and everything at the wall and seeing what sticks.\n\n&amp;#x200B;\n\nIn conclusion, I'm feeling like the world is moving so fast around me whilst I'm standing still. I want to not read anything anymore and just wait until everything dies down abit, just so I can get my bearings. However, I think this is unfeasible. I fear we'll keep going in a frenzy until we just burn ourselves at some point.\n\n&amp;#x200B;\n\nHow are you all fairing? How do you feel about this frenzy in the AI space? What are you the most excited about?","link":"https://www.reddit.com/r/MachineLearning/comments/11ybjsi/d_overwhelmed_by_fast_advances_in_recent_weeks/","created":"2023-03-22","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":299}}
{"title":"[N] [D] GitHub Copilot X Announced","description":"Website: [https://github.com/features/preview/copilot-x](https://github.com/features/preview/copilot-x)Announcement video: [https://www.youtube.com/watch?v=4RfD5JiXt3A](https://www.youtube.com/watch?v=4RfD5JiXt3A)\n\nWhat do you think?\n\nAlso, here are some other open-source GitHub projects and product integrations of GPT-4: [https://github.com/radi-cho/awesome-gpt4](https://github.com/radi-cho/awesome-gpt4). Feel free to contribute to that list.","link":"https://www.reddit.com/r/MachineLearning/comments/11ypgcf/n_d_github_copilot_x_announced/","created":"2023-03-22","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":27}}
{"title":"[R] Introducing SIFT: A New Family of Sparse Iso-FLOP Transformations to Improve the Accuracy of Computer Vision and Language Models","description":"**Note**: Not to be confused with Scale-Invariant Feature Transforms :)\n\nWe are excited to announce the availability of our [paper on arxiv](https://arxiv.org/abs/2303.11525) on Sparse Iso-FLOP Transformations (SIFT), which increases accuracy and maintains the same FLOPs as the dense model using sparsity. In this research, we replace dense layers with SIFT and significantly improve computer vision and natural language processing tasks without modifying training hyperparameters\n\nSome of the highlights of this work include ResNet-18 on ImageNet achieving a 3.5% accuracy improvement and GPT-3 Small on WikiText-103 reducing perplexity by 0.4, both matching larger dense model variants that have 2x or more FLOPs.\n\nThe SIFT transformations are simple to use, provide a larger search space to find optimal sparse masks, and are parameterized by a single hyperparameter - the sparsity level.\n\nThis is independent of the research we [posted](https://www.reddit.com/r/MachineLearning/comments/11xskuk/r_spdf_sparse_pretraining_and_dense_finetuning/) yesterday, which demonstrates the ability to reduce pre-training FLOPs while maintaining accuracy on downstream tasks.\n\nThis is the first work (that we know of!) to demonstrate the use of sparsity for improving the accuracy of models via a set of sparse transformations.\n\nhttps://preview.redd.it/7y8cgaisddpa1.png?width=3536&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9c7123463516291acc495b47625c0dd874fd9c43","link":"https://www.reddit.com/r/MachineLearning/comments/11yzsz6/r_introducing_sift_a_new_family_of_sparse_isoflop/","created":"2023-03-22","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":20}}
{"title":"[P] Open-source GPT4 &amp; LangChain Chatbot for large PDF docs","description":"GitHub: [https://github.com/mayooear/gpt4-pdf-chatbot-langchain](https://github.com/mayooear/gpt4-pdf-chatbot-langchain)  \nDemo video: [https://www.youtube.com/watch?v=ih9PBGVVOO4](https://www.youtube.com/watch?v=ih9PBGVVOO4)","link":"https://www.reddit.com/r/MachineLearning/comments/11z9s3g/p_opensource_gpt4_langchain_chatbot_for_large_pdf/","created":"2023-03-23","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":4}}
{"title":"[D] LLMs\u2019 use of synthetic data","description":"I recently did an \u201c[interview](https://www.tonic.ai/blog/how-bing-uses-synthetic-data-to-improve-its-models-as-explained-by-bing?utm_campaign=Blogs&amp;utm_source=reddit&amp;utm_medium=social&amp;utm_term=r%2FMachineLearning&amp;utm_content=How)\u201d with Bing about its use of synthetic data in its training sets.\n\nIt talked about:\n\n* The definition of synthetic data and use cases\n* Its use of GANs to generate synthetic data when there isn\u2019t enough quality data for it to draw insights and patterns from\n* Methods for synthetic data generation\n\nI\u2019m interested to hear peoples\u2019 thoughts on LLMs generating their own synthetic data to add to their training sets. It described it itself as a bit of a feedback loop and I\u2019m curious to hear peoples\u2019 opinions on the dynamic of a model generating its own data to train on.","link":"https://www.reddit.com/r/MachineLearning/comments/11zf6h0/d_llms_use_of_synthetic_data/","created":"2023-03-23","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":0}}
{"title":"[P] CleanVision: Audit your Image Data for better Computer Vision","description":"To all my computer vision friends working on real-world applications with messy image data, I just open-sourced a Python library you may find useful!\n\n[Some issues detected in the Caltech-256 dataset.](https://preview.redd.it/smaldg3c5bpa1.png?width=960&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b78a1d0aa64b669f9b546ad7321a650acc59f8a7)\n\nCleanVision audits any image dataset to automatically detect common issues such as images that are blurry, under/over-exposed, oddly sized, or near duplicates of others. It\u2019s just 3 lines of code to discover what issues lurk in your data before you dive into modeling, and CleanVision can be used for **any** image dataset \u2014 regardless of whether your task is image generation, classification, segmentation, object detection, etc.\n\n    from cleanvision.imagelab import Imagelab \n    imagelab = Imagelab(data_path=\"path_to_dataset\")\n    imagelab.find_issues()\n    imagelab.report()\n\nAs leaders like Andrew Ng and OpenAI have lately repeated: models can only be as good as the data they are trained on. Before diving into modeling, quickly run your images through CleanVision to make sure they are ok \u2014 it\u2019s super easy!\n\nGithub:  [https://github.com/cleanlab/cleanvision](https://github.com/cleanlab/cleanvision)","link":"https://www.reddit.com/r/MachineLearning/comments/11ym81i/p_cleanvision_audit_your_image_data_for_better/","created":"2023-03-22","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":3}}
{"title":"[P] ChatLLaMA - A ChatGPT style chatbot for Facebook's LLaMA","description":"\ud83d\udc4b  Hey all, we just launched [ChatLLaMA](https://chatllama.baseten.co/). An experimental chatbot interface for interacting with variants of Facebook's LLaMa. Currently, we support the 7 billion parameter variant that was fine-tuned on the Alpaca dataset. This early version isn't as conversational as we'd like, but over the next week or so, we're planning on adding support for the 30 billion parameter variant, another variant fine-tuned on LAION's OpenAssistant dataset and more as we explore what this model is capable of.\n\nIf you want deploy your own instance is the model powering the chatbot and build something similar we've open sourced the Truss here: [https://github.com/basetenlabs/alpaca-7b-truss](https://github.com/basetenlabs/alpaca-7b-truss)\n\nWe'd love to hear any feedback you have!\n\n[Check it out here](https://chatllama.baseten.co/)","link":"https://www.reddit.com/r/MachineLearning/comments/11yof4h/p_chatllama_a_chatgpt_style_chatbot_for_facebooks/","created":"2023-03-22","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":6}}
{"title":"[D] Logistic weka in sklearn","description":"My professor got some results using logistic modul in weka. He then asked me to implement this algorithm in python using sklern. Also he used gain ratio for filtering attributes. I'm having difficulty getting same results on same data. I'm not even sure witch implementation of logistics regression to use in sklern or if the mutal information in sklearn is the same as gain ratio in weka. I ended up using python weka wrapper just so I get the same results as he did. But I would like to know what was the real difference between logistic regression in sklearn and weka? And how to get gain ratio from weka using mutal information in sklearn?","link":"https://www.reddit.com/r/MachineLearning/comments/11zcqtm/d_logistic_weka_in_sklearn/","created":"2023-03-23","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":0}}
{"title":"[D][R] Concerns about using Conformer with Classification Token","description":" \n\nHello everyone,  \nI have a question regarding the combination of Conformers and Classification Tokens.   \nAs I know, Conformers are a variation of Transformers, with added convolutional layers, while Classification Tokens are special-purpose inputs used in models like BERT.   \nThese tokens are usually added to the beginning of sequence data to help identify the entire sequence.  \nIn the original BERT model, where Transformers are used, it seems that there is no issue in using a classification token.   \nHowever, I have concerns about how well it would work with a Conformer due to the presence of convolutional layers.  \nMy specific concern is that if the classification token is added to the beginning of the sequence, only the initial part of the sequence would be influenced by the classification token through the convolutional layer, leaving the latter parts unaffected.  \nDespite my concerns, I have seen research that combines Conformers and Classification Tokens.   \nI am wondering if there is actually no problem with this approach.   \nAlternatively, is there a way to circumvent this issue?   \nThank you in advance!","link":"https://www.reddit.com/r/MachineLearning/comments/11zcouh/dr_concerns_about_using_conformer_with/","created":"2023-03-23","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":0}}
{"title":"[D] LLaMA or Alpaca Weights","description":"Was anyone able to download the LLaMA or Alpaca weights for the 7B, 13B and or 30B models? If yes please share, not looking for HF weights","link":"https://www.reddit.com/r/MachineLearning/comments/11zcog6/d_llama_or_alpaca_weights/","created":"2023-03-23","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":1}}
{"title":"[P] fastLLaMa, A python wrapper to run llama.cpp","description":"Hi all, I have been working on fastLLaMa. It is a Python package that provides a Pythonic interface to a C++ library, llama.cpp. It allows you to use the functionality of the C++ library from within Python, without having to write C++ code or deal with low-level C++ APIs.\n\nUsing fastLLaMa, you can ingest the model with system prompts and then save the state of the model, Then later load the state, and start inferencing the model immediately.\n\nNo noticeable performance drop between lama.cpp and fastLLaMa.\n\nHave a look at it if it is of interest and do let me know what you think :)\n\n[Repo Link](https://github.com/PotatoSpudowski/fastLLaMa) \n\n[Tweet](https://twitter.com/Bahushruth/status/1638231265320239106)","link":"https://www.reddit.com/r/MachineLearning/comments/11y9qgg/p_fastllama_a_python_wrapper_to_run_llamacpp/","created":"2023-03-22","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":21}}
{"title":"[D] Question for use of ML in adaptive authentication","description":"Hi all, I'm looking for advice for using ML for Adaptive Authentication.\n\nThe use case is that I want to generate a unique identifier key from user bahavior. eg: Sam uses my app and I want to generate key 1234, Mel uses the app, her key is 2351, etc\n\nTo generate this key I thought I could use an ML model that takes as input user behavior data and outputs this key or something I can use to derive a key.\n\nTaking typing on a smartphone as an example: a user types 10 words on their keyboard, we take data from that and feed it to the model to generate the key for this user. The data we take might be something like speed of typing a letter, time fingers were pressed on keys, number of times they used backspace, etc...\n\nIs this possible? I'm not an ML specialist so my knowledge is limited, but I was thinking we could do something like using a classifier with 10 categories, and use some statistical value from the output equivalent to prediction accuracy or prediction certainty for each category to generate numbers out of the classifications... but that seems like a hack and there may be something more precise and standard","link":"https://www.reddit.com/r/MachineLearning/comments/11zce7u/d_question_for_use_of_ml_in_adaptive/","created":"2023-03-23","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":3}}
{"title":"GPT-4 For SQL Schema Generation + Unstructured Feature Extraction [D]","description":"GPT-4 is out and I think data engineering is going to be out the door soon, I saw this post on medium recently: [https://medium.com/@nschairer/gpt-4-data-pipelines-transform-json-to-sql-schema-instantly-dfd62f6d1024](https://medium.com/@nschairer/gpt-4-data-pipelines-transform-json-to-sql-schema-instantly-dfd62f6d1024)\n\nAnd I was pretty amazed at how well GPT-4 can generate a SQL schema from raw JSON data, and had to wonder if we are wasting our time with NLP models for extracting information from raw text. For example, you could use bs4 to pull all inner text out of certain web forms and have GPT-4 extract meaningful information from them (say SEC filings with pseudo standard fields)...anyone agree?","link":"https://www.reddit.com/r/MachineLearning/comments/11ytoh1/gpt4_for_sql_schema_generation_unstructured/","created":"2023-03-22","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":5}}
{"title":"[D] ICML 2023 Reviewer-Author Discussion","description":"Thought it might make sense to create a discussion thread specifically for reviewer-author discussion. It seems that many authors (at least those around me) did not receive any further response from the reviewers. How's everyone's discussion period going?\n\nPS: I understand that ICML reviewers are busy with their own research/work and are managing many submissions at the same time. I just wish they could be more active in the discussion period, because all these submissions are the results of many months of hard work. Personally I am also a reviewer at ICML and have responded to most authors.","link":"https://www.reddit.com/r/MachineLearning/comments/11ylumz/d_icml_2023_reviewerauthor_discussion/","created":"2023-03-22","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":10}}
{"title":"[D] ML model to find text/similar text in pdf","description":"Hi all, \nI am trying to build a ML model that find occurrence of text/similar text in a pdf and returns a %match of that text I am looking for. \nTF-IDF looks like one of the models I can use. Does anyone know another model that might be useful for this? Maybe something that can produce reliable results after training on like 500-600 documents?","link":"https://www.reddit.com/r/MachineLearning/comments/11za7qe/d_ml_model_to_find_textsimilar_text_in_pdf/","created":"2023-03-23","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":0}}
{"title":"[R] MM-ReAct: Prompting ChatGPT for Multimodal Reasoning and Action","description":" Blog - [https://multimodal-react.github.io/](https://multimodal-react.github.io/)\n\nPaper - [https://arxiv.org/abs/2303.11381](https://arxiv.org/abs/2303.11381)\n\nCode - [https://github.com/microsoft/MM-REACT](https://github.com/microsoft/MM-REACT)\n\nDemo - [https://huggingface.co/spaces/microsoft-cognitive-service/mm-react](https://huggingface.co/spaces/microsoft-cognitive-service/mm-react)\n\nWildest thing i've seen in a while. Still processing how a connection of foundation models can be this good.","link":"https://www.reddit.com/r/MachineLearning/comments/11y70rx/r_mmreact_prompting_chatgpt_for_multimodal/","created":"2023-03-22","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":25}}
{"title":"[D] ICML rebuttal discussion stage","description":"I have been distributed with four reviewers, three of which missed my contribution, they surely have a unfinished review. \n\nIn rebuttal stage, I answer all of the questions and weakness about technical. However, I have not got any replies.\n\n\n\nP.S I have requested area chair supervising them having another full review. But no any simple feedback submitted.\n\n\nHow to deal with this suitcase? My first time to ICML conference. So frustrated to the so-called openreview discussion stage.","link":"https://www.reddit.com/r/MachineLearning/comments/11yr9k9/d_icml_rebuttal_discussion_stage/","created":"2023-03-22","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":3}}
{"title":"I made a tool that saves your ChatGPT conversations in .md file. And it's Open Source. [P]","description":"[https://github.com/MatveyM11/Mine-ChatGPT](https://github.com/MatveyM11/Mine-ChatGPT)\n\nThe extension has already been submitted for approval in the Chrome Web Store.","link":"https://www.reddit.com/r/MachineLearning/comments/11z56ro/i_made_a_tool_that_saves_your_chatgpt/","created":"2023-03-23","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":1}}
{"title":"[R] Prompting ChatGPT for visual math and text reasoning","description":"&amp;#x200B;\n\nhttps://preview.redd.it/m7tdhkd2gbpa1.jpg?width=449&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7197a41640b06e15e1be78549303791d94dc7f0e","link":"https://www.reddit.com/r/MachineLearning/comments/11ynzc1/r_prompting_chatgpt_for_visual_math_and_text/","created":"2023-03-22","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":1}}
{"title":"Machine Learning for Materials[D]","description":"Is this subfield growing ? Is it advisable to go for a full fledged phd in this subject.","link":"https://www.reddit.com/r/MachineLearning/comments/11yppjz/machine_learning_for_materialsd/","created":"2023-03-22","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":1}}
{"title":"[P] New auqa AI features that will make test case creation faster","description":"Hey!\ud83d\udc4b I really wanted to share some exciting news with you.\n\nMy team and I have been working on something important for all the testers out there, and I am happy to announce this.\n\nAI test creation features are live and available for all aqua users (including free trials)!\ud83d\udd25\n\nAnd now, with the help of AI tech, it will be possible to:\n\n1. Auto-create test descriptions\n2. Auto-create test steps\n3. Create a whole test case from a requirement\n\nI believe it is a game-changer for manual testing that will allow us to work faster and more efficiently.\ud83d\ude4c\n\nYou can try it for free by starting a 30-day trial at aqua \ud83d\udc49 [https://aqua-cloud.io/ai-in-aqua/](https://aqua-cloud.io/ai-in-aqua/)\n\nIf you are going to try it, please contact me afterwards. We worked hard to make this technology happen, and it would be great to hear your feedback!","link":"https://www.reddit.com/r/MachineLearning/comments/11z00ex/p_new_auqa_ai_features_that_will_make_test_case/","created":"2023-03-22","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":0}}
{"title":"[P] CodeAlpaca Code and Data release","description":"Released the data and code used to train CodeAlpaca - [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca)","link":"https://www.reddit.com/r/MachineLearning/comments/11yh8x8/p_codealpaca_code_and_data_release/","created":"2023-03-22","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":15}}
