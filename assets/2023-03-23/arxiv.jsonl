{"title":"MaskCon: Masked Contrastive Learning for Coarse-Labelled Dataset","description":"Deep learning has achieved great success in recent years with the aid of advanced neural network structures and large-scale human-annotated datasets. However, it is often costly and difficult to accurately and efficiently annotate large-scale datasets, especially for some specialized domains where fine-grained labels are required. In this setting, coarse labels are much easier to acquire as they do not require expert knowledge. In this work, we propose a contrastive learning method, called $\\textbf{Mask}$ed $\\textbf{Con}$trastive learning~($\\textbf{MaskCon}$) to address the under-explored problem setting, where we learn with a coarse-labelled dataset in order to address a finer labelling problem. More specifically, within the contrastive learning framework, for each sample our method generates soft-labels with the aid of coarse labels against other samples and another augmented view of the sample in question. By contrast to self-supervised contrastive learning where only the sample's augmentations are considered hard positives, and in supervised contrastive learning where only samples with the same coarse labels are considered hard positives, we propose soft labels based on sample distances, that are masked by the coarse labels. This allows us to utilize both inter-sample relations and coarse labels. We demonstrate that our method can obtain as special cases many existing state-of-the-art works and that it provides tighter bounds on the generalization error. Experimentally, our method achieves significant improvement over the current state-of-the-art in various datasets, including CIFAR10, CIFAR100, ImageNet-1K, Standford Online Products and Stanford Cars196 datasets. Code and annotations are available at https://github.com/MrChenFeng/MaskCon_CVPR2023.","link":"http://arxiv.org/abs/2303.12756v1","created":"2023-03-22","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"AfroDigits: A Community-Driven Spoken Digit Dataset for African Languages","description":"The advancement of speech technologies has been remarkable, yet its integration with African languages remains limited due to the scarcity of African speech corpora. To address this issue, we present AfroDigits, a minimalist, community-driven dataset of spoken digits for African languages, currently covering 38 African languages. As a demonstration of the practical applications of AfroDigits, we conduct audio digit classification experiments on six African languages [Igbo (ibo), Yoruba (yor), Rundi (run), Oshiwambo (kua), Shona (sna), and Oromo (gax)] using the Wav2Vec2.0-Large and XLS-R models. Our experiments reveal a useful insight on the effect of mixing African speech corpora during finetuning. AfroDigits is the first published audio digit dataset for African languages and we believe it will, among other things, pave the way for Afro-centric speech applications such as the recognition of telephone numbers, and street numbers. We release the dataset and platform publicly at https://huggingface.co/datasets/chrisjay/crowd-speech-africa and https://huggingface.co/spaces/chrisjay/afro-speech respectively.","link":"http://arxiv.org/abs/2303.12582v1","created":"2023-03-22","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"RaBit: Parametric Modeling of 3D Biped Cartoon Characters with a Topological-consistent Dataset","description":"Assisting people in efficiently producing visually plausible 3D characters has always been a fundamental research topic in computer vision and computer graphics. Recent learning-based approaches have achieved unprecedented accuracy and efficiency in the area of 3D real human digitization. However, none of the prior works focus on modeling 3D biped cartoon characters, which are also in great demand in gaming and filming. In this paper, we introduce 3DBiCar, the first large-scale dataset of 3D biped cartoon characters, and RaBit, the corresponding parametric model. Our dataset contains 1,500 topologically consistent high-quality 3D textured models which are manually crafted by professional artists. Built upon the data, RaBit is thus designed with a SMPL-like linear blend shape model and a StyleGAN-based neural UV-texture generator, simultaneously expressing the shape, pose, and texture. To demonstrate the practicality of 3DBiCar and RaBit, various applications are conducted, including single-view reconstruction, sketch-based modeling, and 3D cartoon animation. For the single-view reconstruction setting, we find a straightforward global mapping from input images to the output UV-based texture maps tends to lose detailed appearances of some local parts (e.g., nose, ears). Thus, a part-sensitive texture reasoner is adopted to make all important local areas perceived. Experiments further demonstrate the effectiveness of our method both qualitatively and quantitatively. 3DBiCar and RaBit are available at gaplab.cuhk.edu.cn/projects/RaBit.","link":"http://arxiv.org/abs/2303.12564v1","created":"2023-03-22","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"Graph Data Models and Relational Database Technology","description":"Recent work on database application development platforms has sought to include a declarative formulation of a conceptual data model in the application code, using annotations or attributes. Some recent work has used metadata to include the details of such formulations in the physical database, and this approach brings significant advantages in that the model can be enforced across a range of applications for a single database. In previous work, we have discussed the advantages for enterprise integration of typed graph data models (TGM), which can play a similar role in graphical databases, leveraging the existing support for the unified modelling language UML. Ideally, the integration of systems designed with different models, for example, graphical and relational database, should also be supported. In this work, we implement this approach, using metadata in a relational database management system (DBMS).","link":"http://arxiv.org/abs/2303.12376v1","created":"2023-03-22","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"Reliable and Efficient Evaluation of Adversarial Robustness for Deep Hashing-Based Retrieval","description":"Deep hashing has been extensively applied to massive image retrieval due to its efficiency and effectiveness. Recently, several adversarial attacks have been presented to reveal the vulnerability of deep hashing models against adversarial examples. However, existing attack methods suffer from degraded performance or inefficiency because they underutilize the semantic relations between original samples or spend a lot of time learning these relations with a deep neural network. In this paper, we propose a novel Pharos-guided Attack, dubbed PgA, to evaluate the adversarial robustness of deep hashing networks reliably and efficiently. Specifically, we design pharos code to represent the semantics of the benign image, which preserves the similarity to semantically relevant samples and dissimilarity to irrelevant ones. It is proven that we can quickly calculate the pharos code via a simple math formula. Accordingly, PgA can directly conduct a reliable and efficient attack on deep hashing-based retrieval by maximizing the similarity between the hash code of the adversarial example and the pharos code. Extensive experiments on the benchmark datasets verify that the proposed algorithm outperforms the prior state-of-the-arts in both attack strength and speed.","link":"http://arxiv.org/abs/2303.12658v1","created":"2023-03-22","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"Do Backdoors Assist Membership Inference Attacks?","description":"When an adversary provides poison samples to a machine learning model, privacy leakage, such as membership inference attacks that infer whether a sample was included in the training of the model, becomes effective by moving the sample to an outlier. However, the attacks can be detected because inference accuracy deteriorates due to poison samples. In this paper, we discuss a \\textit{backdoor-assisted membership inference attack}, a novel membership inference attack based on backdoors that return the adversary's expected output for a triggered sample. We found three crucial insights through experiments with an academic benchmark dataset. We first demonstrate that the backdoor-assisted membership inference attack is unsuccessful. Second, when we analyzed loss distributions to understand the reason for the unsuccessful results, we found that backdoors cannot separate loss distributions of training and non-training samples. In other words, backdoors cannot affect the distribution of clean samples. Third, we also show that poison and triggered samples activate neurons of different distributions. Specifically, backdoors make any clean sample an inlier, contrary to poisoning samples. As a result, we confirm that backdoors cannot assist membership inference.","link":"http://arxiv.org/abs/2303.12589v1","created":"2023-03-22","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"A survey of hardware-based malware detection approach","description":"Malware is the most significant threat to computer security. This paper aims to overview the malware detection field, focusing on the recent and promising hardware-based approach. This approach leverages the Hardware Performance Counters already available in modern processors and the power of Machine Learning, offering attractive advantages like resilience to disabling the protection, resilience to unknown malware, low complexity/overhead/cost, and run-time detection. The approach is deeply analyzed in light of a generic hardware-based detection framework. Some challenges related to the approach are presented: the necessary accuracy improvements, how to deal with the classification error, better correlating the hardware events behavior with the malware, and essential improvements on the hardware performance monitor.","link":"http://arxiv.org/abs/2303.12525v1","created":"2023-03-22","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"Revisiting DeepFool: generalization and improvement","description":"Deep neural networks have been known to be vulnerable to adversarial examples, which are inputs that are modified slightly to fool the network into making incorrect predictions. This has led to a significant amount of research on evaluating the robustness of these networks against such perturbations. One particularly important robustness metric is the robustness to minimal l2 adversarial perturbations. However, existing methods for evaluating this robustness metric are either computationally expensive or not very accurate. In this paper, we introduce a new family of adversarial attacks that strike a balance between effectiveness and computational efficiency. Our proposed attacks are generalizations of the well-known DeepFool (DF) attack, while they remain simple to understand and implement. We demonstrate that our attacks outperform existing methods in terms of both effectiveness and computational efficiency. Our proposed attacks are also suitable for evaluating the robustness of large models and can be used to perform adversarial training (AT) to achieve state-of-the-art robustness to minimal l2 adversarial perturbations.","link":"http://arxiv.org/abs/2303.12481v1","created":"2023-03-22","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"AIIPot: Adaptive Intelligent-Interaction Honeypot for IoT Devices","description":"The proliferation of the Internet of Things (IoT) has raised concerns about the security of connected devices. There is a need to develop suitable and cost-efficient methods to identify vulnerabilities in IoT devices in order to address them before attackers seize opportunities to compromise them. The deception technique is a prominent approach to improving the security posture of IoT systems. Honeypot is a popular deception technique that mimics interaction in real fashion and encourages unauthorised users (attackers) to launch attacks. Due to the large number and the heterogeneity of IoT devices, manually crafting the low and high-interaction honeypots is not affordable. This has forced researchers to seek innovative ways to build honeypots for IoT devices. In this paper, we propose a honeypot for IoT devices that uses machine learning techniques to learn and interact with attackers automatically. The evaluation of the proposed model indicates that our system can improve the session length with attackers and capture more attacks on the IoT network.","link":"http://arxiv.org/abs/2303.12367v1","created":"2023-03-22","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"Distribution-restrained Softmax Loss for the Model Robustness","description":"Recently, the robustness of deep learning models has received widespread attention, and various methods for improving model robustness have been proposed, including adversarial training, model architecture modification, design of loss functions, certified defenses, and so on. However, the principle of the robustness to attacks is still not fully understood, also the related research is still not sufficient. Here, we have identified a significant factor that affects the robustness of models: the distribution characteristics of softmax values for non-real label samples. We found that the results after an attack are highly correlated with the distribution characteristics, and thus we proposed a loss function to suppress the distribution diversity of softmax. A large number of experiments have shown that our method can improve robustness without significant time consumption.","link":"http://arxiv.org/abs/2303.12363v1","created":"2023-03-22","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"Synthetic Health-related Longitudinal Data with Mixed-type Variables Generated using Diffusion Models","description":"This paper presents a novel approach to simulating electronic health records (EHRs) using diffusion probabilistic models (DPMs). Specifically, we demonstrate the effectiveness of DPMs in synthesising longitudinal EHRs that capture mixed-type variables, including numeric, binary, and categorical variables. To our knowledge, this represents the first use of DPMs for this purpose. We compared our DPM-simulated datasets to previous state-of-the-art results based on generative adversarial networks (GANs) for two clinical applications: acute hypotension and human immunodeficiency virus (ART for HIV). Given the lack of similar previous studies in DPMs, a core component of our work involves exploring the advantages and caveats of employing DPMs across a wide range of aspects. In addition to assessing the realism of the synthetic datasets, we also trained reinforcement learning (RL) agents on the synthetic data to evaluate their utility for supporting the development of downstream machine learning models. Finally, we estimated that our DPM-simulated datasets are secure and posed a low patient exposure risk for public access.","link":"http://arxiv.org/abs/2303.12281v1","created":"2023-03-22","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"Exploring the Benefits of Visual Prompting in Differential Privacy","description":"Visual Prompting (VP) is an emerging and powerful technique that allows sample-efficient adaptation to downstream tasks by engineering a well-trained frozen source model. In this work, we explore the benefits of VP in constructing compelling neural network classifiers with differential privacy (DP). We explore and integrate VP into canonical DP training methods and demonstrate its simplicity and efficiency. In particular, we discover that VP in tandem with PATE, a state-of-the-art DP training method that leverages the knowledge transfer from an ensemble of teachers, achieves the state-of-the-art privacy-utility trade-off with minimum expenditure of privacy budget. Moreover, we conduct additional experiments on cross-domain image classification with a sufficient domain gap to further unveil the advantage of VP in DP. Lastly, we also conduct extensive ablation studies to validate the effectiveness and contribution of VP under DP consideration.","link":"http://arxiv.org/abs/2303.12247v1","created":"2023-03-22","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"Can we trust the evaluation on ChatGPT?","description":"ChatGPT, the first large language model (LLM) with mass adoption, has demonstrated remarkable performance in numerous natural language tasks. Despite its evident usefulness, evaluating ChatGPT's performance in diverse problem domains remains challenging due to the closed nature of the model and its continuous updates via Reinforcement Learning from Human Feedback (RLHF). We highlight the issue of data contamination in ChatGPT evaluations, with a case study of the task of stance detection. We discuss the challenge of preventing data contamination and ensuring fair model evaluation in the age of closed and continuously trained models.","link":"http://arxiv.org/abs/2303.12767v1","created":"2023-03-22","tags":["arxiv","ml","prompteng"],"meta":{"query":"abs:\"chatgpt\""}}
{"title":"Sparks of Artificial General Intelligence: Early experiments with GPT-4","description":"Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.","link":"http://arxiv.org/abs/2303.12712v1","created":"2023-03-22","tags":["arxiv","ml","prompteng"],"meta":{"query":"abs:\"chatgpt\""}}
{"title":"Exploring the Benefits of Visual Prompting in Differential Privacy","description":"Visual Prompting (VP) is an emerging and powerful technique that allows sample-efficient adaptation to downstream tasks by engineering a well-trained frozen source model. In this work, we explore the benefits of VP in constructing compelling neural network classifiers with differential privacy (DP). We explore and integrate VP into canonical DP training methods and demonstrate its simplicity and efficiency. In particular, we discover that VP in tandem with PATE, a state-of-the-art DP training method that leverages the knowledge transfer from an ensemble of teachers, achieves the state-of-the-art privacy-utility trade-off with minimum expenditure of privacy budget. Moreover, we conduct additional experiments on cross-domain image classification with a sufficient domain gap to further unveil the advantage of VP in DP. Lastly, we also conduct extensive ablation studies to validate the effectiveness and contribution of VP under DP consideration.","link":"http://arxiv.org/abs/2303.12247v1","created":"2023-03-22","tags":["arxiv","ml","prompteng"],"meta":{"query":"prompt AND engineering"}}
{"title":"LSTM-based Video Quality Prediction Accounting for Temporal Distortions in Videoconferencing Calls","description":"Current state-of-the-art video quality models, such as VMAF, give excellent prediction results by comparing the degraded video with its reference video. However, they do not consider temporal distortions (e.g., frame freezes or skips) that occur during videoconferencing calls. In this paper, we present a data-driven approach for modeling such distortions automatically by training an LSTM with subjective quality ratings labeled via crowdsourcing. The videos were collected from live videoconferencing calls in 83 different network conditions. We applied QR codes as markers on the source videos to create aligned references and compute temporal features based on the alignment vectors. Using these features together with VMAF core features, our proposed model achieves a PCC of 0.99 on the validation set. Furthermore, our model outputs per-frame quality that gives detailed insight into the cause of video quality impairments. The VCM model and dataset are open-sourced at https://github.com/microsoft/Video_Call_MOS.","link":"http://arxiv.org/abs/2303.12761v1","created":"2023-03-22","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"Constraining f(Q) Cosmology with Standard Sirens","description":"In this dissertation, we study two cosmological models based on $f(Q)$ gravity. We resort to mock catalogs of standard siren (SS) events to see whether data from future gravitational wave (GWs) observatories will be able to distinguish these models from $\\Lambda$CDM.   The first model is the most general $f(Q)$ formulation that replicates a $\\Lambda$CDM background, with deviations appearing only at the perturbative level. It has one additional free parameter compared to $\\Lambda$CDM, $\\alpha$, which when set to zero falls back to $\\Lambda$CDM. We show that LIGO-Virgo is unable to constrain $\\alpha$, due to the high error and low redshift of the measurements, whereas LISA and the ET will, with the ET outperforming LISA. The catalogs for both LISA and LIGO-Virgo show non-negligible statistical fluctuations, where we consider three representative catalogs (the best, median and worst), whereas for the ET, only a single catalog is considered, as the number of events is large enough for statistical fluctuations to be neglected. The best LISA catalog is the one with more low redshift events, while the worst LISA catalog features fewer low redshift events. Additionally, if we are to observe a bad LISA catalog, we can rely on data from LIGO-Virgo to improve the quality of the constrains, bringing it closer to a median LISA catalog.   The second model attempts to replace dark energy by making use of a specific form of the function $f(Q)$. We study this model resorting to dynamical system techniques to show the regions in parameter space with viable cosmologies. Using model selection criteria, we show that no number of SS events is, by itself, able to tell this model and $\\Lambda$CDM apart. We then show that if we add current type Ia Supernova (SnIa) data, tensions in this model arise when compared to the constrains set by the SS events.","link":"http://arxiv.org/abs/2303.12674v1","created":"2023-03-22","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"A Word is Worth a Thousand Pictures: Prompts as AI Design Material","description":"Recent advances in Machine-Learning have led to the development of models that generate images based on a text description.Such large prompt-based text to image models (TTIs), trained on a considerable amount of data, allow the creation of high-quality images by users with no graphics or design training. This paper examines the role such TTI models can playin collaborative, goal-oriented design. Through a within-subjects study with 14 non-professional designers, we find that such models can help participants explore a design space rapidly and allow for fluid collaboration. We also find that text inputs to such models (\"prompts\") act as reflective design material, facilitating exploration, iteration, and reflection in pair design. This work contributes to the future of collaborative design supported by generative AI by providing an account of how text-to-image models influence the design process and the social dynamics around design and suggesting implications for tool design","link":"http://arxiv.org/abs/2303.12647v1","created":"2023-03-22","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"RaBit: Parametric Modeling of 3D Biped Cartoon Characters with a Topological-consistent Dataset","description":"Assisting people in efficiently producing visually plausible 3D characters has always been a fundamental research topic in computer vision and computer graphics. Recent learning-based approaches have achieved unprecedented accuracy and efficiency in the area of 3D real human digitization. However, none of the prior works focus on modeling 3D biped cartoon characters, which are also in great demand in gaming and filming. In this paper, we introduce 3DBiCar, the first large-scale dataset of 3D biped cartoon characters, and RaBit, the corresponding parametric model. Our dataset contains 1,500 topologically consistent high-quality 3D textured models which are manually crafted by professional artists. Built upon the data, RaBit is thus designed with a SMPL-like linear blend shape model and a StyleGAN-based neural UV-texture generator, simultaneously expressing the shape, pose, and texture. To demonstrate the practicality of 3DBiCar and RaBit, various applications are conducted, including single-view reconstruction, sketch-based modeling, and 3D cartoon animation. For the single-view reconstruction setting, we find a straightforward global mapping from input images to the output UV-based texture maps tends to lose detailed appearances of some local parts (e.g., nose, ears). Thus, a part-sensitive texture reasoner is adopted to make all important local areas perceived. Experiments further demonstrate the effectiveness of our method both qualitatively and quantitatively. 3DBiCar and RaBit are available at gaplab.cuhk.edu.cn/projects/RaBit.","link":"http://arxiv.org/abs/2303.12564v1","created":"2023-03-22","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"Anomaly Detection in Aeronautics Data with Quantum-compatible Discrete Deep Generative Model","description":"Deep generative learning cannot only be used for generating new data with statistical characteristics derived from input data but also for anomaly detection, by separating nominal and anomalous instances based on their reconstruction quality. In this paper, we explore the performance of three unsupervised deep generative models -- variational autoencoders (VAEs) with Gaussian, Bernoulli, and Boltzmann priors -- in detecting anomalies in flight-operations data of commercial flights consisting of multivariate time series. We devised two VAE models with discrete latent variables (DVAEs), one with a factorized Bernoulli prior and one with a restricted Boltzmann machine (RBM) as prior, because of the demand for discrete-variable models in machine-learning applications and because the integration of quantum devices based on two-level quantum systems requires such models. The DVAE with RBM prior, using a relatively simple -- and classically or quantum-mechanically enhanceable -- sampling technique for the evolution of the RBM's negative phase, performed better than the Bernoulli DVAE and on par with the Gaussian model, which has a continuous latent space. Our studies demonstrate the competitiveness of a discrete deep generative model with its Gaussian counterpart on anomaly-detection tasks. Moreover, the DVAE model with RBM prior can be easily integrated with quantum sampling by outsourcing its generative process to measurements of quantum states obtained from a quantum annealer or gate-model device.","link":"http://arxiv.org/abs/2303.12302v1","created":"2023-03-22","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"Encoding Binary Concepts in the Latent Space of Generative Models for Enhancing Data Representation","description":"Binary concepts are empirically used by humans to generalize efficiently. And they are based on Bernoulli distribution which is the building block of information. These concepts span both low-level and high-level features such as \"large vs small\" and \"a neuron is active or inactive\". Binary concepts are ubiquitous features and can be used to transfer knowledge to improve model generalization. We propose a novel binarized regularization to facilitate learning of binary concepts to improve the quality of data generation in autoencoders. We introduce a binarizing hyperparameter $r$ in data generation process to disentangle the latent space symmetrically. We demonstrate that this method can be applied easily to existing variational autoencoder (VAE) variants to encourage symmetric disentanglement, improve reconstruction quality, and prevent posterior collapse without computation overhead. We also demonstrate that this method can boost existing models to learn more transferable representations and generate more representative samples for the input distribution which can alleviate catastrophic forgetting using generative replay under continual learning settings.","link":"http://arxiv.org/abs/2303.12255v1","created":"2023-03-22","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
