{"title":"Recommendations for a newbie","description":"I've been reading a lot of articles about AI in general, machine learning and NLP etc but I want to learn more about NLP, creating desktop and mobile apps for questions-answering and summarizing texts. \n\nI've done programming in javascript and C# in the past and I wonder if that is enough or if I must learn python as well. \n\nWhat are your recommendations regarding language, tools, APIs, models, transformers etc and why should I start with these?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11pdpc3/recommendations_for_a_newbie/","created":"2023-03-12","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":9}}
{"title":"Best approach for sarcasm subcategory classification?","description":" Hi All,\n\nI  am currently working on my thesis which is attempting to build on  existing research in the field of sarcasm detection within NLP and  sentiment analysis. The task outlined is to basically build a model  which can identify subcategories of sarcasm (e.g. irony, overstatement,  rhetorical questions etc.). The dataset includes values for whether a  phrase is sarcastic or not, and which subcategories the phrase fits into  (there can be overlap between categories). I have fine-tuned BERT for  sarcasm detection and this works fine but that isn't really the task. My  questions are twofold essentially:\n\n\\-  Is the solely transformer-based approach useful in this instance, given  that it could build on existing research where previous scores obtained  in this task were fairly low. If so, does anyone have any resources on  how to build a multi-subclass classification model, or would it be  better to build separate models for each task?\n\n\\-  Would attempting to use a rule-based approach be more valuable e.g.  using vector semantics and embeddings to attempt to identify the  subcategories using this approach, and if so are there any particular  resources I should have a look at to understand how to implement this in  code? I am currently reading Jurafsky &amp; Martin's 3rd draft of  Speech and Language Processing, but I am unclear on how I could use this  to categorise the subcategories which are difficult to define by  linguistic experts as it is?\n\nI'm  sorry if this is a bit rambling and all over the place, I'm feeling  pretty lost and stressed, but happy to answer any questions and try to  clarify anything :)","link":"https://www.reddit.com/r/LanguageTechnology/comments/11ok7ug/best_approach_for_sarcasm_subcategory/","created":"2023-03-11","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":9}}
{"title":"Identify custom labels as well as existing labels with Spacy v3","description":"Hi all,\n\nI want to train a model with custom labels, and use it in combination with a pretrained model in Spacy v3.\n\nFor example for this code:\n\n    import spacy\n    import random\n    import json\n    \n    # Load the spaCy NLP model\n    nlp = spacy.load('en_core_web_lg')\n    \n    # Define the training data\n    train_data = [\n        ('These tomatoes are red and tasty.', {'entities': [(6, 14, 'VEGETABLE')]}),\n        ('I like red tomatoes.', {'entities': [(11, 19, 'VEGETABLE')]}),\n       \n        ('These bananas are very green.', {'entities': [(6, 13, 'FRUIT')]}),\n        ('Where are my bananas?', {'entities': [(13, 20, 'FRUIT')]}),\n        ('Are there any bananas near?', {'entities': [(14, 21, 'FRUIT')]}),    \n    ]\n    \n    # Define the new entity labels\n    new_labels = [\"FRUIT\", \"VEGETABLE\"]\n    \n    # Add the new labels to the existing entity recognizer\n    ner = nlp.get_pipe(\"ner\")\n    for label in new_labels:\n        ner.add_label(label)\n    \n    # Set up the optimizer\n    #optimizer = nlp.begin_training()\n    optimizer = nlp.initialize()\n    \n    # Iterate over the training data and update the model\n    for i in range(10):\n        random.shuffle(train_data)\n        for text, annotations in train_data:\n            doc = nlp.make_doc(text)\n            example = spacy.training.Example.from_dict(doc, annotations)\n            nlp.update([example], sgd=optimizer)\n    \n    # Test the model\n    text = \"\"\"What kind of color have bananas &amp; tomatoes in London?\"\"\"\n    doc = nlp(text)\n    for ent in doc.ents:\n        print(ent.text, ent.label_)\n\nThe output is:\n\n    bananas FRUIT\n    tomatoes VEGETABLE\n\nThe custom labels are recognized, but why is \"London\" not recognized as \"GPE\"? How can I achieve it?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11nr3r2/identify_custom_labels_as_well_as_existing_labels/","created":"2023-03-10","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":1}}
{"title":"How to interpret actions","description":"Hey guys, I would like to be able to extract actions along with their objects. For example, in the sentence \"Paint all the walls red and hide all the doors and windows.\", I would like to extract the verbs \"paint\" and \"hide\", the objects \"walls, doors, windows\", the relationships \"paint-&gt;walls\", \"hide-&gt;doors, windows\", and the adverb relationship \"paint-&gt;red\".\n\nWhat tools/techniques would you suggest? Is deep learning the way to go?\n\n[Spacy](https://spacy.io/usage/rule-based-matching#dependencymatcher) and [Stanza](https://stanfordnlp.github.io/stanza/available\\_models.html) look promising, but I am not sure.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11nozbv/how_to_interpret_actions/","created":"2023-03-10","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":5}}
