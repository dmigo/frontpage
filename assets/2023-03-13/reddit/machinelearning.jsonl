{"title":"[D] Simple Questions Thread","description":"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!","link":"https://www.reddit.com/r/MachineLearning/comments/11pgj86/d_simple_questions_thread/","created":"2023-03-12","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":8}}
{"title":"[R] Introducing Ursa from Speechmatics | 25% improvement over Whisper","description":"Ursa is the world\u2019s most accurate speech-to-text system and delivers a relative accuracy gain of 22% and 25%\u00a0versus Microsoft and OpenAI's Whisper respectively. \n\nFind out more and try it for free with just one click: [www.speechmatics.com/ursa](http://www.speechmatics.com/ursa) \n\nSpeechmatics achieved this by building on the scaling laws from DeepMind\u2019s Chinchilla paper and applying them to large self-supervised learning models for speech. By scaling to 2 billion parameters, the models can learn richer acoustic features from over 1 million hours of unlabeled multi-lingual data, allowing Ursa to understand a larger spectrum of voices.\n\nhttps://preview.redd.it/y54g784nudna1.png?width=1024&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1ed83c647697e2dfb95ed2277377fadc52e4b8f4","link":"https://www.reddit.com/r/MachineLearning/comments/11prxd9/r_introducing_ursa_from_speechmatics_25/","created":"2023-03-12","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":24}}
{"title":"[Discussion] Searching for end-to-end MLOps training solution","description":"I am working in a small research group and we recently found a problem with our resource utilization. We have 11 servers with 4 gpus in each and I am looking for an automatic execution manager with a queue. Currently we don't have anything in place and just write in a table which GPU is occupied by who, which, as you can imagine, is not ideal, our current utilization is around 50-60%. So we came up with the following two requirements:\n\n* Queue for training/inference tasks with dynamic GPU allocation (ex. you can launch 4 tasks on one machine that require 1 GPU each, or 2 tasks with 2 GPU, etc.)\n* Ability to reserve GPUs so that you can connect to the host and work directly (for example you want to launch 3rd party repository with complex environment setup)\n\nThe only solution I found so far is ClearML with clearml agent, but there are two problems with it, the first one is dynamic GPU allocation is available only in enterprise edition, which means that we need to reserve some hosts to run 2 GPU tasks and some that run 1 GPU tasks, which is not ideal. The second is that you can't directly tell clearml to not use some gpu for the time, so we used a crutch - launch an task with infinite loop in it that does nothing, which is once again is not ideal.\n\nHave some of you encountered a similar problems? How did you solve it? Maybe there is a solution that we missed, any help is appreciated.","link":"https://www.reddit.com/r/MachineLearning/comments/11q53pp/discussion_searching_for_endtoend_mlops_training/","created":"2023-03-13","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[D] What's the mathematical notation for \"top k argmax\"?","description":"I'm trying to express something in mathematical notation - let's say I want to get the top k indices for which a function obtains highest values. So, something like argmax, but for a general k number of indices instead of just the top index. Is there a standard notation for this?","link":"https://www.reddit.com/r/MachineLearning/comments/11po6qw/d_whats_the_mathematical_notation_for_top_k_argmax/","created":"2023-03-12","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":9}}
{"title":"[P] vanilla-llama an hackable plain-pytorch implementation of LLaMA that can be run on any system (if you have enough resources)","description":"I put together this plain pytorch implementation of LLaMA (i just substituted the fairscale layers with the native ones and converted the weights accordingly) that can be more easily run in different environments. \n\nThe big problem with the official implementation is that in order to run the 65B version you need 8 GPUs no matter what, and to run the 30B version you need 4 and so on. In reality you can easily fit the 65B version in 2 A100 with 100G of VRAM.\n\nvanilla-llama solves this problem. You just need to have enough memory and the model will be load in all the available GPUs.\n\n&amp;#x200B;\n\n[https://github.com/galatolofederico/vanilla-llama](https://github.com/galatolofederico/vanilla-llama)","link":"https://www.reddit.com/r/MachineLearning/comments/11ozl85/p_vanillallama_an_hackable_plainpytorch/","created":"2023-03-12","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":7}}
{"title":"[P] Introducing confidenceinterval, the long missing python library for computing confidence intervals","description":"[https://github.com/jacobgil/confidenceinterval](https://github.com/jacobgil/confidenceinterval)\n\npip install confidenceinterval\n\ntldr: You don't have an excuse anymore to not use confidence intervals !\n\n&amp;#x200B;\n\nIn statistics, confidence intervals are commonly reported along accuracy metrics to help interpret them.\n\nFor example, an AUC metric might be 0.9 but if the 95% confidence interval is in the range \\[0.7, 0.96\\], we can't confidently say we didn't just get lucky - we should be really careful making decisions around that result.\n\nMore formally, a confidence interval gives us a range on where the true unknown accuracy metric could be, and a 95% confidence interval means that if we would repeat the experiment many times, 95% of the confidence-intervals we reported would have the actual true metric (which is unknown) inside them - coverage.\n\nConfidence intervals are usually computed analytically, by making some assumptions about the metric distribution and using the central limit theorem,or by using bootstrapping - resampling the results again and again, computing the metric, and checking the resulting distribution.\n\nHowever, in the python data science world, I rarely saw these being used. I guess part of the reason is the culture, where many data science practitioners don't come from the statistics world. But I think the main reason is that there aren't easy to use libraries that do this. While in the R language there is fantastic support for confidence intervals, for python there are mostly scattered pieces of code and blog posts.\n\n&amp;#x200B;\n\nThe confidenceinterval package keeps the clean and popular scikit-learn metric API,\n\ne.g roc\\_auc\\_score(y\\_true, y\\_pred), but also returns confidence intervals.\n\nIt supports analytical computations for many methods (including AUC with the delong method, or F1 with macro, micro averaging, following the recent results from [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8936911/#APP2](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8936911/#APP2), or binary proportions like the TPR using binomial CI methods like the wilson interval).\n\nIt can be easily switched to using bootstrapping (with several supported bootstrapping methods),\n\nand also gives you a way to easily compute the confidence interval for any metric with bootstrapping.","link":"https://www.reddit.com/r/MachineLearning/comments/11orezx/p_introducing_confidenceinterval_the_long_missing/","created":"2023-03-11","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":9}}
{"title":"[D] Tracking Dancing People","description":" \n\nHi everyone!\n\nOne interesting problem has been posed to me!\n\nGiven people dancing in a video, tracking a single one. The reference video I have been given is: [https://www.youtube.com/watch?v=g0BvpzR\\_2MQ](https://www.youtube.com/watch?v=g0BvpzR_2MQ).   As you will see in the video, occlusions happen an incredible amount,  and they are all wearing roughly similar clothing. Further, sometimes  the people get off screen, then come back on.\n\nI  have tried many different things, but I am unable to find a good way to  track a single person, as the re-identification is iffy.\n\nAny help would be appreciated!","link":"https://www.reddit.com/r/MachineLearning/comments/11p9p67/d_tracking_dancing_people/","created":"2023-03-12","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":2}}
{"title":"[P] GITModel: Dynamically generate high-quality hierarchical topic tree representations of GitHub repositories using customizable GNN message passing layers, chatgpt, and topic modeling.","description":"Decompose Python libraries and generate Coherent hierarchical topic models of the repository.  \n[https://github.com/danielpatrickhug/GitModel](https://github.com/danielpatrickhug/GitModel)\n\nThe ability to bootstrap its own codebase is a powerful feature as it allows for efficient self-improvement and expansion. It means that the codebase is designed in such a way that it can use its own output as an input to improve itself. In the context of GitModel, this feature allows for the efficient improvement and expansion of its own codebase. By using its own output to generate hierarchical topic trees of GitHub repositories, it can analyze and extract insights from its own codebase and other codebases to improve its functionality. This can lead to more efficient and effective code generation, better semantic graph generation, and improved text generation capabilities.\n\n  \nI spent around 10 hours today on a major refactor creating a simple pipeline abstraction and allowing dynamic instantiation from yaml configs. It now also supports multiple GNN heads.\n\nPlease try it out and let me know what you think!\n\nExample:  \n[https://github.com/deepmind/clrs](https://github.com/deepmind/clrs)\n\nhttps://preview.redd.it/ut4fc6c401na1.png?width=1506&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b039242432c1f0526d1d81eadbfe8abc1168d2fd","link":"https://www.reddit.com/r/MachineLearning/comments/11o97on/p_gitmodel_dynamically_generate_highquality/","created":"2023-03-11","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":25}}
{"title":"[D] Unsupervised Learning \u2014 have there been any big advances recently?","description":"I feel like unsupervised learning models have always been the less-sexy part of machine learning. There's been some interesting solutions like scBERT and others in the space of single-cell RNAseq, but other than that it seems like clustering, dimensionality reduction, etc, has been mostly the same for years now.\n\nWhat big stuff has come out, and what's on the radar?","link":"https://www.reddit.com/r/MachineLearning/comments/11onol2/d_unsupervised_learning_have_there_been_any_big/","created":"2023-03-11","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":8}}
{"title":"Text2Image ControlNet and Stable Diffusion [R]","description":"In this tutorial, we will show you how to create beautiful and high-quality images from text using the powerful combination of diffusion model and ControlNet. \n\nText2Image generation is a fascinating field of AI that enables machines to understand and visualize human language in a more creative way.\n\n we will walk you through the step-by-step process of how to use the diffusion model and ControlNet to generate images from text. By the end of this tutorial, you will have a thorough understanding of text2image generation and how to use diffusion model and ControlNet to create stunning images from text. You will also have the knowledge and skills to apply these techniques to your own projects and experiments.\n\n So, get ready to dive into the exciting world of text2image generation and start creating your own beautiful images from text today!\n\nhttps://youtu.be/0D5Nlo2REb0","link":"https://www.reddit.com/r/MachineLearning/comments/11p1oqq/text2image_controlnet_and_stable_diffusion_r/","created":"2023-03-12","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[D] What model, methodology is state of the art to calculate similarity of given 2 images?","description":"I am trying to calculate similarity of given 2 images.\n\nI want to utilize this for calculating similarity of different clothes.\n\nSo what is the state of the art methodology / AI model to calculate?","link":"https://www.reddit.com/r/MachineLearning/comments/11os9wy/d_what_model_methodology_is_state_of_the_art_to/","created":"2023-03-11","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":7}}
{"title":"[P] RWKV 14B is a strong chatbot despite only trained on Pile (16G VRAM for 14B ctx4096 INT8, more optimizations incoming)","description":"The latest CharRWKV v2 has a new chat prompt (works for any topic), and here are some raw user chats with RWKV-4-Pile-14B-20230228-ctx4096-test663 model (topp=0.85, temp=1.0, presence penalty 0.2, frequency penalty 0.5). You are welcome to try ChatRWKV v2:  [https://github.com/BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV)\n\nAnd please keep in mind that RWKV is 100% RNN :) Pile v1 date cutoff is year 2020.\n\n[Chat #1](https://preview.redd.it/ripvptomexma1.png?width=438&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=29ed1cd499dc4d693dee32ad7550a7910402d033)\n\n[Chat #2](https://preview.redd.it/8t75njnnexma1.png?width=438&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d20186f2e423d321817b79e6e559dc74a42bf0b8)\n\nThese are surprisingly good because RWKV is only trained on the Pile (and 100% RNN). No finetuning. No instruct tuning. No RLHF. You are welcome to try it.\n\n1. Update ChatRWKV v2 \\[and rwkv pip package\\] to latest version.\n2. Use  [https://huggingface.co/BlinkDL/rwkv-4-pile-14b/blob/main/RWKV-4-Pile-14B-20230228-ctx4096-test663.pth](https://huggingface.co/BlinkDL/rwkv-4-pile-14b/blob/main/RWKV-4-Pile-14B-20230228-ctx4096-test663.pth)\n3. Run v2/chat.py and enjoy.\n\nChatRWKV v2 supports INT8 now (with my crappy slow quantization, **works for windows, supports any GPU**, 16G VRAM for 14B if you offload final layer to CPU). And you can offload more layers to CPU to run it with 3G VRAM though that will be very slow :) More optimizations are coming.\n\nOr you can try the 7B model (less coherency) and 3B model (not very coherent, but still fun).","link":"https://www.reddit.com/r/MachineLearning/comments/11nre6t/p_rwkv_14b_is_a_strong_chatbot_despite_only/","created":"2023-03-10","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":29}}
{"title":"[D] Development challenges of an autonomous gardening robot using object detection and mapping.","description":"Why do some folk think that this futuristic type of robot can't logically achieve a broad array of stated ML tasks?\n\n[https://youtu.be/EYTiTh7\\_zO4](https://youtu.be/EYTiTh7_zO4)\n\nI see the dev cost of this robot as being 100 times less than a self-driving car: single error fatality risk, unlimited chaotic cities, 90mph compute time limits, make self-driving cars unfeasible compared to multitask garden robots. \n\nFruit-picking is very difficult using AI, but weeding, digging, sowing seeds, irrigation, are fairly easy tasks, and an experienced developer knows that anything is possible with logic.\n\nMillions of acres of farmland are chemically and brutally treated for food that is wrapped in plastic, shipped hundreds of miles, to supermarkets, so as an environmental chemist, rural processes analyst and EE dabbler, I have created an emulator prototype for a garden robot :)","link":"https://www.reddit.com/r/MachineLearning/comments/11oaek2/d_development_challenges_of_an_autonomous/","created":"2023-03-11","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":10}}
{"title":"[D] Statsmodels ARIMA model predict function not working","description":"I trained my ARIMA model by doing the following\n\n`from statsmodels.tsa.arima.model import ARIMA`\n\n`model_ar = ARIMA(data.Num_Passengers, order=(1,0, 0))`\n\n`results_ar = model_ar.fit()results_ar.summary()`\n\n&amp;#x200B;\n\nThe code worked with the resulting output\n\n&amp;#x200B;\n\nhttps://preview.redd.it/zi8f1lhak5na1.png?width=746&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=3f5ef9fe1504892e4ce48b5287d8b834f1dfdb27\n\nBut then I tried predicting on the testing dataset, and I got the following error.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/uni7ws1ck5na1.png?width=1675&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ce520334f3b1e420a101adda9f43868714617272\n\nAm I just messing something up, is anyone else dealing with this error?\n\nIs there another way to use the predict function, or is it really unimplemented.\n\nCould you please help me out with this?\n\nHow would I overwrite the method?","link":"https://www.reddit.com/r/MachineLearning/comments/11or4qb/d_statsmodels_arima_model_predict_function_not/","created":"2023-03-11","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":8}}
{"title":"[D] Looking for eye gaze detection dataset","description":" I have a project in my university where i have to make a CNN able to predict where the person is looking on a laptop screen using the webcam of the laptop, does anyone know where i can find data sets that can help me train the network","link":"https://www.reddit.com/r/MachineLearning/comments/11oqhhj/d_looking_for_eye_gaze_detection_dataset/","created":"2023-03-11","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":5}}
