{"title":"Where does the input sentence length is dealt with in a transformer?","description":"Someone understands why the vector size still depends on the sequence length after the positional encoding in a transformer? I thought it needed to be independent from the sequence length at one point?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11wuo4y/where_does_the_input_sentence_length_is_dealt/","created":"2023-03-20","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":8}}
{"title":"Translate a meeting","description":"Hi Everyone,\n\nI need to translate a recording of a meeting of 2 hours where Dutch and French are spoken. I speak Dutch but I don't speak French. What is the best voice translator for longer spoken texts? Does anyone have tips?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11wh2zm/translate_a_meeting/","created":"2023-03-20","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"New to Headline Quality subfield -- what are some good models that are readily customizable for domain nomenclature specificity?","description":"Due to need at work, I've been plugged into a project in the Headline Quality NLP subarea despite my complete inexperience in NLP. The project involves assessing headline/title quality of some pharmaceutical reports and estimating semantic divergence between headline and articles. I'm working through some papers like [this one by Omidvar et al.](https://arxiv.org/abs/1911.11139), but don't need state of the art at the moment, just some quick and dirty initial results, and am looking for some input about types of models I should look at for getting started on limited computational resources (I'll only have access to one GPU at most). I'm particularly interested in models I can flexibly modify/hardcode to account for corporate and pharmaceutical nomenclature and terminology. Is this a problem I should still throw transformer models at, even with a dearth of GPU resources, or do any older, simpler models come to mind? Thank you for your time and help!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11w7mh3/new_to_headline_quality_subfield_what_are_some/","created":"2023-03-20","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Modern Topic Modeling/Discovery","description":"I was wondering what are the modern techniques for topic discovery for short and long text. It seems this topic to be slower advancing compared to the rest. I am aware of bertopic but tbh I always have issues finetuning it. \n\nOn a second thought I was thinking to use qna/chat gpt models in order to generate models, so I wanted to ask your opinion on some potential prompts that I could use. Essentially  a bit of brainstorming. I will open source all the gathered ideas along with mine and share the link here :)","link":"https://www.reddit.com/r/LanguageTechnology/comments/11vwtn3/modern_topic_modelingdiscovery/","created":"2023-03-19","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":1}}
{"title":"multitask learning in classification thesis","description":"Hi! I\u2019m in my senior year and I have two months to write a thesis about multitask learning in nlp classifiers. I\u2019ve read a decent amount of papers on the topic and I would like to share my plan, because I need some feedback!\n\nSo, in my thesis I explore the influence of multitask learning on multilingual models in classification tasks. I\u2019ve decided to use only hard-parameter sharing models \u2014 seems that soft-parameter sharing and other architectural designs (sluice networks and so on) are not SOTA.\n\nModels: I\u2019ve picked XLM-R to investigate encoder + multiple classification heads architecture and mT5 as a SOTA model (and a predecessor of prompt-based models).\n\nDataset: XGLUE, from which I took 6 classification tasks. \n\nAreas of interest: 1. Task relatedness (I found it to be a reaaally ambiguous term. In essence, I would visualise and analyse the embeddings of tasks, but I don\u2019t know how, given they all have two input sentences)\n2. Catastrophic language and task forgetting (my plan is to translate data examples to the target languages and mix languages inside every example \u2014 should it be called \u201clanguage mismatch\u201d?\nThen I would compare it to cross-lingual transfer baselines, where we only train the model on english data)\n3. Shift to multitask pre-training (Small in-house dataset in a foreign language to test models pre-trained on all (or maybe only related) XGLUE tasks\n\nWhat do you think? What should I incorporate? What hypotheses should I test? Are there any recent developments (e.g. adapter layers) that are relevant and open (no GPT I guess)?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11w0i1s/multitask_learning_in_classification_thesis/","created":"2023-03-19","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":2}}
{"title":"Are pre-trained word embeddings (word2vec, glove, fasttext) obsolete now? given wide use of pre-trained languages models like bert etc","description":"","link":"https://www.reddit.com/r/LanguageTechnology/comments/11vav4y/are_pretrained_word_embeddings_word2vec_glove/","created":"2023-03-19","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":21}}
{"title":"Choosing Non-Linear vs Linear","description":"Hello all,\n\nIs there a process for deciding whether to use a Non-Linear or Linear text classifier? From what I have been reading, it seems like people develop scatter plots from their data points to see if their data is linearly separable. \nDo people do this with text data? What does everyone do to evaluate their choice of model?\n\nThanks!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11uyz56/choosing_nonlinear_vs_linear/","created":"2023-03-18","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
