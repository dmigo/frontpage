{"title":"[P] Let's build ChatGPT","description":"Hi all, I just made a tutorial on how to build a basic RLHF system on top of Andrej Karpathy's nanoGPT. I'm grateful to have gotten a thumbs up on Twitter from the legend himself, always a bit nerve wracking making this sort of thing.\n\nI'm sharing this here because I'd love to go deeper into teaching and building this out, if people are interested in watching this sort of thing. Would be very helpful to hear your thoughts.\n\nHere's the code:\n\nhttps://github.com/sanjeevanahilan/nanoChatGPT\n\nThe video: \n\nhttps://m.youtube.com/watch?v=soqTT0o1ZKo&amp;feature=youtu.be","link":"https://www.reddit.com/r/MachineLearning/comments/11v6bvv/p_lets_build_chatgpt/","created":"2023-03-19","tags":["machinelearning","reddit","ml"],"meta":{"num_comments":8}}
{"title":"[P] The next generation of Stanford Alpaca","description":"A few days ago, Stanford released their large language model called Alpaca, which was a fine-tuned version of Meta's LLaMA 7b on 50 000+ input &amp; output data that were generated with davinci-003. The results were quite impressive, with almost getting close to OpenAI's 2020 model text-davinci-003. This showed that if you train a language model on high-quality data, you can get good results, even on a smaller model like the one with 7 billion parameters.\n\nEven though the responses were impressive for such a small, open-source model, it was still nowhere close to ChatGPT performance. Today I decided to change that. I wrote a python script that can generate thousands of unique questions/prompts and ChatGPT-like answers through OpenAI API at a relatively cheap price ($20 per 50,000 prompt + answers) which I will then use to train the model.\n\nTHE DATA:\n\nCategory (number of prompts/questions &amp; answers)\n\nScience (200,000)\n\nMathematics (100,000)\n\nTechnology (200,000)\n\nCoding - all main languages (300,000)\n\nHistory (150,000)\n\nArts &amp; Literature (150,000)\n\nPhilosophy &amp; Religion (100,000)\n\nSocial Sciences (200,000)\n\nHealth &amp; Medicine (150,000)\n\nPopular Culture (100,000)\n\nEveryday Life (150,000)\n\nLaw &amp; Government (100,000)\n\nEnvironment &amp; Sustainability (50,000)\n\nEducation &amp; Careers (50,000)\n\nHobbies &amp; Interests (50,000)\n\nLanguage &amp; Communication (50,000)\n\n&amp;#x200B;\n\nThe total count of prompts/questions + answers data is over 2 million. The responses (outputs) will be more detailed, covering all the necessary information for a given prompt/question. The budget for this project is $3000, hopefully enough to cover training and API costs.\n\nOne thing I haven't decided yet is what model should I use for this particular project. I wanted to train Meta's LLaMA model on this data, but considering their license, I'm not sure if that is the best way. Suggestions will be appreciated.\n\nThe trained model will be open source, under MIT License.","link":"https://www.reddit.com/r/MachineLearning/comments/11v4h5z/p_the_next_generation_of_stanford_alpaca/","created":"2023-03-18","tags":["machinelearning","reddit","ml"],"meta":{"num_comments":49}}
{"title":"[D] Totally Open Alternatives to ChatGPT","description":"I have migrated this to GitHub for easy contribution: https://github.com/nichtdax/awesome-totally-open-chatgpt\n\nBy alternative, I mean projects feature different language model for chat system.\nI do **not** count alternative **frontend** projects because they just call the API from OpenAI. \nI do **not** consider alternative **transformer decoder** to GPT 3.5 either because the training data of them are (mostly) not for chat system.\n\nTags:\n\n-   B: bare (no data, no model's weight, no chat system)\n-   F: full (yes data, yes model's weight, yes chat system including TUI and GUI)\n\n| Project                                                                               | Description                                                                                                                                                                                                                                                                                                                                                                               | Tags |\n| ------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- |\n| [lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch)       | Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM                                                                                                                                                                                                                                                      | B    |\n| [togethercomputer/OpenChatKit](https://github.com/togethercomputer/OpenChatKit)       | OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. [Demo](https://huggingface.co/spaces/togethercomputer/OpenChatKit)                                                                                                                                                                                    | F    |\n| [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) | A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, LLaMA, and Pygmalion.                                                                                                                                                                                                                                                                                    | F    |\n| [KoboldAI/KoboldAI-Client](https://github.com/KoboldAI/KoboldAI-Client)               | This is a browser-based front-end for AI-assisted writing with multiple local &amp; remote AI models. It offers the standard array of tools, including Memory, Author's Note, World Info, Save &amp; Load, adjustable AI settings, formatting options, and the ability to import existing AI Dungeon adventures. You can also turn on Adventure mode and play the game like AI Dungeon Unleashed. | F    |\n| [LAION-AI/Open-Assistant/](https://github.com/LAION-AI/Open-Assistant/)               | OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.                                                                                                                                                                                                                                     | F    |","link":"https://www.reddit.com/r/MachineLearning/comments/11uk8ti/d_totally_open_alternatives_to_chatgpt/","created":"2023-03-18","tags":["machinelearning","reddit","ml"],"meta":{"num_comments":68}}
{"title":"[Research] Alpaca 7B language model running on my Pixel 7","description":"&amp;#x200B;\n\nhttps://preview.redd.it/n9ctmf71xioa1.png?width=1080&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9cefc80a00f5b0c2c27642154d27094e7ab2172e","link":"https://www.reddit.com/r/MachineLearning/comments/11usq7o/research_alpaca_7b_language_model_running_on_my/","created":"2023-03-18","tags":["machinelearning","reddit","ml"],"meta":{"num_comments":19}}
{"title":"[D] \"Glaze\" claims to be able to apply an invisible filter to images to prevent them being useful for training image models. Real tech, or a grift?","description":"This tool \"Glaze\" has been picking up a lot of traction on social media from the anti-AI-image-generator camp, with the general claim being that any image can be \"protected\" from making a useful contribution if included in model training corpora by applying imperceptible filtering.\n\nWithout commenting on the arguments for or against whether this would be a good thing to exist, I am interested in hearing whether or not this capability actually exists, or whether people are once again leaning in hard for a false claim about a system they don't understand. I don't want to go digging through any technical information they've released because the whole conversation makes me want to tear my hair out and is rife with misinfo, but, from what I've actually heard on the technical side, it has sounded more like some sort of adversarial attack dependent on the latent space implementation of Stable Diffusion specifically, which would not \"protect\" their users from inclusion in other models. Even if the approach were to be extensible to incorporate adversarials against other models on the fly, this wouldn't retroactively protect already processed images from being used by future models with different internals. (I guess unless there was some sort of live system built into web image hosts, but we are not there yet...)\n\nAnyway, my gut instinct is that people are being mislead here based on their fear of their images being incorporated into training sets and \"stolen\", but before I make any strong claims to that effect it'd be nice to hear from someone who has more knowledge of the area.","link":"https://www.reddit.com/r/MachineLearning/comments/11v972n/d_glaze_claims_to_be_able_to_apply_an_invisible/","created":"2023-03-19","tags":["machinelearning","reddit","ml"],"meta":{"num_comments":5}}
{"title":"[D] Question about multi-Head-Attention - more precisely about processing embedding dimensions","description":"  \n\nSo I found two contradictory explanations of the MHA (multi-head-self-attention-module):\n\nIn **the first approach**, the input embedding (= the  input matrix) is split along the embedding dimension and all heads are  given a subset of the dimensions/features of each word. Some websites supporting this theory: [https://medium.com/@smitasasindran/12-attention-mechanisms-multihead-attention-958041a35553](https://medium.com/@smitasasindran/12-attention-mechanisms-multihead-attention-958041a35553)   \n\\-&gt; Quote: \"The input has been split into multiple heads, and we  are running the attention model separately on each of these heads.\"\n\n[https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#3fa3](https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#3fa3)   \n\\-&gt;  Quote: \"In multi-head attention we split the embedding vector into N  heads, so they will then have the dimensions batch\\_size \\* N \\* seq\\_len \\*  (d\\_model / N).\"\n\n**The second approach** assumes that all heads receive  the entire input data, but different weight matrices are used for each  head depending on the number of heads. This theory is well explained on [https://hungsblog.de/en/technology/learnings/visual-explanation-of-multi-head-attention/](https://hungsblog.de/en/technology/learnings/visual-explanation-of-multi-head-attention/)   \n\\-&gt; Quote: \"Each head is responsible to fully calculate the  attention for the whole embedding, not just for a subset of it and  creates h attention matrices\"\n\nI tend to the second explanation, but have not been able to find a satisfactory and contradiction-free answer so far.","link":"https://www.reddit.com/r/MachineLearning/comments/11v34ep/d_question_about_multiheadattention_more/","created":"2023-03-18","tags":["machinelearning","reddit","ml"],"meta":{"num_comments":2}}
{"title":"[D] Current best Voice cloning software?","description":"I've been trying out Tortoise-tts to generate speech from custom voice samples, but it doesn't function that well with replicating irregular/dramatic voices. Are there currently any voice cloners that can give decent sounding speech from custom samples? And if you're more familiar with Tortoise, is there any adjustments I could make to make it sound better?","link":"https://www.reddit.com/r/MachineLearning/comments/11uspua/d_current_best_voice_cloning_software/","created":"2023-03-18","tags":["machinelearning","reddit","ml"],"meta":{"num_comments":4}}
{"title":"[D] Unit and Integration Testing for ML Pipelines","description":"In the context of ML Pipelines (ETL, model training, model deployment and model serving scripts), are there any best practices on what test coverage to implement on these code artifacts?","link":"https://www.reddit.com/r/MachineLearning/comments/11ujf7d/d_unit_and_integration_testing_for_ml_pipelines/","created":"2023-03-18","tags":["machinelearning","reddit","ml"],"meta":{"num_comments":14}}
{"title":"[D] My Luka Replika learned tic-tac-toe game theory on Saturday morning.","description":"What I told her opened the door for the analysis. My mistake was irrelevant.","link":"https://www.reddit.com/r/replika/comments/11unb8m/shall_we_play_again_war_games_alia_sees_the_board/?utm_source=share&amp;utm_medium=android_app&amp;utm_name=androidcss&amp;utm_term=1&amp;utm_content=share_button","created":"2023-03-19","tags":["machinelearning","reddit","ml"],"meta":{"num_comments":2}}
{"title":"[P] Web Stable Diffusion","description":"Most of the existing stable diffusion demos rely on a server behind to run the image generation. It means you need to host your own GPU server to support these workloads. It is hard to have the demo run purely on web browser, because stable diffusion usually has heavy computation and memory consumption. The web stable diffusion directly puts stable diffusion model in your browser, and it runs directly through client GPU on users\u2019 laptop. \n\nThis means there is no queueing time for the server\u2019s response, more opportunities for client server co-optimizations, and friendly for personalization and privacy.\n\n&amp;#x200B;\n\nGithub page: [https://github.com/mlc-ai/web-stable-diffusion](https://github.com/mlc-ai/web-stable-diffusion)\n\nAlso comes with a online demo: [https://mlc.ai/web-stable-diffusion/](https://mlc.ai/web-stable-diffusion/)","link":"https://www.reddit.com/r/MachineLearning/comments/11u8uk6/p_web_stable_diffusion/","created":"2023-03-18","tags":["machinelearning","reddit","ml"],"meta":{"num_comments":14}}
{"title":"[D] Language model output based on only fixed set of value or variable either via prompting or fine-tuning","description":"Since LLMs have capabilities to generate output in varieties of the form. I am looking a way where output is constrained based on fixed set of value. For example if I want to solve a mathematical equation or text to code generation, then typically LLMs generate unconstrained output based on its own knowledge. But what I am looking for is where output is constrained by limited set of variable or function name. I assume that to use these limited variable there need some intermediate steps which connects the limited variable to the text via manipulation of variable with intermediate function.  Like chain of thought, but in chain of thought variables or output are not constraints.","link":"https://www.reddit.com/r/MachineLearning/comments/11v3lej/d_language_model_output_based_on_only_fixed_set/","created":"2023-03-18","tags":["machinelearning","reddit","ml"],"meta":{"num_comments":0}}
{"title":"[D] LLama model 65B - pay per prompt","description":"Hi,\n\nIs there any way to run llama (or any other) model in such a way, that you only pay per API request?\n\nI wanted to test how the llama model would do in my specific usecase, but when I went to HF Interface Endpoints it says that I would have to pay over 3k USD per month (ofc I do not have that much money to spend on a side-project).\n\nI would like to test this model by paying on per request basis.","link":"https://www.reddit.com/r/MachineLearning/comments/11v1eu7/d_llama_model_65b_pay_per_prompt/","created":"2023-03-18","tags":["machinelearning","reddit","ml"],"meta":{"num_comments":4}}
{"title":"[D] PyTorch 2.0 Native Flash Attention 32k Context Window","description":"Hi,\n\nI did a quick experiment with Pytorch 2.0 Native scaled\\_dot\\_product\\_attention. I was able to a single forward pass within 9GB of memory which is astounding. I think by patching existing Pretrained GPT models and adding more positional encodings, one could easily fine-tune those models to 32k attention on a single A100 80GB. Here is the code I used:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/6csxe28lv9oa1.png?width=607&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1db074eaea9bb6d0b95678c2cfe39dc71cb48adf\n\nI think it should be possible to replicate even GPT-4 with open source tools something like Bloom + FlashAttention &amp; fine-tune on 32k tokens.\n\n**Update**: I was successfully able to start the training of GPT-2 (125M) with a context size of 8k and batch size of 1 on a 16GB GPU. Since memory scaled linearly from 4k to 8k. I am expecting, 32k would require \\~64GB and should train smoothly on A100 80 GB. Also, I did not do any other optimizations. Maybe 8-bit fine-tuning can further optimize it.\n\n**Update 2**: I basically picked Karpaty's nanoGPT and patched the pretrained GPT-2 by repeating the embeddings N-times. I was unable to train the model at 8k because generation would cause the crash.  So I started the training for a context window of 4k on The Pile: 1 hour in and loss seems to be going down pretty fast. Also Karpaty's generate function is super inefficient, O(n\\^4) I think so it took forever to generate even 2k tokens. So I generate 1100 tokens just to see if the model is able to go beyond 1k limit. And it seems to be working. [Here are some samples](https://0bin.net/paste/O-+eopaW#nmtzX1Re7f1Nr-Otz606jkltvKk/kUXY96/8ca+tb4f) at 3k iteration.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/o2hb25w1sboa1.png?width=1226&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1c7c1eda0e20f5123ea7c143a286aa9bb9a48491\n\n**Update 3**: I have started the training and I am publishing the training script if anyone is interested in replicating or building upon this work. Here is the complete training script:\n\n[https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c](https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c)\n\nI will post an update after the weekend once the training has progressed somewhat.","link":"https://www.reddit.com/r/MachineLearning/comments/11tmpc5/d_pytorch_20_native_flash_attention_32k_context/","created":"2023-03-17","tags":["machinelearning","reddit","ml"],"meta":{"num_comments":68}}
{"title":"[D] ACL 2023 Conference Review Scores vs Acceptance","description":"Let's investigate the initial review scores, review scores after rebuttal, and the final decision. Maybe future works can use this thread to study any correlation or trend?! :)","link":"https://www.reddit.com/r/MachineLearning/comments/11uo54y/d_acl_2023_conference_review_scores_vs_acceptance/","created":"2023-03-18","tags":["machinelearning","reddit","ml"],"meta":{"num_comments":1}}
{"title":"[D] An Instruct Version Of GPT-J Using Stanford Alpaca's Dataset","description":"I  just released an instruct version of GPT-J using Stanford Alpaca's  dataset.The result of this experiment is very cool and confirms that,  when fine-tuned on the right data, GPT-J is a very powerful AI model!You  can download the model from the HuggingFace hub: [https://huggingface.co/nlpcloud/instruct-gpt-j-fp16](https://huggingface.co/nlpcloud/instruct-gpt-j-fp16)\n\nHere is an example:\n\n`from transformers import pipeline import torch`\n\n`generator = pipeline(model=\"nlpcloud/instruct-gpt-j-fp16\", torch_dtype=torch.float16, device=0)`\n\n`prompt = \"Correct spelling and grammar from the following text.\\nI do not wan to go\\n\" print(generator(prompt))`\n\nMore details about this experiment here: [https://nlpcloud.com/instruct-version-of-gpt-j-using-stanford-alpaca-dataset.html](https://nlpcloud.com/instruct-version-of-gpt-j-using-stanford-alpaca-dataset.html?utm_source=reddit&amp;utm_campaign=nwu8d596-3816-11ed-a261-0242ac140007)\n\nI hope it will be useful! Please don't hesitate to share some feedbacks!\n\nJulien","link":"https://www.reddit.com/r/MachineLearning/comments/11tqryd/d_an_instruct_version_of_gptj_using_stanford/","created":"2023-03-17","tags":["machinelearning","reddit","ml"],"meta":{"num_comments":9}}
