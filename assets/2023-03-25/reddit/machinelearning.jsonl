{"title":"Reminder: Use the report button and read the rules!","description":"","link":"https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/","created":"2023-03-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[R] Hello Dolly: Democratizing the magic of ChatGPT with open models","description":"Databricks shows that anyone can take a dated off-the-shelf open source large language model (LLM) and give it magical ChatGPT-like instruction following ability by training it in less than three hours on one machine, using high-quality training data.\n\nThey fine tuned GPT-J using the Alpaca dataset.\n\nBlog: [https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)  \nGithub: [https://github.com/databrickslabs/dolly](https://github.com/databrickslabs/dolly)","link":"https://www.reddit.com/r/MachineLearning/comments/120usfk/r_hello_dolly_democratizing_the_magic_of_chatgpt/","created":"2023-03-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":57}}
{"title":"[R] Reflexion: an autonomous agent with dynamic memory and self-reflection - Noah Shinn et al 2023 Northeastern University Boston - Outperforms GPT-4 on HumanEval accuracy (0.67 --&gt; 0.88)!","description":"Paper: [https://arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366) \n\nBlog: [https://nanothoughts.substack.com/p/reflecting-on-reflexion](https://nanothoughts.substack.com/p/reflecting-on-reflexion) \n\nGithub: [https://github.com/noahshinn024/reflexion-human-eval](https://github.com/noahshinn024/reflexion-human-eval) \n\nTwitter: [https://twitter.com/johnjnay/status/1639362071807549446?s=20](https://twitter.com/johnjnay/status/1639362071807549446?s=20) \n\nAbstract:\n\n&gt;Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, **specifically the ability to learn from mistakes**. **Self-reflection allows humans to efficiently solve novel problems through a process of trial and error.** Building on recent research, we propose Reflexion, an approach that endows an agent with **dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities.** To achieve full automation, we introduce a straightforward yet effective heuristic that **enables the agent to pinpoint hallucination instances, avoid repetition in action sequences, and, in some environments, construct an internal memory map of the given environment.** To assess our approach, we evaluate the agent's ability to complete decision-making tasks in AlfWorld environments and knowledge-intensive, search-based question-and-answer tasks in HotPotQA environments. We observe success rates of 97% and 51%, respectively, and provide a discussion on the emergent property of self-reflection. \n\nhttps://preview.redd.it/4myf8xso9spa1.png?width=1600&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=867a16e1114108053d08d4cdf41485c8b29a132c\n\nhttps://preview.redd.it/bzupwyso9spa1.png?width=1600&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=95cacfe6b99756e7eed9ec8c40784f8c4cb94cee\n\nhttps://preview.redd.it/009352to9spa1.jpg?width=1185&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=5ccc52597d6e001c2ba754fc5f05afd1df09cd63\n\nhttps://preview.redd.it/ef9ykzso9spa1.jpg?width=1074&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=2701778aa5a9f3e80f683a1e3d0eaf0160928f54","link":"https://www.reddit.com/r/MachineLearning/comments/1215dbl/r_reflexion_an_autonomous_agent_with_dynamic/","created":"2023-03-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":40}}
{"title":"[D] I just realised: GPT-4 with image input can interpret any computer screen, any userinterface and any combination of them.","description":"GPT-4 is a multimodal model, which specifically accepts image and text inputs, and emits text outputs. And I just realised: You can layer this over any application, or even combinations of them. You can make a screenshot tool in which you can ask question.\n\nThis makes literally any current software with an GUI machine-interpretable. A multimodal language model could look at the exact same interface that you are. And thus you don't need advanced integrations anymore.\n\nOf course, a custom integration will almost always be better, since you have better acces to underlying data and commands, but the fact that it can immediately work on any program will be just insane.\n\nJust a thought I wanted to share, curious what everybody thinks.","link":"https://www.reddit.com/r/MachineLearning/comments/120guce/d_i_just_realised_gpt4_with_image_input_can/","created":"2023-03-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":95}}
{"title":"[D] Do we really need 100B+ parameters in a large language model?","description":"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \\~25x smaller than GPT-3, challenging the notion that is big always better?\n\nFrom my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?\n\nWould love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?\n\nP.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset","link":"https://www.reddit.com/r/MachineLearning/comments/121a8p4/d_do_we_really_need_100b_parameters_in_a_large/","created":"2023-03-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":17}}
{"title":"[P] DAD-3DHeads Annotation Process","description":"In this paper they discuss how they repurpose a modern 3D modeling tool and introduce a novel annotation scheme. They then go onto say \"the annotators \u201dpin\u201d the points on the 3D mesh surface\u2026 During the labeling process, labelers can see the texture rendered onto the 3D mesh with respect to their fitting to verify that the results are visually plausible\".\n\n  \n1) Which tool do they use for the annotation scheme\n\n2) How do they manipulate the pins onto the mesh.\n\n3) How do they render the texture onto the 3d mesh with respect to their fitting.\n\nI know the team released training data, but the license is restrictive so i wanted to build this tool out.  \n\n\nhttps://preview.redd.it/s04e8nn2wspa1.png?width=807&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2672a48e1c8b642517c70b85037074b4ade522ac\n\n[https://arxiv.org/abs/2204.03688](https://arxiv.org/abs/2204.03688)","link":"https://www.reddit.com/r/MachineLearning/comments/1218k6d/p_dad3dheads_annotation_process/","created":"2023-03-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[P] Reinforcement learning evolutionary hyperparameter optimization - 10x speed up","description":"Hey! We're creating an open-source training framework focused on evolutionary hyperparameter optimization for RL. This offers a speed up of 10x over other HPO methods!\n\nCheck it out and please get involved if you would be interested in working on this - any contributions are super valuable.\n\nWe believe this can change the way we train our models, and democratise access to RL for people and businesses who don't currently have the resources for it!\n\nGitHub: [https://github.com/AgileRL/AgileRL](https://github.com/AgileRL/AgileRL)","link":"https://www.reddit.com/r/MachineLearning/comments/120h120/p_reinforcement_learning_evolutionary/","created":"2023-03-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":24}}
{"title":"[N] Critical exploit in MLflow","description":"We found an LFI/RFI that leads to system takeover and cloud account takeover in MLflow versions &lt;2.2.2. The devs have had it patched for a few weeks now.\n\n* No user interaction required\n* Unauthenticated\n* Remotely exploitable\n* All configurations vulnerable including fresh install\n* No prerequisite knowledge of the environment required\n\nWe urge users of MLflow to patch immediately if they have not done so in the past month.\n\n[https://github.com/protectai/Snaike-MLflow](https://github.com/protectai/Snaike-MLflow)","link":"https://www.reddit.com/r/MachineLearning/comments/120iklh/n_critical_exploit_in_mlflow/","created":"2023-03-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":3}}
{"title":"[D] ChatGpt plugins: are tech innovators feeding a beast that may ultimately devour them?","description":"OpenAI has demonstrated that they may not prioritize ethical concerns. I'm genuinely curious about your opinion on this matter. Are tech companies trapped in a situation where they must engage in partnerships with OpenAI to stay competitive, while simultaneously generating an unprecedented amount of high-quality data? Could OpenAI then use this data to train their future models, rendering these very partnerships less relevant?","link":"https://www.reddit.com/r/MachineLearning/comments/121deu6/d_chatgpt_plugins_are_tech_innovators_feeding_a/","created":"2023-03-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":3}}
{"title":"[P] CUDA accelerated implementation of K-Planes and CoBaFa (recent NeRF techniques)","description":"[K-Planes](https://arxiv.org/abs/2301.10241) was released with PyTorch code only and [CoBaFa](https://arxiv.org/abs/2302.01226) didn't provide code, I implemented both of them in a short repo with CUDA acceleration : [https://github.com/loicmagne/tinynerf](https://github.com/loicmagne/tinynerf)","link":"https://www.reddit.com/r/MachineLearning/comments/120nisq/p_cuda_accelerated_implementation_of_kplanes_and/","created":"2023-03-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":6}}
{"title":"[R] Is there a diffusion-based model that inpaints with image prompt?","description":"The standard diffusion based models (e.g., Stable Diffusion with web UI) provides a tool by which I can inpaint a masked area with a text prompt.\n\nBut I'd like to inpaint the masked area by a prompt of another image (or maybe prompts with both image and text).\n\nIs there any paper for this?","link":"https://www.reddit.com/r/MachineLearning/comments/121f1jp/r_is_there_a_diffusionbased_model_that_inpaints/","created":"2023-03-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":1}}
{"title":"[P] ChatGPT with GPT-2: A minimum example of aligning language models with RLHF similar to ChatGPT","description":"hey folks, happy Friday! I wish to get some feedback for my recent project of a minimum example of using RLHF on language models to improve human alignment. \n\nThe goal is to compare with vanilla GPT-2 and supervised fine-tuned GPT-2 to see how much RLHF can benefit small models. Also I hope this project can show an example of the minimum requirements to build a RLHF training pipeline for LLMs.\n\nGithub: https://github.com/ethanyanjiali/minChatGPT\nDemo: https://colab.research.google.com/drive/1LR1sbWTyaNAmTZ1g1M2tpmU_pFw1lyEX?usp=sharing\n\nThanks a lot for any suggestions and feedback!","link":"https://www.reddit.com/r/MachineLearning/comments/120csub/p_chatgpt_with_gpt2_a_minimum_example_of_aligning/","created":"2023-03-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":14}}
{"title":"[D] ML code project to extract text and speaker from podcast video?","description":"Say I have a few podcast videos or interviews of a particular person. Is there existing off-the-shelf ML code to extract a transcript, and at least label the text as coming from \"person 1\", \"person 2\", etc? \n\nI'm not sure if this is trivial a task now or a state of the art challenge. \n\nAny resources appreciated, cheers","link":"https://www.reddit.com/r/MachineLearning/comments/1217ch1/d_ml_code_project_to_extract_text_and_speaker/","created":"2023-03-25","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":4}}
{"title":"[D] Is there a way to \"clone\" or change a voice to sound like another without using TTS models?","description":"I'm looking for ways to generate a couple of different voices that all say the same thing. What I've used before are some amazing models based on TTS but this time I'm not cloning an English voice. It's Swedish.\n\nSo I'm trying to find out if ther's another way to approach this. Right now it looks like I can't do it without training a new model based on tons of data I don't have. But now I don't need a tts clone that can say whatever I want. I just need it to say one single thing.\n\nIs there a way where I could \"morph\" one voice into another or something like that? Thinking that I'll just make one recording of me saying the things that should be said and then train that with recordings of the other person. I can't see how but I feel I need to ask.\n\nOr is this the universe telling me I should build a Swedish model for voice cloning?","link":"https://www.reddit.com/r/MachineLearning/comments/120ncun/d_is_there_a_way_to_clone_or_change_a_voice_to/","created":"2023-03-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":3}}
{"title":"[N] ChatGPT plugins","description":"[https://openai.com/blog/chatgpt-plugins](https://openai.com/blog/chatgpt-plugins)\n\n&gt;We\u2019ve implemented initial support for plugins in ChatGPT. Plugins are tools designed specifically for language models with safety as a core principle, and help ChatGPT access up-to-date information, run  computations, or use third-party services.","link":"https://www.reddit.com/r/MachineLearning/comments/11zsdwv/n_chatgpt_plugins/","created":"2023-03-23","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":146}}
{"title":"[D] \"Sparks of Artificial General Intelligence: Early experiments with GPT-4\" contained unredacted comments","description":"Microsoft's research paper exploring the capabilities, limitations and implications of an early version of GPT-4 was [found to contain unredacted comments by an anonymous twitter user.](https://twitter.com/DV2559106965076/status/1638769434763608064) ([threadreader](https://threadreaderapp.com/thread/1638769434763608064.html), [nitter](https://nitter.lacontrevoie.fr/DV2559106965076/status/1638769434763608064), [archive.is](https://archive.is/1icMv), [archive.org](https://web.archive.org/web/20230323192314/https://twitter.com/DV2559106965076/status/1638769434763608064)) \n\n- Commented section titled \"Toxic Content\": https://i.imgur.com/s8iNXr7.jpg\n- [`dv3` (the interval name for GPT-4)](https://pastebin.com/ZGMzgfqd)\n- [`varun`](https://pastebin.com/i9KMFcy5) \n- [commented lines](https://pastebin.com/Aa1uqbh1)\n\n[arxiv](https://arxiv.org/abs/2303.12712), [original /r/MachineLearning thread](https://www.reddit.com/r/MachineLearning/comments/11z3ymj/r_sparks_of_artificial_general_intelligence_early), [hacker news](https://twitter.com/DV2559106965076/status/1638769434763608064)","link":"https://www.reddit.com/r/MachineLearning/comments/1200lgr/d_sparks_of_artificial_general_intelligence_early/","created":"2023-03-23","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":64}}
{"title":"[D] hybrid discriminative/generative neural networks","description":"I\u2019ve been reading about generative deep learning and I was wondering if their are neural network architectures that can both classify an input to a given class and generate synthetic examples of those classes","link":"https://www.reddit.com/r/MachineLearning/comments/120ybhd/d_hybrid_discriminativegenerative_neural_networks/","created":"2023-03-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[P] DDPG using Transformers","description":"Hello everyone. I have been trying to integrate transformers with DDPG but to no avail. Any suggestions to solve this issue would be appreciated! Thank you.","link":"https://www.reddit.com/r/MachineLearning/comments/120trcm/p_ddpg_using_transformers/","created":"2023-03-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[P] Playing Pok\u00e9mon battles with ChatGPT","description":"A paper you all have been waiting for \ud83e\udd29 \"[PokemonChat: Auditing ChatGPT for Pokemon Universe Knowledge](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4396798)\"!! \n\nA proof that you can write a paper while having lots of fun (and come up with interesting conclusions too)! \n\nAlright by the time the paper was written, the ChatGPT API didn't even exist. Far less we knew about GPT-4... Anyway, In this work, we rely on the Pok\u00e9mon universe to evaluate the ChatGPT's capabilities. The Pok\u00e9mon universe serves as an ideal testing ground, since its battle system is a well-defined environment (match-ups, weather / status conditions) and follows a closed world assumption. \n\nTo audit ChatGPT, we introduce a staged conversational framework (protocol): (a) Audit Knowledge, (b) Use of knowledge in context, and (c) Introduction of new knowledge, in 3 settings of human-in-the-loop interaction: neutral \ud83e\udd14, cooperative \ud83e\udd17, and adversarial \ud83d\ude08.\n\nWe present a series of well-defined battles starting from simpler to more complex scenarios involving level imbalance, weather and/or status conditions. ChatGPT can make accurate predictions in most cases and explain step-by-step its reasoning.\n\nThe most impressive part is that we are able to introduce new knowledge (made-up Pok\u00e9mon species), in which case the model is able to perform compositional generalization combining prior and new knowledge to predict the battle outcomes.\n\nThanks for reading it and again, don't miss out the paper if you want to know more about it! Available at [SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4396798https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4396798)","link":"https://www.reddit.com/r/MachineLearning/comments/120spol/p_playing_pok\u00e9mon_battles_with_chatgpt/","created":"2023-03-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":3}}
{"title":"[D] There should not be \"handoff\" of the model between the Data Science team and the Platform team","description":"I spoke to ex-ML Platform Lead at Stitch Fix to understand the practical challenges in building and managing the ML platform and if someone has to start what is the ideal starting point.  \nWhat do you think?\n\nlink: [https://www.youtube.com/watch?v=TbP5G188kX8](https://www.youtube.com/watch?v=TbP5G188kX8)","link":"https://www.reddit.com/r/MachineLearning/comments/120rvm5/d_there_should_not_be_handoff_of_the_model/","created":"2023-03-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":2}}
{"title":"[D] Salary for Machine Learning Researcher with PhD?","description":"I've seen salaries ranging from 60k to 500k and I just don't know what to believe anymore...","link":"https://www.reddit.com/r/MachineLearning/comments/120rfxd/d_salary_for_machine_learning_researcher_with_phd/","created":"2023-03-24","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":39}}
