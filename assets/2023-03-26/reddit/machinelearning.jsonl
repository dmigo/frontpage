{"title":"Reminder: Use the report button and read the rules!","description":"","link":"https://www.reddit.com/r/MachineLearning/comments/120f4oy/reminder_use_the_report_button_and_read_the_rules/","created":"2023-03-24","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[D] Transition from classical computer vision engineer to machine learning engineer","description":"I am a junior computer vision engineer (\\~4 years industry experience) working in the embedded systems space. In my role, I am tasked with researching and implementing highly optimised computer vision algorithms from first principles in C++ that can run in real time on embedded hardware. This includes a range of applications (video stabilisation, rolling shutter correction, multi-target indication, wide-angle image stitching etc) for which I implement the low-level algorithms from first principles (feature detection / matching (SIFT, FAST, ORB, BRIEF etc), optical flow, scene reconstruction, image segmentation etc).\n\nWhile I have some industry experience applying some statistical machine learning (ID3, SVM, RANSAC, nearest neighbour searches etc) I have not had the opportunity to pursue deep learning / neural network applications.\n\nI am worried I am pigeonholing myself, limiting future job prospects but am unsure how best to proceed. \n\nIf anyone could speak from experience or provide recommendations, that would be much appreciated:\n\n1. How can someone coming from a statistical machine learning background land a job in machine learning? (i.e Is my experience be enough to get into a junior position or should I be trying to build up more of a portfolio?)\n2. Is my experience still applicable in the machine learning field?","link":"https://www.reddit.com/r/MachineLearning/comments/122etks/d_transition_from_classical_computer_vision/","created":"2023-03-26","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"Is it possible to merge transformers? [D]","description":"In the last few days I had a new thought. I don't know if it is possible or already done somewhere? Is it possible to merge the weights of two transformer models like they do with merging stable diffusion models?\nLike can I merge for example BioBert and LegalBert and get a model that can do both?","link":"https://www.reddit.com/r/MachineLearning/comments/122fj05/is_it_possible_to_merge_transformers_d/","created":"2023-03-26","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[D] Title: Best tools and frameworks for working with million-billion image datasets?","description":"Hi everyone,\n\nI'm working on a project that involves working with image datasets that have tens of thousands to millions of images.I'm looking for some advice and recommendations on the best tools and frameworks to use for this task. Here are some of the questions I have:\n\n\\- What are the best tools for storing and accessing such large image datasets? I've used NetCDFs and Zarrs in the past, but most image-processing libraries like sci-kit-image or opencv don't support it. Do you guys just store all your images in a massive data lake?\n\n\\- I'm familiar with TensorFlow, but I'm sick of its issues it's got a ton of lacking functionality that seems broken or abandoned, such as gradient checkpointing, and its lack of transparency with underlying functionality. I know Pytorch exists, but I feel like there's a higher learning curve to it. Is there a Keras equivalent to Pytorch?\n\n\\- Is there any way to accelerate the image processing tasks using a GPU? I know GPUs are mainly used for training models, but I'm wondering if there is any benefit or possibility of using them for image processing as well. If so, how can I do that?\n\n\\- Is there any way to meaningfully store the image dataset as some form of a database with all of its features in one place? I'm interested in having a structured and searchable way to access the images and their metadata, such as labels, captions, annotations, etc.\n\nI wanna mention that I've spent a LOT of time reading up on these things and haven't been able to find a suitable answer, so I'm posting this here as a final resort","link":"https://www.reddit.com/r/MachineLearning/comments/12285x7/d_title_best_tools_and_frameworks_for_working/","created":"2023-03-26","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":3}}
{"title":"[D] Attention/transformer encoder for small tokens","description":"Hey guys,\n\nI've been trying to speed my transformer model with each batch of roughly 20 tokens and a few hundred for the embedded dimension. I barely see any difference between the baseline attention vs the flash attention used in pytorch 2.0, and that is expected since my tokens are quite small.\n\nWould really appreciate if you could point me towards any paper/repos for small tokens, or any ways I can increase speed from an architecture standpoint. Memory is not a concern for me, just speed!\n\nThanks in advance ;)","link":"https://www.reddit.com/r/MachineLearning/comments/1225bef/d_attentiontransformer_encoder_for_small_tokens/","created":"2023-03-26","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[D] Can the Databricks Dolly model be downloaded from somewhere?","description":"I tried to setup the databricks workspace with aws but ran into issues.\n\nSurely someone has it uploaded somewhere?","link":"https://www.reddit.com/r/MachineLearning/comments/121ueww/d_can_the_databricks_dolly_model_be_downloaded/","created":"2023-03-25","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[R] Reflexion: an autonomous agent with dynamic memory and self-reflection - Noah Shinn et al 2023 Northeastern University Boston - Outperforms GPT-4 on HumanEval accuracy (0.67 --&gt; 0.88)!","description":"Paper: [https://arxiv.org/abs/2303.11366](https://arxiv.org/abs/2303.11366) \n\nBlog: [https://nanothoughts.substack.com/p/reflecting-on-reflexion](https://nanothoughts.substack.com/p/reflecting-on-reflexion) \n\nGithub: [https://github.com/noahshinn024/reflexion-human-eval](https://github.com/noahshinn024/reflexion-human-eval) \n\nTwitter: [https://twitter.com/johnjnay/status/1639362071807549446?s=20](https://twitter.com/johnjnay/status/1639362071807549446?s=20) \n\nAbstract:\n\n&gt;Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, **specifically the ability to learn from mistakes**. **Self-reflection allows humans to efficiently solve novel problems through a process of trial and error.** Building on recent research, we propose Reflexion, an approach that endows an agent with **dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities.** To achieve full automation, we introduce a straightforward yet effective heuristic that **enables the agent to pinpoint hallucination instances, avoid repetition in action sequences, and, in some environments, construct an internal memory map of the given environment.** To assess our approach, we evaluate the agent's ability to complete decision-making tasks in AlfWorld environments and knowledge-intensive, search-based question-and-answer tasks in HotPotQA environments. We observe success rates of 97% and 51%, respectively, and provide a discussion on the emergent property of self-reflection. \n\nhttps://preview.redd.it/4myf8xso9spa1.png?width=1600&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=867a16e1114108053d08d4cdf41485c8b29a132c\n\nhttps://preview.redd.it/bzupwyso9spa1.png?width=1600&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=95cacfe6b99756e7eed9ec8c40784f8c4cb94cee\n\nhttps://preview.redd.it/009352to9spa1.jpg?width=1185&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=5ccc52597d6e001c2ba754fc5f05afd1df09cd63\n\nhttps://preview.redd.it/ef9ykzso9spa1.jpg?width=1074&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=2701778aa5a9f3e80f683a1e3d0eaf0160928f54","link":"https://www.reddit.com/r/MachineLearning/comments/1215dbl/r_reflexion_an_autonomous_agent_with_dynamic/","created":"2023-03-25","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":75}}
{"title":"[P] Can I do better than this? [Image near-duplicate and similarities clustering]","description":"I'm developing an algorithm to find near-duplicates images, I tried various solutions, such as pHash, CNNs and others. In the end I found using \\`sentences-transformers\\` library with \\`CLIP algorithm and clustering them based on similarity matrix using \\`connected components\\` by scikit. It performs very well, it can recognize similar and not similar images in the same environment, like in a disco club it can divide in two separate clusters two images that have the same light type but different subjects.\n\nBy contrast, on some images, like this two (the beautiful Dome of Florence) it recognizes that the building is the same and it classifies them as similar images, but, despite the fact that the subject is the same, the angle of the photos and the photos themselves are very different.\n\n**This is the example:**\n\n&amp;#x200B;\n\nhttps://preview.redd.it/kmv4jq2qkxpa1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=1d500b1318fcd6a1b3b5357bb1a00962a5df968b\n\n&amp;#x200B;\n\nhttps://preview.redd.it/thuqvsuqkxpa1.jpg?width=3024&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=892e87a376ea41a62054d3ba042eda95fb47144e\n\nI'm processing and clustering the images this way:\n\n    encoded_images = model.encode(images, batch_size=128, convert_to_tensor=True)\n    processed_images = util.paraphrase_mining_embeddings(encoded_images)\n    near_duplicates = [image for image in processed_images if image[0] &gt; TRESHOLD] \n\nThen passing the result into \\`connected components\\` to cluster them.\n\nDo you know some other algorithm that can find similar-images better than this one?","link":"https://www.reddit.com/r/MachineLearning/comments/121v5st/p_can_i_do_better_than_this_image_nearduplicate/","created":"2023-03-25","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":2}}
{"title":"[R] Hello Dolly: Democratizing the magic of ChatGPT with open models","description":"Databricks shows that anyone can take a dated off-the-shelf open source large language model (LLM) and give it magical ChatGPT-like instruction following ability by training it in less than three hours on one machine, using high-quality training data.\n\nThey fine tuned GPT-J using the Alpaca dataset.\n\nBlog: [https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)  \nGithub: [https://github.com/databrickslabs/dolly](https://github.com/databrickslabs/dolly)","link":"https://www.reddit.com/r/MachineLearning/comments/120usfk/r_hello_dolly_democratizing_the_magic_of_chatgpt/","created":"2023-03-24","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":98}}
{"title":"[D] Keeping track of ML advancements","description":"General ML question, how do you guys keep track of all the advancements made in AI and the flood of papers coming out?\n\nI'm pretty new to AI, and although I've been following the developments since 2016, I only started taking it seriously and doing development last year. I just started my master's in ML and want to keep up with the developments made in the field. But it feels like a new paper, blog post, or conference gets released with astonishing improvements every second day. With 20 hours of work a week and my studies, I don't seem to catch up with everything going on. So I'm wondering how others are dealing with it.\n\nQuestions:\n\n* Do you read all of the papers/blog posts that get released?\n* The ones you read, do you read them in detail or just skim over them or look for a TLDR?\n* Do you filter only the papers in the topics you're interested in?\n* Is there any website with a clear overview and development of models? I know about paperswithcode\\[.\\]com, but I'm looking more for a website with a chronological timeline of the models released and their previous versions and related developments, etc...\n* Is it important that I stay up-to-date with everything going on in the field ?\n\nMany thanks to anyone who responds !!","link":"https://www.reddit.com/r/MachineLearning/comments/121mvp5/d_keeping_track_of_ml_advancements/","created":"2023-03-25","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":8}}
{"title":"[D] Do we really need 100B+ parameters in a large language model?","description":"DataBricks's open-source LLM, [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) performs reasonably well on many instruction-based tasks while being \\~25x smaller than GPT-3, challenging the notion that is big always better?\n\nFrom my personal experience, the quality of the model depends a lot on the fine-tuning data as opposed to just the sheer size. If you choose your retraining data correctly, you can fine-tune your smaller model to perform better than the state-of-the-art GPT-X. The future of LLMs might look more open-source than imagined 3 months back?\n\nWould love to hear everyone's opinions on how they see the future of LLMs evolving? Will it be few players (OpenAI) cracking the AGI and conquering the whole world or a lot of smaller open-source models which ML engineers fine-tune for their use-cases?\n\nP.S. I am kinda betting on the latter and building [UpTrain](https://github.com/uptrain-ai/uptrain), an open-source project which helps you collect that high quality fine-tuning dataset","link":"https://www.reddit.com/r/MachineLearning/comments/121a8p4/d_do_we_really_need_100b_parameters_in_a_large/","created":"2023-03-25","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":73}}
{"title":"[D] Is it possible to run large language models using NVIDIA Jetson products?","description":"Although I've had trouble finding exact VRAM requirement profiles for various LLMs, it looks like models around the size of LLaMA 7B and GPT-J 6B require something in the neighborhood of 32 to 64 GB of VRAM to run or fine tune. GPU models with this kind of VRAM get prohibitively expensive if you're wanting to experiment with these models locally.\n\nWhen looking for alternatives, I came across the [NVIDIA Jetson](https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/) line of products. Specifically, the Jetson AGX Orin comes in a 64 GB configuration. It looks like these devices share their memory between CPU and GPU, but that should be fine for single model / single purpose use, e.g. running the device headless using GPT-J as a chat bot.\n\nThe problem is that I've not be able to find much information on running LLMs on these devices. The only concrete thing I was able to find was someone [running GPT2 117M on a Jetson Nano](https://youtu.be/IWjPlcpQWNU). Would the AGX Orin's 64 GB of memory scale and allow us to run GPT-J or Dolly or Alpaca, or is there something I'm missing here? I'm aware that the number of CUDA cores on the Jetson devices is smaller than something like an A6000, but the price differential is huge and if the memory holds the model I think the trade off in inference or training speed would be worth it.\n\nI feel like there's a major \"gotcha\" here, otherwise everyone would be running Dolly or Alpaca locally by now. Has anyone here tried running a \"large\" LLM on one of these devices? If so, what was the experience like?","link":"https://www.reddit.com/r/MachineLearning/comments/12220vj/d_is_it_possible_to_run_large_language_models/","created":"2023-03-25","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":3}}
{"title":"[D] Do you use a website or program to organise and annotate your papers?","description":"I'm aware of Mendeley, Zotero, EndNote etc. but I was wondering if people here use more modern stuff with AI plugins and fancy stuff like that.","link":"https://www.reddit.com/r/MachineLearning/comments/121k5og/d_do_you_use_a_website_or_program_to_organise_and/","created":"2023-03-25","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":9}}
