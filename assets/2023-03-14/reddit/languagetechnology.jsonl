{"title":"Finno-Ugric open-source machine translation","description":"We here at the University of Tartu created an NMT engine for 23 Finno-Ugric languages, targeting low-resource languages: Livonian, Komi, Udmurt, V\u00f5ro and several others. Most of the covered low-res languages are not part of Meta's M2M100 or NLLB, nor are they part of Google Translate, Bing Translator or DeepL yet.\n\nFairSeq translation model and full list of supported languages here: [https://huggingface.co/tartuNLP/smugri3-finno-ugric-nmt](https://huggingface.co/tartuNLP/smugri3-finno-ugric-nmt). Online demo here: [https://translate.ut.ee/](https://translate.ut.ee/), submitting corrected translations is also supported, in case you speak any of these languages - we are hoping to use the feedback to improve translation quality in the near future.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11r0izu/finnougric_opensource_machine_translation/","created":"2023-03-14","tags":["languagetechnology","ml","reddit"],"meta":{"num_comments":0}}
{"title":"Is GPT-3(and ChatGPT) trained with the MLM task?","description":"Hi all experts, I have a quick question.\n\n\\- Is the GPT family(GPT1/2/3/Chat) trained with the MLM(Masked Language Modeling) task?\n\nI guess no, because the GPT is basically auto-regressive(unidirectional), and their papers didn't mention the MLM training task, afaik. But when I googled, there is no clear answer, and the ChatGPT answers that the GPT family was trained on MLM.\n\nDoes anyone know the precise answer?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11qvs3t/is_gpt3and_chatgpt_trained_with_the_mlm_task/","created":"2023-03-14","tags":["languagetechnology","ml","reddit"],"meta":{"num_comments":5}}
{"title":"Evaluation Methods for text segment matching","description":"Hi together,\n\nright now I'm working on my masters thesis with the goal of exploring the usage of Language Models for matching Information Security controls. I am having a few questions about the evaluation methods which I have used so far and some which I might have missed. \n\nA little background: \nI have created a data set based on existing mappings between the ISO27001 security framework and another IT security framework. \n\nThe data set is created in the following way:\nI have two sentences/paragraphs per training example, one ISO sentence and one paragraph (might be one sentence up to a full subchapter) from the other framework, and per each pair of sentences a similarity score which indicates their semantic overlap / if they \"fit together\" (derived from an official existing mapping which maps chapters of sentences from both frameworks to each other).\n\nThe task for the models is as follows: I want the models to create embeddings of the sentence pairs and learn to put those embeddings which \"fit together\", as indicated by the ground truth similarity score, close to each other, while pulling those sentence pairs which do not belong together farther away in the embedding space. Later on, I want to let the model encode previously unseen sentences (e.g. new ISO controls) and then use semantic search based on a distance metric, at first cosine similarity (or possibly other methods) to find the most similar sentences from another IT security framework, as to match them together.\n\nFor this task I am using a SentenceBERT variant as a strong baseline.\n\nIn terms of model evaluation I use a held out test set from a 80/20 train test split. On trained models, I have used two evaluation methods so far:\n\n1. Let the model encode sentence pairs from test set (where a ground truth cosine similarity score is known) and then calculate the Cosine similarity. Calculate Cosine similarity loss on test set.\n\n2. For each distinct sentence / ISO control in the test set, use this sentence as query for the trained model and let the model output the top-k most similar sentences from the second security framework. Compare the calculated top-k matches with the actual matches and calculate precision at k and recall at k.\n \nNow coming to the questions:\n\n1. Do you think that the evaluation methods I have used so far are appropriate for evaluating the models' performances on the task described above?\n\n2. Can you think of any other evaluation methods I might have missed? \n\n3. Do you possibly know of similar research, and if so, could you point me in this direction?\n\nI would appreciate any answers or feedback, feel free to point out any flaws if you do not mind.\nOh and also please excuse the formatting, I am typing this on my phone. \n\nThank you!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11r1jch/evaluation_methods_for_text_segment_matching/","created":"2023-03-14","tags":["languagetechnology","ml","reddit"],"meta":{"num_comments":0}}
{"title":"Optimum Dataset for Sequence Labelling","description":"Hi everyone. I'm working on a custom NER like task.\n\nHowever some tags have very low frequency in the data. Hence mode is not good in predicting them.\n\nThis problem got me thinking what would the optimum dataset look like for such a task? \nShould number of samples for each label be similar? \nif we increase the data for all labels but keep a specific label same would it impact the preformance?\n\nPapers usually just take a toy dataset and tweak the model or data little bit. I couldn't find an answer to these questions. Do you have any resource or knowledge on this?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11qf9sv/optimum_dataset_for_sequence_labelling/","created":"2023-03-13","tags":["languagetechnology","ml","reddit"],"meta":{"num_comments":2}}
{"title":"Recommendations for a newbie","description":"I've been reading a lot of articles about AI in general, machine learning and NLP etc but I want to learn more about NLP, creating desktop and mobile apps for questions-answering and summarizing texts. \n\nI've done programming in javascript and C# in the past and I wonder if that is enough or if I must learn python as well. \n\nWhat are your recommendations regarding language, tools, APIs, models, transformers etc and why should I start with these?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11pdpc3/recommendations_for_a_newbie/","created":"2023-03-12","tags":["languagetechnology","ml","reddit"],"meta":{"num_comments":11}}
{"title":"Best approach for sarcasm subcategory classification?","description":" Hi All,\n\nI  am currently working on my thesis which is attempting to build on  existing research in the field of sarcasm detection within NLP and  sentiment analysis. The task outlined is to basically build a model  which can identify subcategories of sarcasm (e.g. irony, overstatement,  rhetorical questions etc.). The dataset includes values for whether a  phrase is sarcastic or not, and which subcategories the phrase fits into  (there can be overlap between categories). I have fine-tuned BERT for  sarcasm detection and this works fine but that isn't really the task. My  questions are twofold essentially:\n\n\\-  Is the solely transformer-based approach useful in this instance, given  that it could build on existing research where previous scores obtained  in this task were fairly low. If so, does anyone have any resources on  how to build a multi-subclass classification model, or would it be  better to build separate models for each task?\n\n\\-  Would attempting to use a rule-based approach be more valuable e.g.  using vector semantics and embeddings to attempt to identify the  subcategories using this approach, and if so are there any particular  resources I should have a look at to understand how to implement this in  code? I am currently reading Jurafsky &amp; Martin's 3rd draft of  Speech and Language Processing, but I am unclear on how I could use this  to categorise the subcategories which are difficult to define by  linguistic experts as it is?\n\nI'm  sorry if this is a bit rambling and all over the place, I'm feeling  pretty lost and stressed, but happy to answer any questions and try to  clarify anything :)","link":"https://www.reddit.com/r/LanguageTechnology/comments/11ok7ug/best_approach_for_sarcasm_subcategory/","created":"2023-03-11","tags":["languagetechnology","ml","reddit"],"meta":{"num_comments":9}}
