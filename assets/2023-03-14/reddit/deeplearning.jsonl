{"title":"Question on study options","description":"I started my career as a quant then programmer, then data scientist and now work for Bloomberg.\n\nI've been using ML for years but have not really worked with NLP and with the recent advances in LLMs the penny dropped that our working world is about to start changing very quickly.\n\nAre there any AI MSc degrees that are aligned to this space that are open to part time study?\n\nOr, should I just dive into the books as the MSc would not be specific enough.\n\nI did an MSc in quant mathematics a few years ago after a break of 20 years from my Physics BSc and found it pretty broad and tbh not all that useful.\n\nAnyway. Just seeing what people's thoughts are \n\nCheers","link":"https://www.reddit.com/r/deeplearning/comments/11r0l52/question_on_study_options/","created":"2023-03-14","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":1}}
{"title":"Calculating the gradient of the marginal log-likelihood function","description":"In the article [The theory behind Latent Variable Models: formulating a Variational Autoencoder](https://theaisummer.com/latent-variable-models/#variational-autoencoders)  , to model the desired probability distribution, estimating the parameters of a probability distribution so that the distribution fits the observed data is presented as an optimization problem of: \n\n&amp;#x200B;\n\nhttps://preview.redd.it/sgfz5txkjnna1.png?width=374&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=73b1ebc471187004419e8f7e1402b1d030a43e00\n\nThe gradient of the marginal log-likelihood function is then calculated using simple calculus and the Bayes rule:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/kwgde2twjnna1.png?width=427&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9f5071bcc63ed8e8c2c31c23a46ee3e51075a9bf\n\nwhere can one find the proof/maths behind this gradient calculation?","link":"https://www.reddit.com/r/deeplearning/comments/11qz5ze/calculating_the_gradient_of_the_marginal/","created":"2023-03-14","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":0}}
{"title":"How To Scale Transformers\u2019 Memory up to 262K Tokens With a Minor Change?","description":"I just published my latest medium article. \n\nThis article is a fabulous attempt to leverage language models in memorizing information by transformers with the least required effort by inserting only an external memory near the last layer of the transformer.\n\nWe can use this to retrieve information that the transformer is trained with which helps the reliability of predictions.\n\nFeel free to share this and/or contact me directly.\n\nhttps://medium.com/towards-artificial-intelligence/extending-transformers-by-memorizing-up-to-262k-tokens-f9e066108777","link":"https://www.reddit.com/r/deeplearning/comments/11qfl2o/how_to_scale_transformers_memory_up_to_262k/","created":"2023-03-13","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":3}}
{"title":"Learning logical relationships with neural networks with differential ILP","description":"Since last week\u2019s post on my lab\u2019s software package [PyReason](https://neurosymoblic.asu.edu/pyreason/), I got a lot of questions on if it would be possible for a neural network to learn logical relationships from data. After all, ChatGPT seems to be able to generate Python code, and Meta released a NeurIPS paper to show you can learn math equations from data using a transformer-based model, so why not logic? In this article, we will one line of research in this area \u2013 differential ILP.\n\nThe research in this area really kicked off with a 2018 [paper from DeepMind](https://www.reddit.com/r/deeplearning/jair.org/index.php/jair/article/view/11172) where Richard Evans and Edward Grefenstette showed that you could adapt techniques from \u201cinductive logic programming\u201d to use gradient descent, and learn logical rules from data. Previous (non-neural) work on inductive logic programming was generally not designed to work with noisy data and instead fit the historical examples in a precise manner. Evans and Grefenstette utilized a neural architecture and a loss function \u2013 and they showed they could handle noisy data and even do some level of integration with CNN\u2019s. Their neural architecture mimicked a set of candidate logical rules \u2013 and the rules assigned higher weights by gradient descent would be thought to best fit the data. However, a downside to this approach is that the neural network was [quintic in the size of the input](https://www.youtube.com/watch?v=SOnAE0EyX8c&amp;list=PLpqh-PUKX-i7URwnkTqpAkSchJHvbxZHB&amp;index=5). This is why they only applied their approach on very small problems \u2013 it did not see very wide adoption.\n\nThat said, in the last two years, there have been some notable follow-ons to this work. Researchers out of Kyoto University and NTT introduced a manner to learn rules that are more expressive in a different manner by allowing function symbols in the logical language ([Shindo et al., AAAI 2021](https://ojs.aaai.org/index.php/AAAI/article/view/16637/16444)). They leverage a clause search and refinement process to limit the number of candidate rules \u2013 hence limiting the size of the neural network. A student team from ASU created a presentation on their work for our recent seminar course on neuro symbolic AI. We released a three part video series from their talk:\n\n[Part 1: Review of differentiable inductive logic programming](https://www.youtube.com/watch?v=JIS78a40q8U&amp;t=270s)\n\n[Part 2: Clause search and refinement In our recent video series](https://www.youtube.com/watch?v=nzfbxlHUwuE&amp;t=345s)\n\n[Part 3: Experiments](https://www.youtube.com/watch?v=-fKWNtHUIN0&amp;t=27s)\n\n[Slides](https://labs.engineering.asu.edu/labv2/wp-content/uploads/sites/82/2022/10/Shindo_dILP.pdf)\n\nSome think that the ability to learn such relationships will represent a significant advancement in ML, specifically addressing shortcomings in areas such as knowledge graph completion and reasoning about scene graphs. However, the gap still remains wide, and ILP techniques, including differentiable ILP still have a ways to go. Really interested in what your thoughts are, feel free to comment below.","link":"https://www.reddit.com/r/deeplearning/comments/11q8tir/learning_logical_relationships_with_neural/","created":"2023-03-13","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":3}}
{"title":"Using GANs to generate defective data","description":"Hey guys,\n\nI'm working on implementing a model to detect defects on the  labels of bottles.\n\nThe model should be able to spot bubbles, folds, and  miss-labels.\n\nBut I'm short on actual defective data, so I'm thinking  about making some artificial data using GANs.\n\nI gave it a shot with  simple image processing, but the model couldn't generalize well.\n\nGot any ideas  or suggestions on how I could make this work?\n\nWould really appreciate  some help.\n\n&amp;#x200B;\n\n[Bubble example](https://preview.redd.it/d9xnpmm1kina1.jpg?width=500&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8de5ec7bcfecfc765bef680f756bf67d629e26c8)","link":"https://www.reddit.com/r/deeplearning/comments/11qanvr/using_gans_to_generate_defective_data/","created":"2023-03-13","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":8}}
{"title":"Multiple objects - Multivariate LSTM","description":"Hello there, I'm new to deeplearning but I'm very fascinated by it.\n\nI'm actually working with multivariate time series (TS) forecasting. I already saw many approaches but none of them appear to be similar to my problem:\n\nI have ~150k objects, for each objects I have 4 independent TS (A B C D) and 1 dependent TS (let's call it Z).\n\nEach TS is about 45-50 observations depending on the studied year.\nI need to train a NN on the TS from the previous years to be used on the TS for this year and the next ones.\n\nI have 2 objectives: \n-predict the next 2-4 observations of TS Z using no more than the last 6 observations of ABCD;\n-predict all TS Z based on all the previous observations from ABCD. Obviously the predictions in the nearest future will be more precise that the ones in the long future. So with gradually increasing obs after each time step.\n\nCan you suggest me the best structure to achieve those 2 objs?","link":"https://www.reddit.com/r/deeplearning/comments/11q9hj3/multiple_objects_multivariate_lstm/","created":"2023-03-13","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Which topic in deep learning do you think will become relevant or popular in the future?","description":"I recently saw Continual Learning (CL) growing, with several papers published recently that have considerable potential to impact real-world applications. Which topic (such as CV, RL, NLP, CL..) will be very relevant to research or be focused on a lot? And which topic do you think still needs a breakthrough and will have a significant impact in real-world applications, such as in the case of these LLM models in recent times? Feel free to mention your current topic of work and why you chose to do it \ud83d\ude0a","link":"https://www.reddit.com/r/deeplearning/comments/11pyvb3/which_topic_in_deep_learning_do_you_think_will/","created":"2023-03-13","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":13}}
{"title":"Display model like tensorspace","description":"Hi guys, quick question.\n\nDo you know any JavaScript module that could be used to display your model layers like in tensorspace playground? \n\nI was trying to use their angular example but I think it\u2019s outdated and doesn\u2019t have the best docs. Thanks!","link":"https://www.reddit.com/r/deeplearning/comments/11qdorq/display_model_like_tensorspace/","created":"2023-03-13","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Image reconstruction","description":"I have a use-case where (say) N RGB input images are used to reconstruct a single RGB output image, using either an Autoencoder, or a U-Net architecture. More concretely, if N = 18, 18 RGB input images are used as input to a CNN which should then predict one target RGB output image.\n\nIf the spatial width and height are 90, then *one input sample* might be (18, 3, 90, 90) **which is not batch-size = 18!** AFAIK, (18, 3, 90, 90) as input to a CNN will reproduce (18, 3, 90, 90) as output, whereas, I want (3, 90, 90) as the desired output.\n\nAny idea how to achieve this?","link":"https://www.reddit.com/r/deeplearning/comments/11qazip/image_reconstruction/","created":"2023-03-13","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":3}}
{"title":"Recommendations sources for Understanding Advanced Mathematical Concepts in Research Papers?","description":"Hey everyone,\n\nI'm struggling with understanding mathematical proofs in research papers. I have a good grasp of basic concepts such as calculus (single variable calculus and basic knowledge of multi-variable calculus), linear algebra, and basic probability.\n\nI was wondering if any of you could recommend some sources (preferably videos or lecture series) to help me become more familiar with advanced mathematical concepts found in research papers.\n\nFor example:([source](https://www.biorxiv.org/content/10.1101/2021.03.21.436284v1.full.pdf))\n\nhttps://preview.redd.it/m19pwqkwkdna1.png?width=1104&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5cb83feec7e92d4e7f991f7c22cda8483c39c377\n\nIn papers, I have frequently encountered concepts like, **KL divergence**, **mathematics in higher-dimensional space**, **hessian**, **topology, Random projections** and many more;What are the subject/module names I need to study  to confidently read and understand proofs in papers?\n\n&amp;#x200B;\n\nThanks in advance!","link":"https://www.reddit.com/r/deeplearning/comments/11pq968/recommendations_sources_for_understanding/","created":"2023-03-12","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":9}}
{"title":"YOLO equation","description":"Hi,Can any one explain this equation?Pr(Classi|Object) \u2217 Pr(Object) \u2217 IOU = Pr(Classi) \u2217 IOUIt is from the You Look Only Once article, but it seems mathematically wrong. Shouldnt be like this  Pr(Classi|Object) \u2217 Pr(Object) \u2217 IOU = Pr(Classi\\^Object) \u2217 IOU ?","link":"https://www.reddit.com/r/deeplearning/comments/11q0c2t/yolo_equation/","created":"2023-03-13","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":3}}
{"title":"Does anyone here have a job in industry using deep learning for genomics/bioinformatic work?","description":"If so, how common would you describe these jobs to be? Asking as a grad student who might spend a considerable amount of time doing deep learning projects and who hopes to get a job in industry. I have asked similar questions on the bioinfornatic sub.","link":"https://www.reddit.com/r/deeplearning/comments/11pr44f/does_anyone_here_have_a_job_in_industry_using/","created":"2023-03-12","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Factors Influencing Adoption Intention of ChatGPT","description":"Hello,\n\nI am an information systems student currently conducting research for my undergraduate thesis on the factors that influence people's adoption intention of ChatGPT, as well as identifying the factors that may be holding them back. These factors include people's concerns about potential negative impacts of ChatGPT, such as increased unemployment and the spread of misinformation. Your participation in this study is crucial as it will provide valuable insights to help us understand how ChatGPT can be improved to meet users' needs.\n\nPlease note that I am not affiliated with OpenAI, no identifying information will be collected during the survey, and all responses will be kept confidential. The survey should take approximately 10 to 15 minutes to complete, and participation is voluntary. You may withdraw from the survey at any time, and there are no known risks associated with participating.\n\nIf you are interested in learning more about the study, please follow the link below. https://forms.gle/EwZCtpYDvk9R9M386\n\nThank you for taking the time to contribute to our research study. Your participation is greatly appreciated!","link":"https://www.reddit.com/r/deeplearning/comments/11p7x0f/factors_influencing_adoption_intention_of_chatgpt/","created":"2023-03-12","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":4}}
{"title":"Text2Image using ControlNet and Stable Diffusion","description":"In this tutorial, we will show you how to create beautiful and high-quality images from text using the powerful combination of diffusion model and ControlNet. \n\nText2Image generation is a fascinating field of AI that enables machines to understand and visualize human language in a more creative way.\n\n we will walk you through the step-by-step process of how to use the diffusion model and ControlNet to generate images from text. By the end of this tutorial, you will have a thorough understanding of text2image generation and how to use diffusion model and ControlNet to create stunning images from text. You will also have the knowledge and skills to apply these techniques to your own projects and experiments.\n\n So, get ready to dive into the exciting world of text2image generation and start creating your own beautiful images from text today!\n\nhttps://youtu.be/0D5Nlo2REb0","link":"https://www.reddit.com/r/deeplearning/comments/11p1par/text2image_using_controlnet_and_stable_diffusion/","created":"2023-03-12","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":0}}
{"title":"How to blackout certain landmarks from face?","description":"I am working on a project to classify faces and I want to blackout certain areas especially eyes, nose, mouth so that my model can generalize well. But I am confused how to do that. Can someone please guide me?","link":"https://www.reddit.com/r/deeplearning/comments/11opl7o/how_to_blackout_certain_landmarks_from_face/","created":"2023-03-11","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":1}}
{"title":"https://www.kaggle.com/code/sadikaljarif/plant-disease-classification-using-mobilenetv2","description":"# About Dataset\n\n# This dataset is recreated using offline augmentation from the original dataset. The original dataset can be found on [this](https://github.com/spMohanty/PlantVillage-Dataset) github repo. This dataset consists of about 87K rgb images of healthy and diseased crop leaves which is categorized into 38 different classes. The total dataset is divided into 80/20 ratio of training and validation set preserving the directory structure. A new directory containing 33 test images is created later for prediction purpose\n\n# Notebook : [https://www.kaggle.com/code/sadikaljarif/plant-disease-classification-using-mobilenetv2](https://www.kaggle.com/code/sadikaljarif/plant-disease-classification-using-mobilenetv2)","link":"https://www.reddit.com/r/deeplearning/comments/11otmgd/httpswwwkagglecomcodesadikaljarifplantdiseaseclass/","created":"2023-03-11","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":0}}
{"title":"(Python) Library for deep video generation?","description":"I'm searching for a library that allows the creation of artificial interview like video sequences for an input text\n\nAre there any libraries like this or would the best approach be to choose an existing video and use deep fake libraries with artificial faces?","link":"https://www.reddit.com/r/deeplearning/comments/11oo3dj/python_library_for_deep_video_generation/","created":"2023-03-11","tags":["deeplearning","reddit","ml"],"meta":{"num_comments":0}}
