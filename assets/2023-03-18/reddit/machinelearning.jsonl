{"title":"[P] Web Stable Diffusion","description":"Most of the existing stable diffusion demos rely on a server behind to run the image generation. It means you need to host your own GPU server to support these workloads. It is hard to have the demo run purely on web browser, because stable diffusion usually has heavy computation and memory consumption. The web stable diffusion directly puts stable diffusion model in your browser, and it runs directly through client GPU on users\u2019 laptop. \n\nThis means there is no queueing time for the server\u2019s response, more opportunities for client server co-optimizations, and friendly for personalization and privacy.\n\n&amp;#x200B;\n\nGithub page: [https://github.com/mlc-ai/web-stable-diffusion](https://github.com/mlc-ai/web-stable-diffusion)\n\nAlso comes with a online demo: [https://mlc.ai/web-stable-diffusion/](https://mlc.ai/web-stable-diffusion/)","link":"https://www.reddit.com/r/MachineLearning/comments/11u8uk6/p_web_stable_diffusion/","created":"2023-03-18","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":6}}
{"title":"[D] PyTorch 2.0 Native Flash Attention 32k Context Window","description":"Hi,\n\nI did a quick experiment with Pytorch 2.0 Native scaled\\_dot\\_product\\_attention. I was able to a single forward pass within 9GB of memory which is astounding. I think by patching existing Pretrained GPT models and adding more positional encodings, one could easily fine-tune those models to 32k attention on a single A100 80GB. Here is the code I used:\n\n&amp;#x200B;\n\nhttps://preview.redd.it/6csxe28lv9oa1.png?width=607&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1db074eaea9bb6d0b95678c2cfe39dc71cb48adf\n\nI think it should be possible to replicate even GPT-4 with open source tools something like Bloom + FlashAttention &amp; fine-tune on 32k tokens.\n\n**Update**: I was successfully able to start the training of GPT-2 (125M) with a context size of 8k and batch size of 1 on a 16GB GPU. Since memory scaled linearly from 4k to 8k. I am expecting, 32k would require \\~64GB and should train smoothly on A100 80 GB. Also, I did not do any other optimizations. Maybe 8-bit fine-tuning can further optimize it.\n\n**Update 2**: I basically picked Karpaty's nanoGPT and patched the pretrained GPT-2 by repeating the embeddings N-times. I was unable to train the model at 8k because generation would cause the crash.  So I started the training for a context window of 4k on The Pile: 1 hour in and loss seems to be going down pretty fast. Also Karpaty's generate function is super inefficient, O(n\\^4) I think so it took forever to generate even 2k tokens. So I generate 1100 tokens just to see if the model is able to go beyond 1k limit. And it seems to be working. [Here are some samples](https://0bin.net/paste/O-+eopaW#nmtzX1Re7f1Nr-Otz606jkltvKk/kUXY96/8ca+tb4f) at 3k iteration.\n\n&amp;#x200B;\n\nhttps://preview.redd.it/o2hb25w1sboa1.png?width=1226&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1c7c1eda0e20f5123ea7c143a286aa9bb9a48491\n\n**Update 3**: I have started the training and I am publishing the training script if anyone is interested in replicating or building upon this work. Here is the complete training script:\n\n[https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c](https://gist.github.com/NaxAlpha/1c36eaddd03ed102d24372493264694c)\n\nI will post an update after the weekend once the training has progressed somewhat.","link":"https://www.reddit.com/r/MachineLearning/comments/11tmpc5/d_pytorch_20_native_flash_attention_32k_context/","created":"2023-03-17","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":63}}
{"title":"[D] An Instruct Version Of GPT-J Using Stanford Alpaca's Dataset","description":"I  just released an instruct version of GPT-J using Stanford Alpaca's  dataset.The result of this experiment is very cool and confirms that,  when fine-tuned on the right data, GPT-J is a very powerful AI model!You  can download the model from the HuggingFace hub: [https://huggingface.co/nlpcloud/instruct-gpt-j-fp16](https://huggingface.co/nlpcloud/instruct-gpt-j-fp16)\n\nHere is an example:\n\n`from transformers import pipeline import torch`\n\n`generator = pipeline(model=\"nlpcloud/instruct-gpt-j-fp16\", torch_dtype=torch.float16, device=0)`\n\n`prompt = \"Correct spelling and grammar from the following text.\\nI do not wan to go\\n\" print(generator(prompt))`\n\nMore details about this experiment here: [https://nlpcloud.com/instruct-version-of-gpt-j-using-stanford-alpaca-dataset.html](https://nlpcloud.com/instruct-version-of-gpt-j-using-stanford-alpaca-dataset.html?utm_source=reddit&amp;utm_campaign=nwu8d596-3816-11ed-a261-0242ac140007)\n\nI hope it will be useful! Please don't hesitate to share some feedbacks!\n\nJulien","link":"https://www.reddit.com/r/MachineLearning/comments/11tqryd/d_an_instruct_version_of_gptj_using_stanford/","created":"2023-03-17","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":9}}
{"title":"[R] ViperGPT: Visual Inference via Python Execution for Reasoning","description":"[https://viper.cs.columbia.edu/](https://viper.cs.columbia.edu/)\n\nPaper - [https://arxiv.org/abs/2303.08128](https://arxiv.org/abs/2303.08128)","link":"https://www.reddit.com/r/MachineLearning/comments/11ty65w/r_vipergpt_visual_inference_via_python_execution/","created":"2023-03-17","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":2}}
{"title":"[D] Unit and Integration Testing for ML Pipelines","description":"In the context of ML Pipelines (ETL, model training, model deployment and model serving scripts), are there any best practices on what test coverage to implement on these code artifacts?","link":"https://www.reddit.com/r/MachineLearning/comments/11ujf7d/d_unit_and_integration_testing_for_ml_pipelines/","created":"2023-03-18","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[D] Newbie question about Stanford Alpaca 7b fine-tuning","description":"Hi, I have a question related to Stanford's newly released model Alpaca. I took the dataset they used to train it and replaced all output fields that were generated by gpt3 (text-davinci-003) with outputs generated by gpt-3.5-turbo (API). When I compared the outputs, the GPT 3.5 were usually a bit longer, and more informative.\n\nMy question is, if I use this updated data to train Facebook's llama, can I expect better outputs than what Stanford Alpaca achieved? And lastly, if I let's say triple the amount of data and feed it to the Facebook's model, could the responses possibly be close to ChatGPT?","link":"https://www.reddit.com/r/MachineLearning/comments/11u4u6b/d_newbie_question_about_stanford_alpaca_7b/","created":"2023-03-17","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":3}}
{"title":"[R] Online AI Game Announcement","description":"Hi all!     \n\n\nI\u2019m a PhD student at Stanford working on foundation models, and thought one of my recent research projects would be of interest to this community.   \n\n\nWe just released on online game (link in comments) where you collaborate with a text-to-image AI model to create target images and compete with players around the world. Takes only 3 min to play (to get a high score you may need to play longer) and helps us study Human-AI collaboration. New image challenges are released daily. Try out the game and share with your friends!","link":"https://www.reddit.com/r/MachineLearning/comments/11twgbv/r_online_ai_game_announcement/","created":"2023-03-17","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":22}}
{"title":"LLMs are getting much cheaper \u2014 business impact? [D]","description":"Saw this out of Stanford. Apologies if it\u2019s been shared here already. \n\n*We introduce Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On our preliminary evaluation of single-turn instruction following, Alpaca behaves qualitatively similarly to OpenAI\u2019s text-davinci-003, while being surprisingly small and easy/cheap to reproduce (&lt;600$).*\n\nBasically, starting w an open source Meta 7B LLaMa model, they recruited GPT-3.5 to use for self-instruct training (as opposed to RLHF) and were able to produce a model that behaved similar to GPT-3.5. Amazingly, the process only took few weeks and $600 in compute cost.  \n\nAny thoughts on how such low cost to train/deploy LLMs could affect companies like AMD, Nvidia and Intel etc? This seems like new idiom of AI tech and trying to wrap my head around CPU/GPU demand implications given the apparent orders of magnitude training cost reduction. \n\nLink: https://crfm.stanford.edu/2023/03/13/alpaca.html","link":"https://www.reddit.com/r/MachineLearning/comments/11tenm7/llms_are_getting_much_cheaper_business_impact_d/","created":"2023-03-17","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":102}}
{"title":"[N] Jumpy 1.0 has now been released by the Farama Foundation","description":"Jumpy 1.0 is now live, and the project is stable and mature.\n\nJumpy is a lightweight project for easily switching between Jax and Numpy functions that can serve as a drop-in replacement for Jax. This allows for writing one codebase that can use either backend, allowing for creating codebases that work with either data structure type or easier debugging of code. This project is already being used in Gymnasium to create environment wrappers that can support both Numpy and Jax-based hardware accelerated environments. We plan to continue improving the project with support for PyTorch functions, all Numpy functions and more functionality to support enabling or disabling different backends\n\nYou can read the full release notes here: [https://github.com/Farama-Foundation/Jumpy/releases/tag/1.0.0](https://github.com/Farama-Foundation/Jumpy/releases/tag/1.0.0)","link":"https://www.reddit.com/r/MachineLearning/comments/11twq6s/n_jumpy_10_has_now_been_released_by_the_farama/","created":"2023-03-17","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":6}}
{"title":"[D] Are there any open source feature stores that do not rely on K8s?","description":"We have investigated some open source feature stores like Feast and FeatureForm, but most require Kubernetes to deploy on the cloud. Unfortunately, our organization isn't very mature in adopting Kubernetes. Are there any recommended feature stores that don't require K8s to deploy its infrastructure?","link":"https://www.reddit.com/r/MachineLearning/comments/11uhrq8/d_are_there_any_open_source_feature_stores_that/","created":"2023-03-18","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[D] ACL 2023 paper reviews.","description":"The reviews for ACL 2023 papers are expected to be released soon, and this post aims to start a conversation about the same. Let's share our thoughts and feelings about the joys and pains of paper reviews!","link":"https://www.reddit.com/r/MachineLearning/comments/11tp27j/d_acl_2023_paper_reviews/","created":"2023-03-17","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":57}}
{"title":"[R] RWKV 14B ctx8192 is a zero-shot instruction-follower without finetuning, 23 token/s on 3090 after latest optimization (16G VRAM is enough, and you can stream layers to save more VRAM)","description":"I try the \"Alpaca prompt\" on RWKV 14B ctx8192, and to my surprise it works out of box without any finetuning (RWKV is a 100% RNN trained on 100% Pile v1 and nothing else):\n\nhttps://preview.redd.it/fciatottq7oa1.png?width=1046&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=891904adbadefb5902b86f67098c852da88dc167\n\nYou are welcome to try it in RWKV 14B Gradio (click examples below the panel):\n\n[https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio](https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio)\n\nTips: try \"Expert Response\" or \"Expert Long Response\" or \"Expert Full Response\" too.\n\nhttps://preview.redd.it/qo71b85vq7oa1.png?width=2516&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c4b1717754d03e28b4bba01530672935407e7797\n\n===================\n\nChatRWKV v2 is now using a CUDA kernel to optimize INT8 inference (23 token/s on 3090): [https://github.com/BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV)\n\nUpgrade to latest code and \"pip install rwkv --upgrade\" to 0.5.0, and set os.environ\\[\"RWKV\\_CUDA\\_ON\"\\] = '1' in v2/chat.py to enjoy the speed.\n\nThe inference speed (and VRAM consumption) of RWKV is independent of ctxlen, because it's an RNN (note: currently the preprocessing of a long prompt takes more VRAM but that can be optimized because we can process in chunks).\n\nMeanwhile I find the latest RWKV-4-Pile-14B-20230313-ctx8192-test1050 model can utilize a long ctx:\n\nhttps://preview.redd.it/a68dw0hzq7oa1.png?width=398&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=307e4d7847cb03cab3930b3ea07e9b2f856c9b1c","link":"https://www.reddit.com/r/MachineLearning/comments/11teywc/r_rwkv_14b_ctx8192_is_a_zeroshot/","created":"2023-03-17","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":27}}
{"title":"[P] nanoT5 - Inspired by Jonas Geiping's Cramming and Andrej Karpathy's nanoGPT, we fill the gap of a repository for pre-training T5-style \"LLMs\" under a limited budget in PyTorch","description":"We release the code to reproduce the pre-training of a \"Large Language Model\" (T5) under a limited budget (1xA100 GPU, \\~20 hours) in PyTorch. We start from the randomly initialised T5-base-v1.1 (248M parameters) model implemented in HuggingFace. Next, we pre-train it on the English subset of the C4 dataset and then fine-tune it on Super-Natural Instructions (SNI).\n\n**In \\~20 hours on a single GPU, we achieve \\~40 RougeL on the SNI test set, compared to \\~42 RougeL of the original model available on HuggingFace Hub and pre-trained through \"a combination of model and data parallelism \\[...\\] on slices of Cloud TPU Pods\", each with 1024 TPUs.**\n\nOur core contribution is not the T5 model itself, which follows the HuggingFace implementation. Instead, we optimise everything else in the training pipeline to offer you a user-friendly starting template for your NLP application/research.\n\nWe are keen to hear your suggestions to improve the codebase further.\n\n&amp;#x200B;\n\nGithub: [https://github.com/PiotrNawrot/nanoT5](https://github.com/PiotrNawrot/nanoT5)\n\nTwitter: [https://twitter.com/p\\_nawrot/status/1636373725397520384](https://twitter.com/p_nawrot/status/1636373725397520384)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/zluas7u235oa1.png?width=1152&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=8d642abdce1753841b7fc977a141d0f13ca2b213","link":"https://www.reddit.com/r/MachineLearning/comments/11t1857/p_nanot5_inspired_by_jonas_geipings_cramming_and/","created":"2023-03-16","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":18}}
{"title":"[D] instruction tuning : what should I read?","description":"I think I have a decent grasp on transformers, LLMs, prompting, one/few shot learning, fine-tuning. But till now I haven't studied instruction fine tuning and the technique has outgrown my expectations. \nWhere should I start reading about it?\nDo you know any good literature review article to suggest ?","link":"https://www.reddit.com/r/MachineLearning/comments/11tugik/d_instruction_tuning_what_should_i_read/","created":"2023-03-17","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[D] Generate diverse candidates with T5?","description":"Hey guys, I am trying to generate masked span predictions with T5 to use for teacher student distillation. Because of this method, I need the teacher generate a diverse set of predictions, so the student can be trained to match its distribution. However, even when changing parameters like temperature to obscene values (1000+), the teacher still generates the same things every time, and the temperature value doesn\u2019t seem to affect the generation at all. I have also tried beam search, top p, and others. Any ideas how I can do this?","link":"https://www.reddit.com/r/MachineLearning/comments/11tujkb/d_generate_diverse_candidates_with_t5/","created":"2023-03-17","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":3}}
{"title":"[D] Our community must get serious about opposing OpenAI","description":"OpenAI was founded for the explicit purpose of democratizing access to AI and acting as a counterbalance to the closed off world of big tech by developing open source tools.\n\nThey have abandoned this idea entirely.\n\nToday, with the release of GPT4 and their direct statement that they will not release details of the model creation due to \"safety concerns\" and the competitive environment, they have created a precedent worse than those that existed before they entered the field. We're at risk now of other major players, who previously at least published their work and contributed to open source tools, close themselves off as well.\n\nAI alignment is a serious issue that we definitely have not solved. Its a huge field with a dizzying array of ideas, beliefs and approaches. We're talking about trying to capture the interests and goals of all humanity, after all. In this space, the one approach that is horrifying (and the one that OpenAI was LITERALLY created to prevent) is a singular or oligarchy of for profit corporations making this decision for us. This is exactly what OpenAI plans to do.\n\nI get it, GPT4 is incredible. However, we are talking about the single most transformative technology and societal change that humanity has ever made. It needs to be for everyone or else the average person is going to be left behind.\n\nWe need to unify around open source development; choose companies that contribute to science, and condemn the ones that don't.\n\nThis conversation will only ever get more important.","link":"https://www.reddit.com/r/MachineLearning/comments/11sboh1/d_our_community_must_get_serious_about_opposing/","created":"2023-03-15","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":388}}
{"title":"training KGE model [Project]","description":"I have knowledge graph : 24 relationships, 11 entities , &gt; 20K facts   (rows). What I need is the embedding for only one entity, out of those 11. Once the training is completed, I will extract those embeddings and  use them to train a separate GNN model.  \nMy idea was to over-fit  the  KGE model and use all data for training. Given my use case I don't  see  why a test set is needed. Once the model is trained, I will  evaluate it  on the train set, if MRR/ Hits@10 are good, I would extract  the embedding  and mode forward. If not, I will test a different model  and iterate.  \nAm I doing something stupid ?","link":"https://www.reddit.com/r/MachineLearning/comments/11tda6k/training_kge_model_project/","created":"2023-03-17","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":2}}
{"title":"[D] Will Chat GPT X replace Software Engineers and if so when ?","description":"Hello, I'm a newbie at machine learning and I wanted ask the NLP experts here of the possibility in the future that these language models could actually replace software engineers, considering the fact that only experts in this field will be able to answer this question to some degree because they understand the limitations of techs and models.","link":"https://www.reddit.com/r/MachineLearning/comments/11u5voe/d_will_chat_gpt_x_replace_software_engineers_and/","created":"2023-03-17","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":25}}
{"title":"[N] A $250k contest to read ancient Roman papyrus scrolls with ML","description":"Today we launched [the Vesuvius Challenge](https://scrollprize.org/), an open competition to read a set of charred papyrus scrolls that were buried by the eruption of Mount Vesuvius 2000 years ago. The scrolls can't be physically opened, but we have released 3d tomographic x-ray scans of two of them at 8\u00b5m resolution.  The scans were made at a particle accelerator. \n\nA team at UKY led by Prof Brent Seales has [very recently demonstrated](https://scrollprize.org/tutorial4) the ability to detect ink inside the CT scans using CNNs, and so we believe that it is possible for the first time in history to read what's in these scrolls without opening them. There are hundreds of carbonized scrolls that we could read once the technique works \u2013 enough to more than double our total corpus of literature from antiquity.\n\nMany of us are fans of /r/MachineLearning and we thought this group would be interested in hearing about it!","link":"https://www.reddit.com/r/MachineLearning/comments/11sgn67/n_a_250k_contest_to_read_ancient_roman_papyrus/","created":"2023-03-16","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":32}}
{"title":"[D] Creating an open platform for collecting corrective feedback on conversational ML products &amp; projects","description":"Any applied scientist or engineer working with deep learning would tell you that corrective info/feedback is the only way up (especially for massive deep networks). With some of my fellows, we are watching what has been happening in the last couple of months with quite a shock and wonder as the community is throwing valuable feedback (for free) to a (closed source) company from all available online channels (e.g., Reddit, Twitter, Github, blogs), boosting their models (again for free).\n\nI should better note here that this is what they need to go from GPT-4 to v5, or GPTX (folks love X these days).\n\nThere are also valuable calls to action from the community. As a start, community can attempt to create an open initiative to organize all the feedback thrown to open or closed (e.g., GPT-4) conversational models/papers/products. One may argue this will make companies' work even easier, but if there is a resource to mine, it will be mined. Therefore creating a (legal) initiative may work in the favor of the community.\n\nWe should consider that conversational DL (maybe in the far future AI, not sure about that yet) becoming like the commercial aircraft industry where the only way to succeed is to fall and enforce what went wrong. Although the community is very excited now, folks may (probably will) get saturated, and the new companies and initiatives in the future may not get the same amount of feedback. \n\nThis may create a monopoly, so it can be a better idea now to discuss the options how we can unify these valuable resources.","link":"https://www.reddit.com/r/MachineLearning/comments/11szdo9/d_creating_an_open_platform_for_collecting/","created":"2023-03-16","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":5}}
{"title":"[N] bloomz.cpp: Run any BLOOM-like model in pure C++","description":"[bloomz.cpp](https://github.com/NouamaneTazi/bloomz.cpp) allows running inference of BLOOM-like models in pure C/C++ (inspired by llama.cpp). It supports all models that can be loaded with `BloomForCausalLM.from_pretrained()`. For example, you can achieve 16 tokens per second on a M1 Pro.","link":"https://www.reddit.com/r/MachineLearning/comments/11spw6r/n_bloomzcpp_run_any_bloomlike_model_in_pure_c/","created":"2023-03-16","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[N] PyTorch 2.0: Our next generation release that is faster, more Pythonic and Dynamic as ever","description":"Preview of the post since it's dropping in a few hours: https://deploy-preview-1313--pytorch-dot-org-preview.netlify.app/blog/pytorch-2.0-release/\n\n\nAlso a post about Accelerated Diffusers with 2.0: https://deploy-preview-1315--pytorch-dot-org-preview.netlify.app/blog/accelerated-diffusers-pt-20/\n\nGPT Summary:\n\n- PyTorch 2.0 is a next generation release that offers faster performance and support for dynamic shapes and distributed training using torch.compile as the main API.\n\n- PyTorch 2.0 also includes a stable version of Accelerated Transformers, which use custom kernels for scaled dot product attention and are integrated with torch.compile.\n\n- Other beta features include PyTorch MPS Backend for GPU-accelerated training on Mac platforms, functorch APIs in the torch.func module, and AWS Graviton3 optimization for CPU inference.\n\n- The release also includes prototype features and technologies across TensorParallel, DTensor, 2D parallel, TorchDynamo, AOTAutograd, PrimTorch and TorchInductor.","link":"https://www.reddit.com/r/MachineLearning/comments/11s58n4/n_pytorch_20_our_next_generation_release_that_is/","created":"2023-03-15","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":32}}
{"title":"[D] What do people think about OpenAI not releasing its research but benefiting from others\u2019 research? Should google meta enforce its patents against them?","description":"It seems like the days for open research in AI are gone.\n\nAlso, since one of the main reasons they say about not releasing any details is competetive pressure (aka commercial interest), I feel it is fair for others to enforce their patents just like in other fields like pharma? I am very interested in the counter arguments and understanding around this.","link":"https://www.reddit.com/r/MachineLearning/comments/11rtzv6/d_what_do_people_think_about_openai_not_releasing/","created":"2023-03-15","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":166}}
