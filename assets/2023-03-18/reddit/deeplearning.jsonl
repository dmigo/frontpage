{"title":"Question on Attention","description":"Hello Everyone!\n\nI have a question about scoring in attention. In attention, you find the dot product between the query and the keys. From my understanding, the intuition is that we can use the inner product as the a mechanism to understand similarity.\n\n&amp;#x200B;\n\nI don't think I fully understand this:\n\nLets say we have a query q\\_1 = \\[1, 1, 1\\]\n\nAnd we have two keys k\\_1=  \\[1, 1, 1\\] and k\\_2 = \\[100, 50, 100\\]\n\n&amp;#x200B;\n\nThe dot-product for q\\_1 @ k\\_1 = 3 while the dot product for q\\_1 @ k\\_2 = 250\n\nSo in the soft-max, the value associated with k\\_2 will be weighted much higher, even though q\\_1 and k\\_1 were literally the same vector.\n\n&amp;#x200B;\n\nNow this would work if all the vectors were unit length (ie cosine similarity), but most examples I have seen online don't normalize by the magnitude of the vectors, but by sqrt{d} where d seems to be the number of elements?\n\n&amp;#x200B;\n\nIf someone could explain where I am missing something, I would really appreciate it!","link":"https://www.reddit.com/r/deeplearning/comments/11ugj0f/question_on_attention/","created":"2023-03-18","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":1}}
{"title":"Hidden Markov Model in Golang using Gonum","description":"please I'm making a credit card fraud detection system using unsupervised learning. The approach I'm taking is using the hidden Markov Model implemented in golang but same algorithm for some reason, after adding my scaling factors, my transition, emission and start probabilities matrix still get an underflow problem where after like 5 iterations, my matrices start given off NaN (meaning is far off to like 0.00000000....). When debugging I noticed using the scaling vector from my forward process to normalise my beta, the sum of each row doesn't equal 1 and I feel that should be a problem","link":"https://www.reddit.com/r/deeplearning/comments/11u6q66/hidden_markov_model_in_golang_using_gonum/","created":"2023-03-17","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"MOOC/YT tutorials for best Deep Learning","description":"A DS generalist here. Have got some years of experience in traditional ML models and occasionally used TF to build projects for fun/personal interest. But want to be get into DL more seriously. Any suggestions on any MOOC/YouTube channel that would be best for that?","link":"https://www.reddit.com/r/deeplearning/comments/11tplmv/moocyt_tutorials_for_best_deep_learning/","created":"2023-03-17","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":3}}
{"title":"I`m so confused about intel Deep learning course I use the right formula but the answer is always incorrect","description":"&amp;#x200B;\n\n[the law is yt=Wy\\*ht+by but the answer is still wrong ](https://preview.redd.it/61mi5u5umdoa1.jpg?width=738&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=120b5b3aa926108f75f75d5e47bdb6ab73cd9060)","link":"https://www.reddit.com/r/deeplearning/comments/11u6a7z/im_so_confused_about_intel_deep_learning_course_i/","created":"2023-03-17","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":1}}
{"title":"Reading PointNet (CVPR2017) and wondering what 'feature alignment' does","description":"as per subject\n\nI am reading PointNet CVPR 2017, and according to the paper,\n\nthe authors said they wanted to introduce transform invariance and, therefore, inserted a 'joint alignment network'.\n\nI guess the first alignment network implements transform invariance because it happens before feature extraction. \n\nBut what would the exact role of feature transform be?","link":"https://www.reddit.com/r/deeplearning/comments/11tjpcp/reading_pointnet_cvpr2017_and_wondering_what/","created":"2023-03-17","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"Quick Question: If I wanted to be cost efficient in terms of building out GPU cluster, I should look for the most cuda cores bang per buck?","description":"So for example, it would be more cost efficient to get two 3090ti than one 4090, assuming they are the same price correct? Lets assume that the cost of electricity is nulled for this question. Is that a safe assumption?","link":"https://www.reddit.com/r/deeplearning/comments/11tlf3x/quick_question_if_i_wanted_to_be_cost_efficient/","created":"2023-03-17","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":2}}
{"title":"I need some material on metric learning","description":"Thats the code I need to write:\n\nA.Train a neural network model on this dataset.\nB. The dataset has a large variety of celebrities.\n\nC.Once the model is trained, you must create a database with images of the celebrities with the output of the descriptor vector from your model.\nAfter training the neural network, you should include this person in the database.\n\nD.Once steps (1), (2), and (3) are completed, you must use this image, which is the person from item (3) wearing a face mask, and perform facial recognition on them.","link":"https://www.reddit.com/r/deeplearning/comments/11tf8g7/i_need_some_material_on_metric_learning/","created":"2023-03-17","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"Deeplearning.AI Mobile AI Event Mar 16th","description":"Check out this deeplearning.ai event today and learn about the hackathon we are launching: [https://www.eventbrite.com/e/pie-ai-palo-alto-build-and-deploy-mobile-ai-apps-tickets-580912263217](https://www.eventbrite.com/e/pie-ai-palo-alto-build-and-deploy-mobile-ai-apps-tickets-580912263217)  \n\n\n&amp;#x200B;\n\nhttps://preview.redd.it/9d9p4o6w84oa1.png?width=2160&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c93f7daf94ce3ed8a374c289f15e5fcdccadf6d1","link":"https://www.reddit.com/r/deeplearning/comments/11swlv4/deeplearningai_mobile_ai_event_mar_16th/","created":"2023-03-16","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":1}}
{"title":"[Tutorial] PyTorch Class Activation Map using Custom Trained Model","description":"PyTorch Class Activation Map using Custom Trained Model\n\n[https://debuggercafe.com/pytorch-class-activation-map-using-custom-trained-model/](https://debuggercafe.com/pytorch-class-activation-map-using-custom-trained-model/)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/wgg3s1ca17oa1.png?width=1000&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e63bb11e597b7ec43d3dfa25539fe99acedccd53","link":"https://www.reddit.com/r/deeplearning/comments/11tbj9v/tutorial_pytorch_class_activation_map_using/","created":"2023-03-17","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"Choose wisely","description":"Hello everyone, I am building my firsts deep-learning based projects and i just noticed that pytorch 2.0 is officially available. I started to learn tensorflow a while ago, but i have heard that pytorch is one of the most popular DL frameworks out there besides tf. Which one you guys prefer and why?\n\n[View Poll](https://www.reddit.com/poll/11t4c9c)","link":"https://www.reddit.com/r/deeplearning/comments/11t4c9c/choose_wisely/","created":"2023-03-16","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":7}}
{"title":"Optimism Phase 2 Token Airdrop! | $OP","description":"","link":"https://www.reddit.com/r/deeplearning/comments/11t9ews/optimism_phase_2_token_airdrop_op/","created":"2023-03-16","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"[HELP] NEED SOMEONE WITH INDEPTH KNOWLEDGE OF Face AI MODIFIERS","description":"I have an account with a crypto app that I need verified and they have frozen my funds. However, all I need is to complete a simple KYC where I have to submit a live selfie. Anyone who can use AI to help me do this? If yes, comment below and I'll reach out or send me a PM indicating the same and expected pay. Thanks","link":"https://www.reddit.com/r/deeplearning/comments/11tievn/help_need_someone_with_indepth_knowledge_of_face/","created":"2023-03-17","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":5}}
{"title":"How To Fine-tune LLaMA Models, Smaller Models With Performance Of GPT3","description":"Recently, the LLaMA models by Meta were released. What makes these models so exciting, is that despite being small enough to run on consumer hardware, popular metrics show that the models perform as well or better than GPT3 despite being over 10X smaller!\n\nThe reason for this increased performance seems to be due to a larger number of tokens being used for training.\n\nNow, following along with the video tutorial and open-source code, you can now fine-tune these powerful models on your own dataset to further increase the ability of these models!\n\n[https://youtu.be/d4Cnv\\_g3GiI](https://youtu.be/d4Cnv_g3GiI)","link":"https://www.reddit.com/r/deeplearning/comments/11ryc3s/how_to_finetune_llama_models_smaller_models_with/","created":"2023-03-15","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":10}}
{"title":"Detect cracks and scratches on microchips..","description":"Hello guys,\n\ni need to classify images of microchips, which have cracks and scratches on them. I want to classify them in good and bad.\n\nThe dataset consist 4 classes and about 3000 images. The Classes are microchip with cooler good/bad and microchip without cooler good/bad.\n\nThe Images are all black/white and have such a structure like in the image i posted. Is it possible to classify this with a CNN and furthermore how could i achieve to highlight the scratched.. like paint the scratch in blue or red or something? Is that possible to achieve? This entire task is for my bachelorthesis and i search ideas on how to solve this with neural networks..\n\n[https://imgur.com/a/v0GkcAZ](https://imgur.com/a/v0GkcAZ)\n\n&amp;#x200B;","link":"https://www.reddit.com/r/deeplearning/comments/11sp5ga/detect_cracks_and_scratches_on_microchips/","created":"2023-03-16","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"How do I prepare for the Microsoft Exams?","description":"","link":"https://www.reddit.com/r/deeplearning/comments/11snji6/how_do_i_prepare_for_the_microsoft_exams/","created":"2023-03-16","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"[P] We are building a curated list of awesome curated list closely related to machine learning, looking for contributions.","description":"Hey r/MachineLearning,\n\nWe are collecting a hand-crafted curated list of awesome curated lists closely related to machine learning.\n\nHere is the link to the Github repo: [https://github.com/zhimin-z/awesome-awesome-machine-learning](https://github.com/zhimin-z/awesome-awesome-machine-learning)\n\nDo any lists need to be included from your perspective? Please let me know, or feel free to submit a pull request.\n\nThe motivation underlying this project is that so many awesome lists regarding machine learning exist on GitHub. But, gradually, it adds a mental burden to memorize where to look for when the ML world is progressing faster and faster these days.\n\nThus, there the project comes, as a unification to sew together all awesome lists closely related to machine learning.","link":"https://www.reddit.com/r/deeplearning/comments/11schoa/p_we_are_building_a_curated_list_of_awesome/","created":"2023-03-15","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
{"title":"How to Use mpirun to Launch a LLaMA Inference Job Across Multiple Cloud Instances","description":"[How to Use mpirun to Launch a LLaMA Inference Job Across Multiple Cloud Instances](https://lambdalabs.com/blog/how-to-use-mpirun-to-launch-a-llama-inference-job-across-multiple-cloud-instances?utm_source=reddit&amp;utm_medium=organic-social&amp;utm_campaign=2023-03-llama-github-repo&amp;utm_content=blog)\n\nGuide on how to use mpirun to launch a LLaMA inference job across multiple Lambda Cloud instances, including a cost analysis for running various LLaMA models on different GPU instances. Notable updates include:\n\n* A script to easily set up a \"cluster\" of cloud instances that is ready to run LLaMA inference (all models from 7B to 65B).\n* mpirun compatible, so you can launch the job directly from the head node without the need of typing in the torchrun command on the worker nodes.\n* Interactive inference mode across multiple nodes.\n* eos\\_w: controls how \"lengthy\" the results are likely to be by scaling the probability of eos\\_token\n* Inference speed profiling (\"tokens/sec\").","link":"https://www.reddit.com/r/deeplearning/comments/11s4u0b/how_to_use_mpirun_to_launch_a_llama_inference_job/","created":"2023-03-15","tags":["reddit","ml","deeplearning"],"meta":{"num_comments":0}}
