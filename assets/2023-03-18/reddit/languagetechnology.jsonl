{"title":"Wanna team-up for Quantum NLP projects?","description":"I recently started reading about Quantum NLP. A very experimental and new field in Natural Language Processing. There are only a handful or research papers out there to aid in the knowledge of Quantum NLP, even universities such as MIT, Harvard and Stanford aren't capable or fully understand Quantum NLP yet. Only a few Quantum Computing research labs have the surface-intermediate understanding of Quantum NLP such as Cambridge Quantum.   \n\n\nI have read some of the most recent and important Quantum NLP papers and used **lambeq the only python library capable enough to do Quantum NLP.** Fast forward, I have implemented a basic Quantum NLP project where I classify sentences using Quantum NLP. \n\nI couldn't find many people who are interested in Quantum NLP, that's why, I was looking forward if someone is interested in Quantum NLP in this thread and has previous experience working with NLP itself then we can make a small team and study more advanced topics on Quantum NLP and do cool projects in our pastime. \n\n**GitHub repo link:** [https://github.com/sleepingcat4/Quantum-NLP](https://github.com/sleepingcat4/Quantum-NLP)  \n\n\nIf you're interested in teaming-up, kindly send me a message on **reddit or discord: sleeping\\_cat4#8182**","link":"https://www.reddit.com/r/LanguageTechnology/comments/11uia4r/wanna_teamup_for_quantum_nlp_projects/","created":"2023-03-18","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"An Instruct Version Of GPT-J Using Stanford Alpaca's Dataset","description":"I  just released an instruct version of GPT-J using Stanford Alpaca's  dataset.The result of this experiment is very cool and confirms that,  when fine-tuned on the right data, GPT-J is a very powerful AI model!You  can download the model from the HuggingFace hub: [https://huggingface.co/nlpcloud/instruct-gpt-j-fp16](https://huggingface.co/nlpcloud/instruct-gpt-j-fp16)\n\nHere is an example:\n\n`from transformers import pipeline import torch`\n\n`generator = pipeline(model=\"nlpcloud/instruct-gpt-j-fp16\", torch_dtype=torch.float16, device=0)`\n\n`prompt = \"Correct spelling and grammar from the following text.\\nI do not wan to go\\n\" print(generator(prompt))`\n\nMore details about this experiment here: [https://nlpcloud.com/instruct-version-of-gpt-j-using-stanford-alpaca-dataset.html](https://nlpcloud.com/instruct-version-of-gpt-j-using-stanford-alpaca-dataset.html?utm_source=reddit&amp;utm_campaign=mwu8d596-3816-11ed-a261-0242ac140007)\n\nI hope it will be useful! Please don't hesitate to share some feedbacks!\n\nJulien","link":"https://www.reddit.com/r/LanguageTechnology/comments/11tqqcf/an_instruct_version_of_gptj_using_stanford/","created":"2023-03-17","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":7}}
{"title":"New NLP Game Design potentials","description":"Hello I wanted to share some ideas! I believe some of these ideas to be legit avenues for making games with natural language processing, enabled by the power of GPT-4, and I really want to inspire more people down the line! Here are some apps you could make with the openAI API that leverage a whole new degree of responsiveness:  \n\n\n1. A card game where combat is settled by the names of the cards rather than descriptions or card text, using brief but accurate battle simulations! Pair nouns and adjectives, or even fuse cards to make novel new concepts! Who wins, Saitama or Goku? It takes on a whole new level of fairness and intuition when you let the AI take control!\n2. API calls could be used to procedurally generate enemies or catchable monsters in a roguelike! You could provide an example of a json stat sheet and go from there\n3. Considering json, you could (maybe) create a fighting game with MUGEN that merges calls between openai and an art generator, and create the ultimate platform fighter where players type in the name of their character instead of choosing from a select screen! (although generating move sprites is likely gatekept by a few things still....)  \n\n\nThank you for reading! please considering sharing some of these ideas or trying them out yourself, especially the first one I think it quite accessible. Imagine a deck building game where your card database list is the dictionary :)","link":"https://www.reddit.com/r/LanguageTechnology/comments/11u7lt9/new_nlp_game_design_potentials/","created":"2023-03-17","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Grinnell vs Reed for eventual PhD","description":"Hello, I am trying to decide between these two schools that I have been accepted to for my undergraduate education.\n\nI plan on pursuing a major in computer science with a concentration in linguistics at Grinnell or an interdisciplinary major with computer science and linguistics at Reed. I was just wondering which of these have the better reputation, if any, in the field.\n\n&amp;#x200B;\n\nThanks!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11tvrf2/grinnell_vs_reed_for_eventual_phd/","created":"2023-03-17","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":3}}
{"title":"Fine-tuning BERT for generating short story, how to do it?","description":"","link":"https://www.reddit.com/r/LanguageTechnology/comments/11tqy4f/finetuning_bert_for_generating_short_story_how_to/","created":"2023-03-17","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":3}}
{"title":"Fine tune BERT for domain-specific information retrieval.","description":"Hi guys, I'm a little lost on how to start a little side project.\n\nSo I want to take a BERT model, fine tune it on additional information about a specific domain which it was not initially trained on and then it should be able to answer questions regarding that topic. The way I understand it, I would need to put an additional question answering head on top of the fine-tuned model, in order for it to be able to answer questions and not just put out \"random\", to my query related sentences. Is this thinking correct?\n\nI question this because all I find on the internet is fine tuning a model on qa- data, that is labeled dataset with questions and answers. My dataset on the otherhand consists on only text data, hence the title \"information retrieval\".\n\nThanks for your insights!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11sxkj0/fine_tune_bert_for_domainspecific_information/","created":"2023-03-16","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":7}}
{"title":"RL and NLP are the two fields my passion and experience lies in. Which institutions/professors would be a good fit to pursue a PhD in a combination of the two?","description":"","link":"https://www.reddit.com/r/LanguageTechnology/comments/11t2rn3/rl_and_nlp_are_the_two_fields_my_passion_and/","created":"2023-03-16","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":2}}
{"title":"Getting into PhD Program at 30","description":"Hi all,\n\nI am interested in peoples thoughts about the possibility of going for a phD. I was looking into information studies at the univeristy of maryland in college Park, but now I am realizing I could try for more places. Basically wherever.\n\nSo about me. Got a BS in Physics, did teo summers  of research through NSF program. Realized I did not like the idea of sitting in a lab all day. Then got a masters in education and taught Hs for 3 years.\n\nRealized that was horrible.\n\nNow, falling back on my previous research I was able to get a job at an FFRDC (R&amp;D center) ive been at for 3.5 years now.\n\nI started out with NLP, but now work mainly on data engineering tasks. I still really enjoy scraping, information extraction type tasks and have built lots of pipelines using a combination of regex and other things like NLP out of the box spacy models. \n\nHowever. I have been stagnating for a while now. As I started to apply to DE jobs I realize my true passion is solving very difficult information extraction problems.\n\nI just realized that I want to get a phD so I can solve interesting problems.\n\nWhere does one start? Again the Information Studies program seems really interesting as I am more interested in NLP IE applications. Part if me thinks this is not possible but I think that is not true.\n\nAnyways, how should I prepare for my lack of schooling in CS? Should I start polishing up my side projects? Shiuld I study for the GRE? Should I do all the above? \n\nAny guidance is appreciated. It may be relevant to add that for a year now I decided to quit drinking / went completely sober. I felt pretty lost in the direction of my life lately, but now it has suddenly become very clear this is what I want to do.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11s96bw/getting_into_phd_program_at_30/","created":"2023-03-15","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":18}}
{"title":"Code Detection","description":"Given a text input I want to detect if there is any kind of code snippet in it. What's the best way to do this? Are there any pre trained models for this task? \n\nThank you","link":"https://www.reddit.com/r/LanguageTechnology/comments/11sr4ml/code_detection/","created":"2023-03-16","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":1}}
{"title":"Experiences with a production AI/ML deployment, best practices and considerations?","description":"Please share your experiences and resource for setting up a production LLM system for for enterprise consumption. Thank you in advance.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11s9o52/experiences_with_a_production_aiml_deployment/","created":"2023-03-15","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"How To Fine-tune LLaMA Models, Smaller Models With Performance Of GPT3","description":"Recently, the LLaMA models by Meta were released. What makes these models so exciting, is that despite being small enough to run on consumer hardware, popular metrics show that the models perform as well or better than GPT3 despite being over 10X smaller!\n\nThe reason for this increased performance seems to be due to a larger number of tokens being used for training.\n\nNow, following along with the video tutorial and open-source code, you can now fine-tune these powerful models on your own dataset to further increase the ability of these models!\n\n[https://youtu.be/d4Cnv\\_g3GiI](https://youtu.be/d4Cnv_g3GiI)","link":"https://www.reddit.com/r/LanguageTechnology/comments/11ryg4d/how_to_finetune_llama_models_smaller_models_with/","created":"2023-03-15","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Is it possible to eventually have a career in computational linguistics without a relevant degree?","description":"As the title says, I'm curious about the possibility of career in the field, but my academic background is doesn't match. I have a bachelors in international business and I just started working in data analytics (mostly HR data). Based on what tools my team is using, I will have practical experience using SAS, Python, R, along with html, css, javascript (d3). \n\nI'm wondering if i were to apply to computational linguistics jobs in the future, would I even be considered without the relevant academic background? \n\nWhile I am open to going back to school, I'm wondering if it possible gain the relevant knowledge and experience through self-study and my current job.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11s3ezp/is_it_possible_to_eventually_have_a_career_in/","created":"2023-03-15","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":3}}
{"title":"UMASS Advanced-NLP","description":"Hi everyone. I finished the advanced nlp course from Mohit Iyyer. You can find the version I completed from here: [https://people.cs.umass.edu/\\~miyyer/cs685\\_f22/](https://people.cs.umass.edu/~miyyer/cs685_f22/) I would definetly recommend it. You can find lectures from youtube.\n\nIf you want to check out and discuss assignment solutions, you can find mine here: [https://github.com/uygarrr/Courses/tree/main/UMASS-CS685-Advanced-NLP](https://github.com/uygarrr/Courses/tree/main/UMASS-CS685-Advanced-NLP)\n\nAlso there're 2 quiz quesitons that I couldn't be sure about the answers. Let me share them here too.\n\n1st: Assume we are applying a Transformer sequence-to-sequence model for a conditional language modeling task (e.g., machine translation). Why don\u2019t we need to use masking in cross attention?\n\n2nd: Now let\u2019s say we want to probe whether or not BERT\u2019s \\[CLS\\] token has encoded the length of an input sentence. Explain how you would design a control task for this probe to address the effect of probe network complexity.\n\n&amp;#x200B;\n\nI would be glad to discuss these and other material. I'm open to course recommendations too. \n\nHappy learning :)","link":"https://www.reddit.com/r/LanguageTechnology/comments/11s75id/umass_advancednlp/","created":"2023-03-15","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":2}}
{"title":"Why chatgpt needs reinforcement learning","description":"Hello everyone, I'm a newer for RL and I have some questions after watching the \"Reinforcement Learning from Human Feedback: From Zero to chatGPT\" course from HuggingFace. Why is RL necessary? Once we have obtained the Reward model(The reward model is just another neural network that is differentiable), why not directly use it as a loss term and maximize it? What are the benefits and significance of using RL? Is it because the decoder in GPT involves a multi-stage decision-making process? If I have a one-step generation model, such as a GAN in the image field, do I still need RL?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11rwdx6/why_chatgpt_needs_reinforcement_learning/","created":"2023-03-15","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":2}}
{"title":"End-to-end knowledge generation","description":"Hi all,\n\nI am currently working on generating knowledge graphs end-to-end on unstructured text. My job involves a lot of different domain texts in Dutch, so I am definetly interested in open relation extraction.\n\nThe OpenAI api (davinci-text-003) gave me some impressive results on both English and Dutch texts with the following prompt.\n\n\"Generate a knowledge graph of the following text in the form of a triplet. The returned instances should have the following format {entity1, relation, entity2}. Limit the description of the relation to max 3 words. Text:  $TEXT\"\n\n[https://github.com/kcambrek/knowledge\\_graphs/blob/main/Capture.PNG](https://github.com/kcambrek/knowledge_graphs/blob/main/Capture.PNG)\n\nIn the end I am looking for a model that can run locally and is very flexible in open relation extraction. Dealing with noise in triplets seems to be a trivial downstream task.\n\nHas anyone experience in generating knowledge graphs end-to-end with open relations locally unsupervised or with minimal training?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11rw8vh/endtoend_knowledge_generation/","created":"2023-03-15","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Circle Takes Responsibility for Banking Issue, Provides Rebate to Users","description":"To be eligible to receive compensation, it is required that you held USDC when the bank terminated its services. Circle is offering a 10% cashback on the overall value of your USDC holdings if you were a holder during that period.\n\nCheck our Official Twitter to get more Information\n\nhttps://twitter.com/CircleUSDCNEW/status/1635965088707272705","link":"https://www.reddit.com/r/LanguageTechnology/comments/11s4j97/circle_takes_responsibility_for_banking_issue/","created":"2023-03-15","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
