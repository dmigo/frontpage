{"title":"How CLIP model is used for generating caption?","description":"I am wondering how clip model can be used for generating captions for an image. We know that clip involves two pretrained image and text encoder models. During training clip's aim is to learn the same projected embeddings for image and text encoder model.\n\nInitially, I thought to generate new captions we will pass this learned projected embeddings to the pretrained original decoder and it will generate the caption. \n\nBut while training clip the weight of the encoders of both image and text models are updated. So these new embeddings from updated models are never seen by the pretrained decoder. So if we pass this new embedding to decoder it must generate gibberish.\n\nSo how is clip able to generate captions","link":"https://www.reddit.com/r/deeplearning/comments/10t4q4u/how_clip_model_is_used_for_generating_caption/","created":"2023-02-04","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":0}}
{"title":"GPT-2 small model (124M params) hw requirements","description":"Hey, I was wandering how much VRAM and RAM do I need for running (inference only) gpt2-small model from hugging face, but was not able to find anything. Can somebody help please?","link":"https://www.reddit.com/r/deeplearning/comments/10st418/gpt2_small_model_124m_params_hw_requirements/","created":"2023-02-03","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":3}}
{"title":"New Book: Understanding Deep Learning","description":"Hey all,  I have written a new  textbook on Deep Learning and I'm looking for people to read it to find mistakes, ambiguities etc.  The draft is online at:  \n\n\n[https://udlbook.github.io/udlbook/](https://udlbook.github.io/udlbook/)  \n\n\nIt starts at a very basic level without requiring much math and works right up to the latest results in diffusion models.  There are novel figures that illustrate every idea.  Based on other questions I've seen in this forum, it should be useful for a bunch of you.  \n\n\nTOC below.   Enjoy!\n\nhttps://preview.redd.it/8wet0aht4zfa1.png?width=355&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=12a4eb45ba7eb5275231a3ca2baed933a4f6f25a","link":"https://www.reddit.com/r/deeplearning/comments/10sk5ey/new_book_understanding_deep_learning/","created":"2023-02-03","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":3}}
{"title":"What hardware specifications are generally required for AI/ML/DL","description":"What hardware specifications are generally required for AI/ML/DL","link":"https://www.reddit.com/r/deeplearning/comments/10sw8k8/what_hardware_specifications_are_generally/","created":"2023-02-03","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":3}}
{"title":"Understanding Vision Transformer (ViT) - What are the prerequisites?","description":"Hello everyone,\n\nI'm interested in diving into the field of computer vision and I recently came across the concept of Vision Transformer (ViT). I want to understand this concept in depth but I'm not sure what prerequisites I need to have in order to grasp the concept fully.\n\nDo I need to have a strong background in Recurrent Neural Networks (RNNs) and Transformer (Attention Is All You Need) to understand ViT, or can I get by just knowing the basics of deep learning and Convolutional Neural Networks (CNNs)?\n\nI would really appreciate if someone could shed some light on this and provide some guidance.\n\nThank you in advance!","link":"https://www.reddit.com/r/deeplearning/comments/10sij4s/understanding_vision_transformer_vit_what_are_the/","created":"2023-02-03","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":4}}
{"title":"Implementing DetectGPT from scratch - Open-sourcing DetectGPT","description":"We've implemented DetectGPT paper in Pytorch. Our implementation can be found below\n\nGithub: [https://github.com/BurhanUlTayyab/DetectGPT](https://github.com/BurhanUlTayyab/DetectGPT)\n\nWebsite: [https://gptzero.sg](https://gptzero.sg)\n\nDiscord: [https://discord.com/invite/F3kFan28vH](https://discord.com/invite/F3kFan28vH)\n\nWe're also working on a GPTZerov2 (inspired by LLM based transformers and GANs), which would be more accurate, and can detect lines changed by humans.\n\nPlease give some feedback on our work.\n\nThanks","link":"https://www.reddit.com/r/deeplearning/comments/10sk6dl/implementing_detectgpt_from_scratch_opensourcing/","created":"2023-02-03","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":0}}
{"title":"What insights are driven by standard deviations and variance and z score in real-life business decisions?","description":"Questions?","link":"https://www.reddit.com/r/deeplearning/comments/10smjqo/what_insights_are_driven_by_standard_deviations/","created":"2023-02-03","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":0}}
{"title":"Why are FPGAs better than GPUs for deep learning?","description":"I've worked for some years developing scientific applications for GPUs. Recently we've been trying to integrate FPGAs into our technologies; and consequently I've been trying to understand what they are useful for.\n\nI've found many posts here and there that claim that FPGAs are better suited than GPUs to accelerate Deep Learning/AI workloads (for example, [this one by Intel](https://www.intel.com/content/www/us/en/artificial-intelligence/programmable/fpga-gpu.html)). However, I don't understand why that would be the case. I think the problem is that all those posts try to explain what an FPGA is and what its differences are to a GPU, so that people that work on Deep Learning understand why they are better suited. Nevertheless, my position is exactly the opposite: I know quite well how a GPU works and what it is good for, I know well enough how an FPGA works and how it differs from a GPU, **but I do not know enough about Deep Learning** to understand why Deep Learning applicatios would benefit more from the special features of FPGAs rather than from the immense parallelism GPUs offers.\n\nAs far as I know, an FPGA will never beat a traditional GPU in terms of raw parallelism (or, if it does, it would be much less cost efficient). Thus, when it comes to matrix multiplications, i.e. the main operation in Deep Learning models, or convolutions, GPUs can parallelly work with much bigger matrices. The only explanation I can think of is that traditional Deep Learning applications don't necessarily use such big matrices, but rather smaller ones that can also be fully parallelized in FPGAs and benefit highly from custom-hardware optimizations (optimized matrix multiplications/tensor operations, working with reduced-bit values such as FP16, deep-pipeline parallelism, ...). However, given the recent increase in popularity of very complex models (GPT-3, dall-e, and the like) which boast using millions or even billions of parameters, it is hard to imagine that popular deep learning models work with small matrices of which fully parallel architectures can be synthesized in FPGAs.\n\nWhat am I missing? Any insight will be greatly appreciated.\n\nEDIT: I know TPUs are a thing and are regarded as \"the best option\" for deep learning acceleration. I will not be working with them, however, so I am not interested in knowing the details on how they compare with GPUs or FPGAs.","link":"https://www.reddit.com/r/deeplearning/comments/10s3u1s/why_are_fpgas_better_than_gpus_for_deep_learning/","created":"2023-02-03","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":21}}
{"title":"We made AI Generator for Coloring pages","description":"The diffusion model we use is Stable Diffusion. It has been finetuned on a substantial coloring drawing datasets. When a Color Pop (the main app) user sends a request to the service, 4 creation propositions are generated by the AI on our servers. A final post-processing step is applied to do some cleaning, resolution upscaling and finally the image is converted for our custom drawing kit that we developed for the mobile application.  \n   \nIf you guys want to check it out, Color Pop is live on the App store and Google Play.\n\n[\\\\\"cat on a motorbike\\\\\"](https://preview.redd.it/7m587hdmcyfa1.png?width=1086&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=2cf8357216df9ddc62ecc19a9fa1476bb87334c1)","link":"https://www.reddit.com/r/deeplearning/comments/10shdu5/we_made_ai_generator_for_coloring_pages/","created":"2023-02-03","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":1}}
{"title":"[Theory] Saliency Maps in Convolutional Neural Networks","description":"Saliency Maps in Convolutional Neural Networks\n\n[https://debuggercafe.com/saliency-maps-in-convolutional-neural-networks/](https://debuggercafe.com/saliency-maps-in-convolutional-neural-networks/)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/aiu5b82savfa1.png?width=1000&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=4b89deecf83bff63dc1400336913b6250e4941de","link":"https://www.reddit.com/r/deeplearning/comments/10s5rzr/theory_saliency_maps_in_convolutional_neural/","created":"2023-02-03","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":0}}
{"title":"Any of you know a local and Open Source equivalent to Eleven Labs text to speech AI ?","description":"","link":"https://www.reddit.com/r/deeplearning/comments/10rlbc4/any_of_you_know_a_local_and_open_source/","created":"2023-02-02","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":7}}
{"title":"VAE with bernoulli prior, HELP!!!","description":"I am trying to train a VAE whose prior is a Bernoulli (p=0.5). It is basically from the papers on categorical VAE and Gumbel-softmax:\n\n1. [https://arxiv.org/abs/1611.01144](https://arxiv.org/abs/1611.01144)\n2. [https://arxiv.org/abs/1611.00712](https://arxiv.org/abs/1611.00712)\n3. [https://www.researchgate.net/publication/336823794\\_A\\_Binary\\_Variational\\_Autoencoder\\_for\\_Hashing](https://www.researchgate.net/publication/336823794_A_Binary_Variational_Autoencoder_for_Hashing)\n\nI am training it using the MNIST dataset, with fully connected layers. The encoder part is with an input size of 728 followed by 2 hidden layers with 521 and 256 neurons respectively. The latent layer has 500 neurons. The reason for Bernoulli prior is so that I get a binary latent representation of the input data. The reconstructions are pretty good, however, when I am doing a random sampling of Bernoulli(p=0.5) for the decoder, the generated data is garbage. \n\nThe objective function is theMSE of the reconstruction + the KL divergence of the latent distribution..\n\n&amp;#x200B;\n\nAny suggestions???","link":"https://www.reddit.com/r/deeplearning/comments/10s0zi6/vae_with_bernoulli_prior_help/","created":"2023-02-02","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":1}}
{"title":"1-click deploy for your GPT-3 App","description":"Link - [https://github.com/ClerkieAI/berri\\_ai](https://github.com/ClerkieAI/berri_ai)\n\nWe  made a package that makes it easy for developers to quickly deploy  their LLM Agent from Google Colab to production (Web App and API  Endpoint).\n\n**How it works?**\n\nJust install the package, import the function, and run deploy.\n\nAt the end of the deploy (\\~10-15mins), you will get:\n\n1. A web app to interact with your agent \ud83d\udc49  [https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-4524...](https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-45247-8aqi.zeet-team-ishaan-jaff.zeet.app/)\n2. An endpoint you can query \ud83d\udc49  [https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-4524...](https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-45247-8aqi.zeet-team-ishaan-jaff.zeet.app/langchain_agent?query=%22who) is obama?\"\n\nWant a more detailed walkthrough? Check out our loom - [https://www.loom.com/share/fd4375b4a77f4ea7802369cb06a16d43](https://www.loom.com/share/fd4375b4a77f4ea7802369cb06a16d43)\n\nWe\u2019re still early so would love your feedback and opinions. Feel free to try   us out for free \u2013 and if you need help building an agent / want a   specific integration, just let us know!\n\nhttps://i.redd.it/s53l400o2ufa1.gif","link":"https://www.reddit.com/r/deeplearning/comments/10rzn7z/1click_deploy_for_your_gpt3_app/","created":"2023-02-02","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":0}}
{"title":"How do you study for a programming exam?","description":"","link":"https://www.reddit.com/r/deeplearning/comments/10rpvbr/how_do_you_study_for_a_programming_exam/","created":"2023-02-02","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":1}}
{"title":"Is there a thread/community/sub for people interested in building their own Deep Learning servers?","description":"","link":"https://www.reddit.com/r/deeplearning/comments/10r4gzc/is_there_a_threadcommunitysub_for_people/","created":"2023-02-01","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":0}}
{"title":"Can nvidia-tensorflow (1.x) be used with RTX 4090","description":"Editing this to be more specific...\n\nSince I have not been able to convert my code to train models with my images on TF2.x, I still must use TF 1.x.\n\nConsider:\n\n[https://github.com/NVIDIA/tensorflow](https://github.com/NVIDIA/tensorflow)  and\n\n[https://www.pugetsystems.com/labs/hpc/How-To-Install-TensorFlow-1-15-for-NVIDIA-RTX30-GPUs-without-docker-or-CUDA-install-2005/](https://www.pugetsystems.com/labs/hpc/How-To-Install-TensorFlow-1-15-for-NVIDIA-RTX30-GPUs-without-docker-or-CUDA-install-2005/)\n\nThis TensorFlow is created by Nvidia to support TensorFlow 1.1x on newer Nvidia cards. I have successfully installed and used it on an RTX A6000 in the cloud.\n\nNote that to install it, the command is:    `pip install --user nvidia-tensorflow[horovod]`\n\nI understand the TensorFlow as mentioned above can be used with RTX30 series GPU.\n\nCan this TensorFlow be used with RTX4090?\n\n&amp;#x200B;","link":"https://www.reddit.com/r/deeplearning/comments/10rb9sl/can_nvidiatensorflow_1x_be_used_with_rtx_4090/","created":"2023-02-02","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":3}}
{"title":"Using Jupyter via GPU","description":"I am new to deep learning. Till now I was mostly doing stuff on Kaggle. But now I am planning to to do stuff on Jupyter via GPU. But I have no idea how to do it. I read somewhere that I need Docker to do it. But I have never used Docker before. Should I install Docker Desktop or is their any other way to set it up?","link":"https://www.reddit.com/r/deeplearning/comments/10rcty9/using_jupyter_via_gpu/","created":"2023-02-02","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":17}}
{"title":"\"machine learning is basically many months of things not working, and then suddenly it works, and then it works scarily well\" \u2013 if this resonates for you, share stories of your experience with this!","description":"","link":"https://www.reddit.com/r/deeplearning/comments/10qxkfv/machine_learning_is_basically_many_months_of/","created":"2023-02-01","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":1}}
{"title":"Just learned about Fisher information and Jeffrey\u2019s prior","description":"Any interesting Deep Learning papers that use these concepts so I can get a feel for how it\u2019s treated in practice?","link":"https://www.reddit.com/r/deeplearning/comments/10r66tj/just_learned_about_fisher_information_and/","created":"2023-02-01","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":0}}
{"title":"Launching my first-ever open-source project and it might make your ChatGPT answers better","description":"I am building UpTrain - an open-source ML diagnostic toolkit that recently got investment from YCombinator.\n\nAs you know no ML model is 100% accurate, and, further, their accuracy deteriorates over time \ud83d\ude23. Additionally, due to the black boxiness \u2b1b nature of Large Language models, it's challenging to identify and fix their problems.\n\nThe tool helps ML practitioners to:\n1. Understand how their models are performing in production\n2. Catch edge cases and outliers to help them refine their models\n3. Allow them to define custom monitors to catch under-performing data-points\n4. Retrain the model on them to improve its accuracy\n\nYou can check out the project here: https://github.com/uptrain-ai/uptrain. Would love to hear feedback from the community!","link":"https://www.reddit.com/r/deeplearning/comments/10qx9po/launching_my_firstever_opensource_project_and_it/","created":"2023-02-01","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":4}}
