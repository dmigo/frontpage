{"title":"Need help with chat bot and text processing task","description":"Hi,\n\nI am trying to build a system that a user can enter sentences and an agent (or bot) will process each sentence and extract relevant information. The extracted data is saved into a queue for processing later.  \n\nFor example: Given the following input, (left side), I want the system the following output (right side) as shown below:\n\nInput: \"Add a line called LINE1 from point (1,1,1) to point (10,2,5) -&gt; output : \\[\"LINE1\",\\[1,1,1\\],\\[10,2,5\\]\\]\n\nInput: Add a new line, LINE2, that starts from (2,4,3) to (2,5,3) -&gt; output: \\[\"LINE2\", \\[2,4,3\\],\\[2,5,3\\]\\]\n\nInput\" Draw a line from (2,1,3) to (3,6,1) -&gt; output: \\[\"DUMMY1\",\\[2,1,3\\], \\[3,6,1\\]\\]\n\nThe output from the agent is of the form \\[Line name, \\[coordinate 1\\], \\[coordinate 2\\]\\].\n\nThe last example may add complication, where if the agent can't get the name of the line from the text, it may subsitute some name, like \"dummy1\", \"dummy2\" (this is done later after knowing that the name of the line is missing or not provided). If this complicated the task, we can restrict the use case to the first two examples. \n\nNote that the coordinate points are arbitrary (for now) but I may need to add some constraints, e.g., where there is a min and max value for each axis (but this is not critical). If the bounds are violated, perhaps I can return back and error. \n\nHow might I go about achieving something like this?\n\nI want to use my knowlegde build an agent (if necessary) or there is some open source that can help with this task. \n\nThanks","link":"https://www.reddit.com/r/LanguageTechnology/comments/10t7j1z/need_help_with_chat_bot_and_text_processing_task/","created":"2023-02-04","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":1}}
{"title":"Do we need word embeddings nowadays?","description":"I just finished the Sequence Model [Deeplearning.ai](https://Deeplearning.ai) course, and since the field is so fast passed, a lot have changed between when the course was made and what is currently the best practice.\n\nI was wondering if we need to use word embedding nowadays with the new architecture like BERT and others, they seem to get a better sense of context and word similarities than previous models. It was just a thought.","link":"https://www.reddit.com/r/LanguageTechnology/comments/10sow1s/do_we_need_word_embeddings_nowadays/","created":"2023-02-03","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":6}}
{"title":"Generate Knowledge Graphs from Unstructured Texts with GPT-3!","description":"Using GraphGPT, convert your favorite movie synopsis, a Wikipedia page, or a video transcript into an interactive graph visualization of entities and their relationships. [https://www.youtube.com/watch?v=mYCIRcobukI](https://www.youtube.com/watch?v=mYCIRcobukI)\n\nGithub: [https://github.com/varunshenoy/GraphGPT](https://github.com/varunshenoy/GraphGPT)  \nDemo: [https://graphgpt.vercel.app/](https://graphgpt.vercel.app/)","link":"https://www.reddit.com/r/LanguageTechnology/comments/10skua4/generate_knowledge_graphs_from_unstructured_texts/","created":"2023-02-03","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":1}}
{"title":"Anyone know of a tool to align (existing) subtitles to audio along sentence boundaries?","description":"I have an audiobook that I've aligned with the text using Youtube's auto align. The text and audio are perfectly aligned now, but I'm wondering if there's a tool that can align the subtitles to be one sentence per line. \n\nI'm trying to make flashcards, but would like to put the audio for the sentence on the card, but the current splits in the subtitles are pretty random, and not at sentence boundaries.\n\nI've tried [syncabook](https://github.com/r4victor/syncabook), but that didn't help. I've tried [whisperX](https://github.com/m-bain/whisperX), to get word-level timings, but the results are pretty bad/unusable. \n\nI would like to use the existing subtitles/text (as opposed to generated) since it is from the book itself.\n\nAny help would be great!","link":"https://www.reddit.com/r/LanguageTechnology/comments/10ss8hb/anyone_know_of_a_tool_to_align_existing_subtitles/","created":"2023-02-03","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":0}}
{"title":"[ADVISE NEEDED] Extracting clauses from contracts","description":"Hi everyone!\n\nI am currently trying to extract specific clauses from employment contracts that describe something the employee needs to ask approval for from the employer (e.g., requesting time off or requesting a waiver for a non-compete) while ignoring other clauses that do not contain asking-for-approval actions for something (e.g., statement about working days and hours, the position description, or the salary components). And I would like your advice or recommendations on doing this.\n\nThe scenario: the employment contracts are in English and PDF format. I have a manually labeled data set of example clauses that I want (containing asking-for-approval actions). The data describes exactly where the clauses are located in the contract (coordinates and page number). This data set is created from multiple employment contract PDFs. Basically, annotations with the full text from the PDF and also the starting and ending coordinates on where it is located on the page of the contract.\n\nWhat approach would you suggest or recommend for me to tackle this challenge?\n\n&amp;#x200B;\n\nThank you very much!","link":"https://www.reddit.com/r/LanguageTechnology/comments/10seo2i/advise_needed_extracting_clauses_from_contracts/","created":"2023-02-03","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":2}}
{"title":"Fine tuning mt5","description":"How do I fine-tune an MT5 model for generating Bengali paraphrases? I have enough datasets but I can't find a working script to fine-tune an MT5  model.","link":"https://www.reddit.com/r/LanguageTechnology/comments/10rvura/fine_tuning_mt5/","created":"2023-02-02","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":1}}
{"title":"1-click deploy for your GPT-3 App","description":"Link - [https://github.com/ClerkieAI/berri\\_ai](https://github.com/ClerkieAI/berri_ai)\n\nWe  made a package that makes it easy for you to quickly deploy your LLM Agent from Google Colab to production (Web App and API   Endpoint).\n\n**How it works?**\n\nJust install the package, import the function, and run deploy.\n\nAt the end of the deploy (\\~10-15mins), you will get:\n\n1. A web app to interact with your agent \ud83d\udc49  [https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-4524...](https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-45247-8aqi.zeet-team-ishaan-jaff.zeet.app/)\n2. An endpoint you can query \ud83d\udc49  [https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-4524...](https://agent-repo-35aa2cf3-a0a1-4cf8-834f-302e5b7fe07e-45247-8aqi.zeet-team-ishaan-jaff.zeet.app/langchain_agent?query=%22who) is obama?\"\n\nWant a more detailed walkthrough? Check out our loom - [https://www.loom.com/share/fd4375b4a77f4ea7802369cb06a16d43](https://www.loom.com/share/fd4375b4a77f4ea7802369cb06a16d43)\n\nWe\u2019re still early so would love your feedback and opinions. Feel free to try     us out for free \u2013 and if you need help building an agent / want a specific integration, just let us know!","link":"https://www.reddit.com/r/LanguageTechnology/comments/10rzror/1click_deploy_for_your_gpt3_app/","created":"2023-02-02","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":0}}
{"title":"merging two vectors in word2vec","description":"lets say X is a vector that contains the traits of person 1\n\nand Y is a vector that contains the traits of a person 2 \n\nhow to merge X and Y into a vector that describes both","link":"https://www.reddit.com/r/LanguageTechnology/comments/10rufby/merging_two_vectors_in_word2vec/","created":"2023-02-02","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":4}}
{"title":"Ordered Keyword Extraction","description":"I'm interested in finding a way to order important terms, phrases and keywords extracted from a text so that they may be passed to a generative language model in an attempt to create a condensed summary of the original text.\n\nConsider a document that contains the following terms in descending order of importance: solar, rooftop, cheap, advanced, panels, photovoltaics, manufacture, etc. These terms won't necessarily have appeared in this order in the document they're extracted from, so I would like to first extract the important terms (as above) and then place them in order so they still make syntactic sense.\n\nFor example, we may have something like: advanced, manufacture, photovoltaics, rooftop, solar, panels, cheap. This ordering seems to suggest that advanced manufacturing of photovoltaics has helped make rooftop solar panels very cheap. I expect that ordering the terms will help provide context for the generative language model and help make the abstractive summary more accurate.\n\nObviously, in this simple toy example, I could just extract the keywords and place them in the sequential order in which they appear in the original text. Not all applications will be this simple, so is there a way to order the keywords so that they most closely resemble the context of the original text? I think that a graph-based approach like TextRank may be the way to proceed, but I would be very grateful for any thoughts or guidance.","link":"https://www.reddit.com/r/LanguageTechnology/comments/10rf68m/ordered_keyword_extraction/","created":"2023-02-02","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":2}}
{"title":"EMNLP video interviews, workshops, and posters","description":"I learned a lot at EMNLP in December and captured some of what I learned in this video.\n\n**Interviews**\n\nI asked five NLP researchers these questions:\n\n1- What is the most exciting development in NLP in 2022\n\n2- What are you looking forward to in 2023?\n\n3- What is an underrated idea that the field should pay more attention to?\n\nTheir answers start at [01:22](https://www.youtube.com/watch?v=plCvF_7qrmY&amp;t=82s).\n\n**Workshops**\n\nI got to spend time at these workshops:\n\n* [Generation, Evaluation &amp; Metrics (GEM)](https://gem-benchmark.com/workshop)\n* [Massively Multilingual NLU](https://mmnlu-22.github.io/)\n* [Blackbox NLP](https://blackboxnlp.github.io/2022/)\n\nMy main takeaways are at [09:25](https://www.youtube.com/watch?v=plCvF_7qrmY&amp;t=565s).\n\n**Posters**\n\nIf you've been to a conference you'd know there's an overwhelming number of posters. I recorded four of the ones I came across and thought were interesting (covering retrieval-augmented text generation, human evaluation, the BLOOM multimodal dataset, and a multimodal method to name music playlists).\n\nPoster presentations start at [14:38](https://www.youtube.com/watch?v=plCvF_7qrmY&amp;t=878s)\n\nFull video: [https://www.youtube.com/watch?v=plCvF\\_7qrmY](https://www.youtube.com/watch?v=plCvF_7qrmY)\n\nWhat's your answer to these questions?\n\n&gt;1- What is the most exciting development in NLP in 2022  \n2- What are you looking forward to in 2023?  \n3- What is an underrated idea that the field should pay more attention to?","link":"https://www.reddit.com/r/LanguageTechnology/comments/10qxm0l/emnlp_video_interviews_workshops_and_posters/","created":"2023-02-01","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":0}}
{"title":"Can NLP identify interesting quotes?","description":"**I don't have any knowledge of NLP or machine learning in general.**\n\nI have a small product that gathers users' highlights from their books (like ReadWise, but free). I'd like to find a way to separate the 'interesting' highlights (i.e. those you learn something from, although I know it's subjective), from the meaningless ones.\n\nExample of 'interesting' highlight:\n\n*\"As you consider building your own minimum viable product, let this simple rule suffice: remove any feature, process, or effort that does not contribute directly to the learning you seek.\"*\n\n&amp;#x200B;\n\nExample of 'not-interesting' highlight:\n\n*\"My voice is nothing special, but when your mother tells you something about yourself, even if you\u2019ve coaxed it out of her, it\u2019s hard not to always believe it.\"*\n\nIt's probably a dumb question, but I'm running in circles on how to automate this selection. I thought  NLP could maybe help, so any insight is appreciated!","link":"https://www.reddit.com/r/LanguageTechnology/comments/10r4stt/can_nlp_identify_interesting_quotes/","created":"2023-02-01","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":1}}
{"title":"Reducing mistakes by as much as 50% with Recitation aided models: How can you acquire good questions/solutions for samples?","description":"Recently this paper showed how reciting a similar, known truthful question and answer could significantly boost the performance of question/answers, as shown here:\n\n[https://openreview.net/pdf?id=-cqvvvb-NkI](https://openreview.net/pdf?id=-cqvvvb-NkI)\n\nThe paper followed advancements and earlier work such as the notable benefits of few-shots examples with unfinetuned, non-instruct base GPT models, and the \"Ask my anything\" paper which showed how simply formulating questions as ... question and answer pairs... in prompts has similar benefits for retrieval as \"Let's think step by step\" had for factual, logical and consistent answers.\n\n&amp;#x200B;\n\n**What's next? Can we reach almost 100% recall one day for Retrieval augmented tasks and finally overcome hallucination?**\n\nLiterally today the REPLUG paper came out, which clarifies how altering the retriever model can be yet more effective (and in particular by having a retrainable retriever, with echos to the first paper)\n\n[https://arxiv.org/pdf/2301.12652.pdf](https://arxiv.org/pdf/2301.12652.pdf)\n\n&amp;#x200B;\n\n**What's in common between these two?**\n\nIt's the context around the model generation that matters - not finetuning the model  - not for knowledge retrieval tasks anyway (different story for classification, ideation, RTE tasks etc). And excitingly, even small models can reach top level recall with enough context. Sometimes building the context out, and correctly embedding it (/few shot examples), increases your performance on recall at a rate 10x that work on larger models does... this excites me, because it gives power back to the user of the system who equips it with the data, and reduces the power / potential innovation monopoly of those who innovate the cutting edge of LLMs alone...\n\nI am working on retrieval (actually something useful for in context Knowledge retrieveal) and I'd love to know:\n\n**What do you think of the recent trend in better, sourced knowledge retrieval?** If the rate of knowledge retrieval recall continues to skyrocket, will we one day see LLMs equipped with entire companies documents trusted as more factual than any one persons thoughts? Able to identify areas of cognitive dissonance situations, and cite almost every sentence?","link":"https://www.reddit.com/r/LanguageTechnology/comments/10qxh3h/reducing_mistakes_by_as_much_as_50_with/","created":"2023-02-01","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":0}}
