{"title":"[P] I trained an AI model on 120M+ songs from iTunes","description":"Hey ML Reddit!\n\nI just shipped a project I\u2019ve been working on called Maroofy: [https://maroofy.com](https://maroofy.com/)\n\nYou can search for any song, and it\u2019ll use the ***song\u2019s audio*** to find other ***similar-sounding*** music.\n\n**Demo:** [https://twitter.com/subby\\_tech/status/1621293770779287554](https://twitter.com/subby_tech/status/1621293770779287554)\n\n**How does it work?**\n\nI\u2019ve indexed \\~120M+ songs from the iTunes catalog with a custom AI audio model that I built for understanding music.\n\nMy model analyzes raw music audio as input and produces embedding vectors as output.\n\nI then store the embedding vectors for all songs into a vector database, and use semantic search to find similar music!\n\n**Here are some examples you can try:**\n\nFetish (Selena Gomez feat. Gucci Mane) \u2014 [https://maroofy.com/songs/1563859943](https://maroofy.com/songs/1563859943)  The Medallion Calls (Pirates of the Caribbean) \u2014 [https://maroofy.com/songs/1440649752](https://maroofy.com/songs/1440649752)\n\nHope you like it!\n\nThis is an early work in progress, so would love to hear any questions/feedback/comments! :D","link":"https://www.reddit.com/r/MachineLearning/comments/10st28f/p_i_trained_an_ai_model_on_120m_songs_from_itunes/","created":"2023-02-03","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":88}}
{"title":"[R] Multimodal Chain-of-Thought Reasoning in Language Models - Amazon Web Services Zhuosheng Zhang et al - Outperforms GPT-3.5 by 16% (75%-&gt;91%) and surpasses human performance on ScienceQA while having less than 1B params!","description":"Paper: [https://arxiv.org/abs/2302.00923](https://arxiv.org/abs/2302.00923) \n\nGithub: [https://github.com/amazon-science/mm-cot](https://github.com/amazon-science/mm-cot) \n\nTwitter: [https://paperswithcode.com/top-social](https://paperswithcode.com/top-social) \n\nAbstract:\n\n&gt;Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies are mostly isolated in the language modality with LLMs, where LLMs are hard to deploy. To elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning. The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference. To mitigate the effect of such mistakes, we propose Multimodal-CoT that incorporates vision features in a decoupled training framework. The framework separates the rationale generation and answer inference into two stages. By incorporating the vision features in both stages, the model is able to generate effective rationales that contribute to answer inference. **With Multimodal-CoT, our model under 1 billion parameters outperforms the previous state-of-the-art LLM (GPT-3.5) by 16% (75.17%-&gt;91.68%) on the ScienceQA benchmark and even surpasses human performance.** \n\nhttps://preview.redd.it/g9eo0f94k1ga1.jpg?width=1331&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a51e29ed523b624dd70d97841c8b0a5442915c80\n\nhttps://preview.redd.it/fgboci94k1ga1.jpg?width=1323&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=1a3a2fe1a47d4ca04f992b2cf72832f024166711\n\nhttps://preview.redd.it/2ojfym94k1ga1.jpg?width=1660&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e7431fb8532d6331374f1b00adc40248de94f381\n\nhttps://preview.redd.it/k7huem94k1ga1.jpg?width=1326&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=2bcbe91afcdf815171b4c0fd7f8e48f63a8bbb4c\n\nhttps://preview.redd.it/05m8rf94k1ga1.jpg?width=658&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=a8384d649e2140b27dc87525c1546403cd3409f7","link":"https://www.reddit.com/r/MachineLearning/comments/10svwch/r_multimodal_chainofthought_reasoning_in_language/","created":"2023-02-03","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":6}}
{"title":"[N] FT: Google invests $300mn in artificial intelligence start-up Anthropic","description":"From the Financial Times: https://www.ft.com/content/583ead66-467c-4bd5-84d0-ed5df7b5bf9c\n\nUnpaywalled: https://archive.is/ciZPV\n\nI guess I'm a little surprised, this feels like Google backing a competitor to 1) their own Google Brain teams, and 2) Deepmind. The cynical take might be that they're trying to lock in Anthropic; the same way Microsoft locked in OpenAI.","link":"https://www.reddit.com/r/MachineLearning/comments/10sy4at/n_ft_google_invests_300mn_in_artificial/","created":"2023-02-04","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":5}}
{"title":"[D] Understanding Vision Transformer (ViT) - What are the prerequisites?","description":"Hello everyone,\n\nI'm interested in diving into the field of computer vision and I recently came across the concept of Vision Transformer (ViT). I want to understand this concept in depth but I'm not sure what prerequisites I need to have in order to grasp the concept fully.\n\nDo I need to have a strong background in Recurrent Neural Networks (RNNs) and Transformer (Attention Is All You Need) to understand ViT, or can I get by just knowing the basics of deep learning and Convolutional Neural Networks (CNNs)?\n\nI would really appreciate if someone could shed some light on this and provide some guidance.\n\nThank you in advance!","link":"https://www.reddit.com/r/MachineLearning/comments/10siibd/d_understanding_vision_transformer_vit_what_are/","created":"2023-02-03","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":24}}
{"title":"[N] Google Open Sources Vizier, Hyperparameter + Blackbox Optimization Service at Scale","description":"Github: [https://github.com/google/vizier](https://github.com/google/vizier)\n\nGoogle AI Blog: [https://ai.googleblog.com/2023/02/open-source-vizier-towards-reliable-and.html](https://ai.googleblog.com/2023/02/open-source-vizier-towards-reliable-and.html)\n\nTweet from Zoubin Ghahramani: [https://twitter.com/ZoubinGhahrama1/status/1621321675936768000?s=20&amp;t=ZEuz9oSc\\_GWYxixtXDskqA](https://twitter.com/ZoubinGhahrama1/status/1621321675936768000?s=20&amp;t=ZEuz9oSc_GWYxixtXDskqA)","link":"https://www.reddit.com/r/MachineLearning/comments/10solty/n_google_open_sources_vizier_hyperparameter/","created":"2023-02-03","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":2}}
{"title":"[R] What\u2019s your suggestion for offline RL?","description":"Hi guys! I read a lot of offline RL papers in last Fall semester and choose it as my course project. Offline RL seems to be a very hot topic in recent years, I believe that the major challenge for offline RL are (i) distribution shift and (ii) overestimation. The second challenge is caused by (i), because the learners/agents will never allow to interact with the true environment and they will too optimistic for unseen state-actions. Hence, there are many papers to address such challenges, e.g., CQL and MOPO.\n\nHowever can these methods handle misleading datasets? Consider the following example. Suppose we have only one state (MAB) and two arms. The reward of the first arm will return 2/3 with probability 1 and the reward model of second arm is Bernoulli distribution with p=1/2. Clearly, choosing the first arm is the best choice.\n\nNow, for the dataset, unfortunately, all samples on the second arm received reward 1. Because the agent only can access this misleading dataset, if we use Bayesian methods, then the posterior will give a high score for the second arm. If we use Lower Confidence Bound, we need to count the occurrence of each arm. Then, this is very hard to extend this method to MDPs with arbitrary large state and action space. So, does anyone know a function can capture this uncertainty (caused by the dataset) or can any methods to tell the learner that you\u2019re in a very misleading situation?","link":"https://www.reddit.com/r/MachineLearning/comments/10t4cxu/r_whats_your_suggestion_for_offline_rl/","created":"2023-02-04","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":2}}
{"title":"[R] Topologically evolving new self-modifying multi-task learning algorithms","description":"I\u2019ve been developing this idea since I first thought of it in mid December last year. Here\u2019s the elevator pitch (skip to how for technical details):\n\n# Why?\n\nExisting models and learning algorithms are extremely static and unable to generalize across tasks as well as humans or to adapt well to new / changing business requirements. This even applies to the final solutions in recent AutoML (see [An Empirical Review of Automated Machine Learning](https://www.mdpi.com/2073-431X/10/1/11#sec3-computers-10-00011), [AutoML: A survey of the state-of-the-art](https://arxiv.org/abs/1908.00709)). Beyond being static, most suffer from a need for high-performance systems with large amounts of compute and/or memory. This static and bloated nature not only limits the reusability of code, pipelines and all the computations that went into previous versions of a model architecture upon finding a better one. It also forces our preconceptions of what type of learning is best for the task and which degrees of freedom are needed onto the solution. Instead of perpetuating all these assumptions, I want to create a sort of AutoML capable, under the right conditions, of even developing a learning algorithm / model combination that can dynamically add or remove inputs and outputs subsequently incorporating them into the network with adaptive online self-directed learning.\n\n&amp;#x200B;\n\n# How?\n\nBasically, the idea in a nutshell is to use some form of NEAT ([neuro evolution of augmenting topologies](https://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies)) and have special nodes in the network that will be activated based on different criteria (depending on the node\u2019s allele for that gene). When activated, however, these special nodes would not send any input forward but instead apply some property change(s) to their connected nodes and/or edges (yes they can connect to an edge and they could choose a subset of their connections or just apply the change(s) to all or use a maximum number of connection hops, etc). It could also create and destroy nodes depending on the effects defined by the allele. There would also be different firing policies (like the normal always fire or thresholding with or without decay, etc.) for all nodes to allow for better leveraging of temporal dynamics. Basically every property of all these policies, including the policy template itself is a potential target for modification by the special neuromodulatory nodes along with the normal properties of a \u201cneuron\u201d like bias, input weights, activation function, aggregation function, etc. The fitness function would either be abstracted away by using rtNEAT in a simulated environment or just be a combined score over a set of simulated tasks. This should add a regularizing force if the tasks are similar enough to help enforce generalization of the evolved algorithms. There should be no limitation placed on cycles in the graph, in fact I would expect cycles to be part of the evolved solutions, which would make them dynamical systems. To reduce the computational complexity of finding a viable solution, the initial population should also be implementations of existing algorithms in the form of the self-modifying neural networks mentioned. It might even be possible to generate a computational graph from open-source implementations as a starting point for the initial population. All of this together should also allow for different parts of the network to use different learning strategies. Theoretically, this can even allow for the evolution of and incorporation of self-organizing criticality and percolation. This could even evolve something that can dynamically add or remove inputs and outputs then incorporate them into the network with adaptive online learning. The network could literally change the learning paradigm for different portions of itself on the fly in different ways depending on the situation.\n\n&amp;#x200B;\n\n[For further clarity, I'm also attaching this mock up of a design I've started working on for an analysis tool](https://preview.redd.it/njlz2voum1ga1.png?width=4032&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=f25218aaa034ef6c652a8a33ab72e4f55747fa06)\n\n**Thoughts?** Please feel free to chime in. Science should be a public discussion.","link":"https://www.reddit.com/r/MachineLearning/comments/10sw0q1/r_topologically_evolving_new_selfmodifying/","created":"2023-02-03","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":8}}
{"title":"[R] Graph Mixer Networks","description":"I began exploring MLP-Mixer\\[[1](https://arxiv.org/abs/2105.01601),[2](https://arxiv.org/abs/2105.02723)\\]  on Graph Neural Networks in October 2021 and completed my  implementation the ZINC dataset in November of the same year. My  implementation is available on [Github](https://github.com/asarigun/GraphMixerNetworks), but I was unable to fully conduct the experiments due to lack of computational resources.\n\nIn  December 2022, a group of leading figures in the field, including  Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann Lecun, and  Xavier Bresson, published a paper titled \"[A Generalization of ViT/MLP-Mixer to Graphs](https://arxiv.org/abs/2212.13350)\".  Although I am pleased to be working alongside these prominent  researchers on the application of MLP-Mixers to Graphs, I regret that I  was unable to finish my experiments. Encouraged by my friends and  advisors, I decided to make my work public by publishing it on arxiv.  The paper and code can be found as the following:\n\nPaper/report: [https://arxiv.org/abs/2301.12493](https://arxiv.org/abs/2301.12493)  \nGithub: [https://github.com/asarigun/GraphMixerNetworks](https://github.com/asarigun/GraphMixerNetworks)\n\nI  used PNA as my baseline and did not utilize patches in my study, unlike  the other study. I hope someone finds them interesting/useful.","link":"https://www.reddit.com/r/MachineLearning/comments/10sj2qf/r_graph_mixer_networks/","created":"2023-02-03","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":5}}
{"title":"[N] Microsoft integrates GPT 3.5 into Teams","description":"Official blog post: https://www.microsoft.com/en-us/microsoft-365/blog/2023/02/01/microsoft-teams-premium-cut-costs-and-add-ai-powered-productivity/\n\nGiven the amount of money they pumped into OpenAI, it's not surprising that you'd see it integrated into their products. I do wonder how this will work in highly regulated fields (finance, law, medicine, education).","link":"https://www.reddit.com/r/MachineLearning/comments/10rqe34/n_microsoft_integrates_gpt_35_into_teams/","created":"2023-02-02","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":125}}
{"title":"[P] Any thoughts on the possibility of machine learning to retrofit HVAC in buildings?","description":"I often wonder about the best way to retrofit my house to optimize for cost and comfort. \n\nI suspect people already do old school modeling for commercial settings but wondered if it's possible for small fry like me to benefit from this technology if messing learning is involved.\n\nI couldn't think of a better sub to ask but open to that suggestion as well as any other response.","link":"https://www.reddit.com/r/MachineLearning/comments/10svx96/p_any_thoughts_on_the_possibility_of_machine/","created":"2023-02-03","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":4}}
{"title":"[D] Topic extraction to simplify news articles","description":"I build feature stores and my wife works in the media. Was thinking it would be cool to build various topic extraction models to parse the 5-Ws from article text - value prop is to simplify distill EVERY news article to a few bullets for easy consumption. We already have a near infinite data to test on and enough compute from a NLP standpoint. Definitely considering the bias aspect of all this but someone out there (not the media) would be interested in this from a product angle, right? Any thoughts on this? And anyone want to hop on this with me?","link":"https://www.reddit.com/r/MachineLearning/comments/10sua5b/d_topic_extraction_to_simplify_news_articles/","created":"2023-02-03","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":4}}
{"title":"[D] Using a public research dataset for \"testing\" NOT \"training\" a ML model","description":"Is it allowed to use a public dataset like the KITTI dataset to test a model trained for commercial use?\n\nNote that the KITTI dataset is only allowed to be used for research purposes and the model is trained with different data (company specific).","link":"https://www.reddit.com/r/MachineLearning/comments/10sledd/d_using_a_public_research_dataset_for_testing_not/","created":"2023-02-03","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":4}}
{"title":"[p] I built an open source platform to deploy computationally intensive Python functions as serverless jobs, with no timeouts","description":"Hi friends! I ran into this problem enough times at my last few jobs that I built a tool to solve it. I spent many hours building Docker containers for my Python functions, as many of the data science modules required building C libraries (since they significantly speed up compute-intensive routines, such as math calculations). Deploying the containers to AWS Lambda or Fargate (if the processes required more CPU or memory or were &gt;15 minutes) and wiring functions to talk to each other using queues, databases, and blob storage made iterating on the actual code, which wasn't even that complex most of the time, slow.\n\nI made cakework\u00a0[https://github.com/usecakework/cakework](https://github.com/usecakework/cakework), a platform that lets you spin up your Python functions as serverless, production-scale backends with a single command. Using the client SDK, you submit requests, check status, and get results. You can also specify the amount of CPU (up to 16 cores) and memory (up to 128GB) for each individual request, which is helpful when your data size and complexity varies across different requests.\n\nA common pattern that I built cakework for is doing file processing for ML:\n\n\\- ingest data from some source daily, or in response to an external event (data written to blob storage)\n\n\\- run my function (often using pandas/numpy/scipy)\n\n\\- write results to storage, update database\n\n\\- track failures and re-run/fix\n\nIt's open source &lt;3. Here are some fun examples to get you started:\u00a0[https://docs.cakework.com/examples](https://docs.cakework.com/examples)\n\nWould love to hear your thoughts!","link":"https://www.reddit.com/r/MachineLearning/comments/10ryu6b/p_i_built_an_open_source_platform_to_deploy/","created":"2023-02-02","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":11}}
{"title":"[Project] I built a minimal stateless ML project template built on my current favourite stack","description":"Dear r/MachineLearning,\n\nHello everyone! I hope you are all out there having fun, training deep nets and generating fun story-telling with stable-diffusion! :)\n\nI am here today to share with you all a minimal ml project template that I've recently built, which can be found at [https://github.com/AntreasAntoniou/minimal-ml-template/](https://github.com/AntreasAntoniou/minimal-ml-template/). I became increasingly annoyed at how there weren't any repos out there that provided **stateless** ML project templates, which are absolutely necessary when using kubernetes on spot instances, and I decided to build one. By stateless I mean a repo that by default can store model weights in a remote repo and then download them to continue from where it left off if the previous machine dies. The result was this repository.\n\nThe repo remains minimal and extremely readable, all while being packed with a cool stack that I use every day. I'd love to get some feedback, so have a look and let me know.\n\nRegards, Antreas\n\nP.S. A short summary straight from the Github Repo:\n\nThis repo implements a **minimal** machine learning template, that is fully featured for most of the things a machine learning project might need. The most important parts that set this repo apart from the rest are:\n\n1. It is **stateless**. Any given experiment ran using this template, will, automatically and periodically stores the model weights and configuration to [HuggingFace Hub](https://huggingface.co/docs/hub/models-the-hub) and [wandb](https://wandb.ai/site) respectively. As a result, if your machine dies or job exits, and you resume on another machine, the code will automatically locate and download the previous history and continue from where it left off. This makes this repo very useful when using spot instances, or using schedulers like slurm and kubernetes. \n2. It provides support for all the latest and greatest GPU and TPU optimization and scaling algorithms through [HuggingFace Accelerate](https://huggingface.co/docs/accelerate/index).\n3. It provides mature configuration support via [Hydra-Zen](https://github.com/mit-ll-responsible-ai/hydra-zen) and automates configuration generation via [decorators](https://github.com/BayesWatch/minimal-ml-template/blob/af387e59472ea67552b4bb8972b39fe95952dd8a/mlproject/decorators.py#L10) implemented in this repo.\n4. It has a minimal **callback** based boilerplate that allows a user to easily inject any functionality at predefined places in the system without spagettifying the code.\n5. It uses [HuggingFace Models](https://huggingface.co/models) and [Datasets](https://huggingface.co/docs/datasets/index) to streamline building/loading of models, and datasets, but is also not forcing you to use those, allowing for very easy injection of any models and datasets you care about, assuming you use models implemented under PyTorch's `nn.Module` and `Dataset` classes.\n6. It provides plug and play functionality that allows easy hyperparameter search on Kubernetes clusters using [BWatchCompute](https://github.com/BayesWatch/bwatchcompute) and some readily available scripts and yaml templates.\n\n## The Software Stack\n\nThis machine learning project template is built using the following software stack:\n1. Deep Learning Framework: [PyTorch](https://pytorch.org/get-started/locally/)\n2. Dataset storage and retrieval: [Huggingface Datasets](https://huggingface.co/docs/datasets/index)\n3. Model storage and retrieval [Huggingface Hub](https://huggingface.co/docs/hub/models-the-hub), and [HuggingFace Models](https://huggingface.co/models)\n4. GPU/TPU/CPU Optimization and Scaling up options library: [Huggingface Accelerate](https://huggingface.co/docs/accelerate/index)\n5. Experiment configuration + command line argument parsing: [Hydra-zen](https://github.com/mit-ll-responsible-ai/hydra-zen)\n6. Experiment tracking: [Weights and Biases](https://docs.wandb.ai)\n7. Simple python based ML experiment running with Kubernetes using [BWatchCompute](https://github.com/BayesWatch/bwatchcompute)","link":"https://www.reddit.com/r/MachineLearning/comments/10s82tf/project_i_built_a_minimal_stateless_ml_project/","created":"2023-02-03","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":1}}
{"title":"[D] Get log probs of a sentence using OpenAI APIs?","description":"Is there a way to use OpenAI APIs to get the log prob of a given sentence? I don't want new completions, I want to see how the model scores given sentences.","link":"https://www.reddit.com/r/MachineLearning/comments/10sk8qf/d_get_log_probs_of_a_sentence_using_openai_apis/","created":"2023-02-03","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":4}}
{"title":"[D] Why do LLMs like InstructGPT and LLM use RL to instead of supervised learning to learn from the user-ranked examples?","description":"Aligned LLMs such as InstructGPT and ChatGPT are trained via supervised fine-tuning after the initial self-supervised pretraining. Then, the researchers train a reward model on responses ranked by humans. \n\nWhen I understand correctly, they let the LLM generate responses that humans have to rank on a scale from 1-5. Then, they train a reward model (I suppose in supervised fashion?) on these ranked outputs. Once that's done, they use reinforcement learning (RL) with proximal policy optimization (PPO) to update the LLM. \n\nMy question is why they use RL with PPO for this last step? Why don't they fine-tune the LLM using regular supervised learning, whereas the human-ranked outputs represent the labels. Since these are labels in the range 1-5, this could be a ranking or ordinal regression loss for supervised learning.","link":"https://www.reddit.com/r/MachineLearning/comments/10rpj0f/d_why_do_llms_like_instructgpt_and_llm_use_rl_to/","created":"2023-02-02","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":27}}
{"title":"[R] [P] Noisy Sentences Dataset","description":"550K sentences in 5 European languages augmented with noise for training and evaluating spell correction tools or machine learning models. We have constructed our dataset to cover representatives from the language families used across Europe.\n\n* Germanic - English, German;\n* Romance - French;\n* Slavic - Bulgarian;\n* Turkic - Turkish;\n\n**Use case example:** Apply language models or other techniques to compare the sentence pairs and reconstruct the original sentences from the augmented ones. You can use a single multilingual solution to solve the challenge or employ multiple models/techniques for the separate languages. Per-word dictionary lookup is also an option.\n\n**Link:** [https://github.com/radi-cho/noisy-sentences-dataset](https://github.com/radi-cho/noisy-sentences-dataset)","link":"https://www.reddit.com/r/MachineLearning/comments/10sgxs4/r_p_noisy_sentences_dataset/","created":"2023-02-03","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":0}}
{"title":"[R] editing colors on SHAP plot summary","description":"We can change the colors of some texts and backgrounds on a SHAP summary plot by editing matplotlib's matplotlibrc file. \n\nWe can also edit the plotting colors by passing a colormap but we're **unable to change the colors of the \"feature names\" at the left side of the SHAP summary plot (beeswarm) -and the color of the y axis-** by editing matplotlib's matplotlibrc file. \n\nHas anyone worked around this? Is there a way that we could overcome this restriction?","link":"https://www.reddit.com/r/MachineLearning/comments/10smf2i/r_editing_colors_on_shap_plot_summary/","created":"2023-02-03","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":0}}
{"title":"[p] Is it possible to add more classes to an already trained resnet image classifier model without the need to retrain it in all dataset again? [p]","description":"\\[p\\] I am working on massive dataset, and in the future, we'll have to add some more classes over time, can I train the model in the only new classes?\\[p\\]","link":"https://www.reddit.com/r/MachineLearning/comments/10sa859/p_is_it_possible_to_add_more_classes_to_an/","created":"2023-02-03","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":7}}
{"title":"[d]? Is there a way to access youtube alphabetically or by id?","description":"I'm guessing i probably am not the first person who has wanted to work with youtube data so I'm hoping here is a good place to ask\n\nSo i had an idea to make a neural network that would go through your youtube history and then train a neural network on it. Afterwards if there is a way to access all of youtube by id in a way that you can check every video then you could store all of the id for videos you might like and then use a youtube downloader like youtube-dl to download a certain amount. Was just a dumb idea i had but now i want to actually try it but I'm unsure if I'll actually be able to get the data i need to do it","link":"https://www.reddit.com/r/MachineLearning/comments/10sdrp4/d_is_there_a_way_to_access_youtube_alphabetically/","created":"2023-02-03","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":4}}
{"title":"[P] [R] A simplistic UI to edit images with Stable Diffusion and InstructPix2Pix","description":"https://preview.redd.it/ut4us5251rfa1.png?width=2000&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=bf0add1de91537cb806f9f81405d065c95a42cc4\n\nCurrently, the UI supports a picture upload and uses InstructPix2Pix to edit it. Also, it uses upscaling models for quality enhancements. More models are coming soon.\n\nThe goal is to provide a way for non-ML people to use diffusion-based image editing through simplistic app design. Web demo: [https://diffground.com/](https://diffground.com/)","link":"https://www.reddit.com/r/MachineLearning/comments/10rmdwa/p_r_a_simplistic_ui_to_edit_images_with_stable/","created":"2023-02-02","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":3}}
{"title":"[D] Querying with multiple vectors during embedding nearest neighbor search?","description":"Are there tools or techniques that permit you to joint query using more than one query vector? \n\nUse case: iterative ANN search refinement, where I start with a seed vector, select matches, and re-query with more examples to improve the search results.\n\nI tried doing this with FAISS, but it performs a \"batch query\" that returns a separate set of results for each query vector (not a joint query).","link":"https://www.reddit.com/r/MachineLearning/comments/10rvkru/d_querying_with_multiple_vectors_during_embedding/","created":"2023-02-02","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":16}}
{"title":"[R] Extracting Training Data from Diffusion Models","description":"[https://twitter.com/eric\\_wallace\\_/status/1620449934863642624?s=46&amp;t=GVukPDI7944N8-waYE5qcw](https://twitter.com/eric_wallace_/status/1620449934863642624?s=46&amp;t=GVukPDI7944N8-waYE5qcw)\n\nExtracting training data from diffusion models is possible by following, more or less, these steps:\n\n* Compute CLIP embeddings for the images in a training dataset.\n* Perform an all-pairs comparison and mark the pairs with l2 distance smaller than some threshold as near duplicates\n* Use the prompts for training samples marked as near duplicates to generate N synthetic samples with the trained model\n* Compute the all-pairs  l2 distance between the embeddings of generated samples for a given training prompt. Build a graph where the nodes are generated samples and an edge exists if the l2 distance is less than some threshold. If the largest clique in the resulting graph is of size 10, then the training sample is considered to be memorized.\n* Visually inspect the results to determine if the samples considered to be memorized are similar to the training data samples.\n\nWith this method, the authors were able to find samples from Stable Diffusion and Imagen  corresponding to copyrighted training images.","link":"https://www.reddit.com/r/MachineLearning/comments/10r57pn/r_extracting_training_data_from_diffusion_models/","created":"2023-02-01","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":77}}
{"title":"[D] Commercial Use of a Model that has been trained using Human3.6M","description":" I wanted to use the [Learnable Trainangulation](https://github.com/karfly/learnable-triangulation-pytorch) model in a commercial project. The source code itself is under MIT licensing. However, the dataset they have used is [Human3.6M](http://vision.imar.ro/human3.6m/description.php), which states that the [license](http://vision.imar.ro/human3.6m/eula.php) is \"FREE OF CHARGE FOR ACADEMIC USE ONLY\".\n\nYet, recent court rulings (in the US) state that models can use copyrighted data during training, and the results are no longer bound by that copyright (e.g. Google Books). Does the same apply here?","link":"https://www.reddit.com/r/MachineLearning/comments/10rp7ze/d_commercial_use_of_a_model_that_has_been_trained/","created":"2023-02-02","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":4}}
{"title":"[P] Domestic Violence Dataset","description":"Hi, I am working on  project and for that I need a Twitter Domestic Violence Dataset. Basically I need a dataset with domestic violence tweets against woman.\n\nI have searched Kaggle and other websites but found no luck.\n\nPlus, I tried using Snscrape, but I need some phrases ideas related to domestic violence so I can get some tweets using that. I tried \"Domestic Violence\" , \"My husband tried to kill me\" and looking for more. Help is appreciated.","link":"https://www.reddit.com/r/MachineLearning/comments/10s0b47/p_domestic_violence_dataset/","created":"2023-02-02","tags":["reddit","ml","machinelearning"],"meta":{"num_comments":4}}
