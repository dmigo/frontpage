{"title":"[R] Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models","description":"","link":"https://www.reddit.com/gallery/11mlwty","created":"2023-03-09","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":7}}
{"title":"[P] I built a Spotify iOS tool that makes a 'Discover Daily' endless feed","description":"My friend and I got annoyed with trying to find new music on Spotify\n\nSo we built a program that takes a song and shortens in order to learn, predict and deliver  the \"best\" 10-60 seconds to you and your Spotify listening history\n\nYou can discover new music every day that's curated to your taste on snippets rather than full length songs\n\nWe added filters like genre/class/valence/key/BPM/chorus/bridge/5000+ unique hyper genres\n\nApp Store link: [https://apps.apple.com/us/app/smores-music-discovery/id1626768775](https://apps.apple.com/us/app/smores-music-discovery/id1626768775)\n\nTC Demo + Review: [https://techcrunch.com/2023/01/19/smores-is-a-music-discovery-app-with-a-tiktok-like-feed/](https://techcrunch.com/2023/01/19/smores-is-a-music-discovery-app-with-a-tiktok-like-feed/)\n\nWould love any feedback/criticisms/feature requests, thanks :)\n\n&amp;#x200B;\n\nhttps://preview.redd.it/nxpw3u96tlma1.png?width=443&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=7ae8ae3f9f0ea4e0d1fb74b5daebe8bd8f9c33be","link":"https://www.reddit.com/r/MachineLearning/comments/11mcm7k/p_i_built_a_spotify_ios_tool_that_makes_a/","created":"2023-03-08","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":6}}
{"title":"[P] Introducing the GitHub profile summarizer","description":"Hi guys, I built a website that summarizes a GitHub user using GPT.\n\nWhat is it?You type a GitHub profile URL, then it gives you a summary of the user.\n\nHow does it work?It finds the most important work by heuristics, then summarizes it using GPT.\n\nGive it a try and let me know what you think. :)\n\n[sample summary](https://preview.redd.it/c5o8tccc2jma1.png?width=1238&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9a5c6bc7ba5661020f75b22e3d76aa4441483ff4)\n\n[http://devmarizer.firebaseapp.com/](http://devmarizer.firebaseapp.com/)","link":"https://www.reddit.com/r/MachineLearning/comments/11ly4d9/p_introducing_the_github_profile_summarizer/","created":"2023-03-08","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":30}}
{"title":"[D] Bag of items to item model","description":"Hi,\n\nI have a dataset which consists of bags of items at some time T and other bags of items at some future time T\u2019. I want to build a NN to predict which items will be in the future based on the unordered bag of items in the present. \n\nWhat\u2018s the best way to preserve the information in the bag of unordered items? Averaging their embeddings and doing neural matrix factorization doesn\u2019t seem like the best approach.","link":"https://www.reddit.com/r/MachineLearning/comments/11mfc07/d_bag_of_items_to_item_model/","created":"2023-03-09","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[D] Machine/Deep learning jupyter notebooks for computer vision, NLP, and recommender systems","description":"Hi, I work at Intel as an academic outreach coordinator.  I'm sharing about Intel's open source OpenVINO toolkit for optimizing and deploy AI inference on CPUs, discrete and integrated GPUs, and other accelerators like Movidius VPUs and Intel FPGA.  The [github](https://github.com/openvinotoolkit/openvino_notebooks) has over 60 jupyter notebooks that can work on Intel PCs/laptop using Windows &amp; Linux, or on Macs on MacOS including M1 processors.\n\nTry out the stable diffusion Jupyter Notebook [\\#225](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/225-stable-diffusion-text-to-image),  or try out the vehicle recognition and detection Jupyter Notebook [\\#218](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/218-vehicle-detection-and-recognition)\n\nIts easy to install in 9 simple steps on Windows with pip install, 8 steps on MacOS, and 7 steps on Linux.","link":"https://www.reddit.com/r/MachineLearning/comments/11mbikv/d_machinedeep_learning_jupyter_notebooks_for/","created":"2023-03-08","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"\"[Discussion]\" Do you use synthetic data in your projects?","description":"&amp;#x200B;\n\nhttps://preview.redd.it/odfjzu3aqoma1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=916ec995aadd282f1f50b56d4c3d52f1acf5dc04\n\nHi all!  \nMy name is Vadim, I work in [OpenCV.ai](https://OpenCV.ai). We provide consulting services in the field of Computer Vision and AI. Now we work on a new tool for creating photorealistic synthetic data. \n\nWe eager to know what problems you most usually face while using it or why you don't use it. Your experience is extremely valuable for us. If you are open to discuss it, please write a private message to gleb.tuzov@opencv.ai or leave a comment. \n\nThank you!","link":"https://www.reddit.com/r/MachineLearning/comments/11mo71a/discussion_do_you_use_synthetic_data_in_your/","created":"2023-03-09","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":4}}
{"title":"[D] Text embedding model for financial documents","description":"I'm currently working on a project where I'm analyzing financial documents such as 10Ks and 10Qs. I'm looking for a pretrained text embedding model that has been fine-tuned on such documents to generate accurate embeddings. While there are models like FinBERT that are tuned for sentiment analysis, I'm interested in a model that can generate more accurate embeddings in general, without focusing solely on sentiment.\n\n&amp;#x200B;\n\nThanks!","link":"https://www.reddit.com/r/MachineLearning/comments/11m99js/d_text_embedding_model_for_financial_documents/","created":"2023-03-08","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":10}}
{"title":"[D] Has Anyone Used AutoML?","description":"Hi All,\n\nI just recently found out about AutoML and was wondering if anyone had used it before. If so, how was your experience? Are there any limitations I should be aware of, or is it fairly comprehensive?\n\nThanks ahead of time for your help!","link":"https://www.reddit.com/r/MachineLearning/comments/11m34uz/d_has_anyone_used_automl/","created":"2023-03-08","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":17}}
{"title":"[D] Feature Engineering","description":"I'm looking to do some feature engineering. Was wondering if y'all knew some platforms/libraries I could use that do it well?","link":"https://www.reddit.com/r/MachineLearning/comments/11mgijd/d_feature_engineering/","created":"2023-03-09","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[P] Feste, an open-source framework to optimize and parallelize NLP tasks","description":"Hi, just sharing a new open-source framework called Feste.\n\nDocumentation: https://feste.readthedocs.io\n\nGithub: https://github.com/perone/feste\n\nFeste is a tool for LLMs task composition that does automatic parallelization of backend API calls, tools, and *automatic batching* using graph optimization. Contributions are welcome!","link":"https://www.reddit.com/r/MachineLearning/comments/11m4l8y/p_feste_an_opensource_framework_to_optimize_and/","created":"2023-03-08","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[D] In AI, is bigger always better? Article in Nature; Bing summary and comment","description":"**In AI, is bigger always better?**\n\n**As generative AI models grow larger and more powerful, some scientists advocate for leaner, more energy-efficient systems.**\n\n[https://www.nature.com/articles/d41586-023-00641-w](https://www.nature.com/articles/d41586-023-00641-w)\n\nBing says:\n\n\\# In AI, is bigger always better?\n\nArtificial intelligence (AI) has made remarkable progress in recent years, thanks to the development of large language models (LLMs) that can generate coherent and fluent text on various topics. LLMs are trained on massive amounts of text data, such as books, news articles and social media posts, and learn to predict the next word given some previous words. They can also perform other tasks, such as answering questions, summarizing texts and translating languages.\n\nHowever, there is a debate among AI researchers about whether bigger LLMs are always better. Some argue that increasing the size of LLMs (in terms of parameters, data and computing power) will lead to more general and human-like intelligence. Others question this assumption and point out the limitations and challenges of scaling up LLMs.\n\nIn a recent article published in Nature , Anil Ananthaswamy explores this debate and examines the pros and cons of building bigger LLMs. He interviews several experts from academia and industry who share their views on the current state and future direction of AI research.\n\nOne of the main advantages of bigger LLMs is that they can achieve higher accuracy and performance on various natural language processing (NLP) benchmarks. For example, GPT-3 , one of the largest LLMs to date with 175 billion parameters, can generate convincing texts on almost any topic given a few words or sentences as input. It can also answer factual questions, write summaries and perform simple arithmetic.\n\nAnother benefit of bigger LLMs is that they can learn from more diverse and representative data sources, which can reduce bias and improve fairness. For instance, Google's LaMDA , an LLM with 137 billion parameters that can engage in open-ended conversations on any topic, was trained on text data from multiple languages and domains.\n\nHowever, bigger LLMs also come with significant drawbacks. One of them is the environmental cost of training them. According to a study by researchers at the University of Massachusetts Amherst , training a large LLM can emit as much carbon dioxide as five cars over their lifetimes. Moreover, training bigger LLMs requires more specialized hardware and infrastructure, which increases the entry barrier for smaller research groups and organizations.\n\nAnother challenge of bigger LLMs is that they are not very interpretable or explainable. It is hard to understand how they make decisions or what they learn from their data. This poses ethical risks when deploying them in real-world applications that affect human lives, such as health care or education.\n\nA third limitation of bigger LLMs is that they are not truly intelligent or generalizable. They lack common sense reasoning, causal understanding and world knowledge that humans have. They also struggle to adapt to new situations or tasks that are different from their training data.\n\nTherefore, some researchers suggest that scaling up LLMs alone is not enough to achieve artificial general intelligence (AGI), which is defined as AI that can perform any intellectual task that humans can do. They propose alternative approaches that combine symbolic reasoning with neural networks , incorporate multimodal inputs such as images or sounds , or leverage human feedback or interaction .\n\nIn conclusion, building bigger LLMs has both advantages and disadvantages for advancing AI research. While they offer impressive capabilities for generating natural language texts and performing various NLP tasks, they also face challenges in terms of environmental impact, transparency, and generality.\n\nTherefore, it is important to balance the trade-offs and explore other directionsfor developing more robust, responsible, and human-like AI systems.\n\n: [https://www.nature.com/articles/d41586-023-00641-w](https://www.nature.com/articles/d41586-023-00641-w)\n\n: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)\n\n: [https://ai.googleblog.com/2021/05/lamda-towards-conversational-agent-that.html](https://ai.googleblog.com/2021/05/lamda-towards-conversational-agent-that.html)\n\n: [https://arxiv.org/abs/1906.02243](https://arxiv.org/abs/1906.02243)\n\n: [https://arxiv.org/abs/2102.12730](https://arxiv.org/abs/2102.12730)\n\n: [https://arxiv.org/abs/2103.06332](https://arxiv.org/abs/2103.06332)\n\n: [https://arxiv.org/abs/2106.04501](https://arxiv.org/abs/2106.04501)","link":"https://www.reddit.com/r/MachineLearning/comments/11m57gr/d_in_ai_is_bigger_always_better_article_in_nature/","created":"2023-03-08","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":14}}
{"title":"[P] What Cloud Instance provider?","description":"Hi all, I am looking for a cloud provider that can offer 4xA100 or  8xA100 instances on demand with no wait time. I have seen Lambda and  Google Cloud but not sure which AWS or Azure instances are comparable.  The problem with Lambda cloud is that it seems like there is often a  wait on instances, too. TIA","link":"https://www.reddit.com/r/MachineLearning/comments/11mcm6q/p_what_cloud_instance_provider/","created":"2023-03-08","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[R] Analysis of 200+ ML competitions in 2022","description":"I run mlcontests.com, a website that aggregates ML competitions across Kaggle and other platforms.\n\nI've just finished a detailed analysis of **200+ competitions** in 2022, and what winners did (we found winning solutions for 67 competitions).\n\nSome highlights:\n\n* **Kaggle still dominant** with the most prize money, most competitions, and most entries per competition...\n* ... but there are **10+ other platforms** with interesting competitions and decent prize money, and dozens of single-competition sites\n* **Almost all competition winners used Python**, 1 used C++, 1 used R, 1 used Java\n* **96% (!) of Deep Learning solutions used PyTorch** (up from 77% last year)\n* **All winning NLP solutions we found used Transformers**\n* **Most computer vision solutions used CNNs**, though some used Transformer-based models\n* **Tabular data competitions were mostly won by GBDTs** (mostly LightGBM), though ensembles with PyTorch are common\n* **Some winners spent hundreds of dollars on cloud compute** for a single training run, **others managed to win just using Colab**'s free tier\n* Winners have largely converged on a common toolkit - PyData stack for the basics, PyTorch for deep learning, LightGBM/XGBoost/CatBoost for GBDTs, Optuna for hyperparam optimisation.\n* Half of competition winners are first-time winners; a third have won multiple comps before; half are solo winners. Some *serial winners* won 2-3 competitions just in 2022!\n\nWay more details as well as methodology here in the full report: [https://mlcontests.com/state-of-competitive-machine-learning-2022?ref=mlc\\_reddit](https://mlcontests.com/state-of-competitive-machine-learning-2022?ref=mlc_reddit)\n\n[Most common Python Packages used by winners](https://preview.redd.it/kwqmozh9lbma1.png?width=1600&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=1096de087592eb4cc2fbe85c8068617cb4f73d8f)\n\nWhen I published something similar here [last year](https://www.reddit.com/r/MachineLearning/comments/tdd889/news_analysis_of_83_ml_competitions_in_2021/), I got a lot of questions about tabular data, so I did a [deep dive](https://mlcontests.com/state-of-competitive-machine-learning-2022/#tabular-data?ref=mlc_reddit) into that this year.People also asked about [leaderboard shakeups](https://mlcontests.com/state-of-competitive-machine-learning-2022/#cross-validation?ref=mlc_reddit) and [compute cost trends](https://mlcontests.com/state-of-competitive-machine-learning-2022/#compute-and-hardware?ref=mlc_reddit), so those are included too. I'd love to hear your suggestions for next year.\n\nI managed to spend way more time on this analysis than last year thanks to the report sponsors (**G-Research**, a top quant firm, and **Genesis Cloud**, a renewable-energy cloud compute firm) - if you want to support this research, please check them out. I won't spam you with links here, there's more detail on them at the bottom of the report.","link":"https://www.reddit.com/r/MachineLearning/comments/11kzkla/r_analysis_of_200_ml_competitions_in_2022/","created":"2023-03-07","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":27}}
{"title":"[D] Does/Could it exist: LLMs as a means of specifying an Image Analysis Procedure","description":"Applied side here. I\u2019m wondering if we can do away with programming a dedicated algorithm for each discrete image manipulation.\n\nSay, in one batch of images, I want the average intensity of the red test tubes. In another, I want the width of the foreground object in pixels. In another I want the bottom right entry of the table in the pictured scan.\n\nI feel like I shouldn\u2019t have to build each of these. I feel like I should be able to just say what I want. I\u2019ve seen NLP or ImgProc NN models that individually could produce image descriptions or responses to querries that are way more nuanced.\n\nWhat\u2019s progress on large language models as a sort of natural language description - image analysis operation translator? What\u2019s the hold up? C\u2019mon would be super useful!","link":"https://www.reddit.com/r/MachineLearning/comments/11maelv/d_doescould_it_exist_llms_as_a_means_of/","created":"2023-03-08","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"Semantic Search: With Exclusions [P][D]","description":"I am making a semantic search engine in Python that takes a user input and returns the 5 most similar results from a list of sentences. \n\nThe list of sentences features a list of things not included in the category at the end of a sentence e.g.   \u201cThis category includes: lions, tigers. This category excludes: birds, bees\u201d  \n\nCurrently if I search \u201cbirds\u201d the above example would be returned as strong similarity due to the word matching with \u201ccategory excludes: birds\u201d \n\nDoes anyone know any way to prevent this?\nAny help appreciated!!","link":"https://www.reddit.com/r/MachineLearning/comments/11m4wim/semantic_search_with_exclusions_pd/","created":"2023-03-08","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":5}}
{"title":"[D] GPT 3.5 Turbo Issue - Any Suggestions?","description":"I coded a script using Python that uses the OpenAI API to generate articles. The way it works is by generating an article outline from a keyword. Then, it takes that outline and generates the text for each section, one by one. Instead of generating the whole article at once, I found that generating it in sections based on the different headings in the outline, gave me a higher-quality article at the end.\n\nAnyway, I had this working fine and was happy with it. However, since switching over to the gpt-3.5-turbo model, I've been having some issues. To me, it seems that when the code generates the text for each new section, it has \"forgotten\" what it previously generated. This means that each section starts with the same sentence. Overall, the article doesn't flow together correctly.\n\n**Here is an example of the output im getting, so you can see what I mean:**\n\n&gt;  \n*H2: How to Interpret Your Dream about Teeth Falling Out*  \n&gt;  \n&gt;*Hey there! So, you're curious about dreams where your teeth fall out? It's actually a pretty common dream that many people experience. But what does it mean?*  \n&gt;  \n&gt;*Well, dreams about teeth falling out can have different interpretations depending on the person and their personal experiences. Generally, though, it's believed to represent feelings of insecurity or vulnerability. Teeth are often associated with our appearance and how we present ourselves to others, so losing them in a dream can symbolize a fear of losing control or power.*  \n&gt;  \n&gt;*For example, I once had a dream where all my teeth fell out while I was giving a presentation at work. I felt embarrassed and exposed in front of my colleagues. Looking back, I realized that I was feeling insecure about my abilities at work and worried about being judged by others.*  \n&gt;  \n&gt;*But don't worry - not all dreams about teeth falling out are negative! Some people interpret them as a sign of growth or transformation. Losing old teeth can represent shedding old habits or beliefs to make way for new ones.*  \n&gt;  \n&gt;*So next time you have a dream about your teeth falling out, take some time to reflect on your current emotions and experiences. What could this dream be trying to tell you? And remember, it's just a dream - don't let it cause unnecessary stress or anxiety in your waking life.*  \n&gt;  \n&gt;*H2: How to Cope with a Dream about Teeth Falling Out*  \n&gt;  \n&gt;*Have you ever had a dream about your teeth falling out? It's a common dream that can leave you feeling anxious and confused. But what does it mean? And how can you cope with the emotions it brings up?*  \n&gt;  \n&gt;*First, let's delve into the science behind dreams. Dreams are a natural part of our sleep cycle and occur during the rapid eye movement (REM) stage. During this time, our brains are highly active and processing information from our daily lives.*  \n&gt;  \n&gt;*Research studies have shown that dreams can be influenced by our emotions, experiences, and even our physical state. For example, if you're feeling stressed or anxious, you may be more likely to have a dream about your teeth falling out.*  \n&gt;  \n&gt;*But what does this dream actually mean? There are many interpretations, but some psychologists believe that it could represent feelings of insecurity or powerlessness. Teeth are often associated with confidence and self-image, so losing them in a dream could symbolize a loss of control or fear of judgment from others.*  \n&gt;  \n&gt;*So how can you cope with these emotions? One approach is to try to identify any underlying stressors in your life and work on addressing them. This could involve talking to a therapist or practicing relaxation techniques like meditation or yoga.*  \n&gt;  \n&gt;*It's also important to remember that dreams are not always literal representations of reality. Just because you had a dream about your teeth falling out doesn't necessarily mean it will happen in real life.*  \n&gt;  \n&gt;*In conclusion, while dreams about teeth falling out can be unsettling, they are a normal part of the sleep cycle and can provide insight into our emotional state. By understanding the science behind dreams and working on coping strategies for any underlying stressors, we can learn to navigate these experiences with greater ease.*  \n\n\nNow, the easy solution would be to switch over to using text-davinci-003 as I had been originally. But, im curious to see the level of output I can get using the new gpt-3.5-turbo model (once I get it working correctly).\n\nDoes anyone have any idea of how I can make the AI \"remember\", using gpt-3.5-turbo model. Any tips on how to make my article flow together, instead of each section being written in a way that looks like it's the start of the article, would be much appreciated.\n\nBelow is the section of my code that generates each section of the article. If anyone has any ideas, then let me know, please. I coded this using ChatGPT with no prior coding knowledge, so forgive me if the code is messy.\n\n    # function to generate articles\n    def generate_article(outline, keyword):\n    article = []\n    headings = re.findall(r\"&lt;h[23]&gt;(.*?)&lt;/h[23]&gt;\", outline)\n    headings_list = []\n    for heading_text in headings:\n    # remove any irrelevant headings\n    if heading_text.lower().startswith(\"introduction\") or \\\n    heading_text.lower().startswith(\"conclusion\") or \\\n    len(heading_text.split()) &lt; 2:\n    continue\n    # remove any duplicate headings\n    if heading_text in headings_list:\n    continue\n    if not headings_list:\n    headings_list.append(heading_text)\n    continue\n    headings_list.append(heading_text)\n    memory = []\n    # Add some variation to the prompts for each section\n    prompt_list = [\n    {\"role\": \"user\", \"content\": f\"Take your readers on a step-by-step journey through '{heading_text}', using '{keyword}' as a framework. Use clear and concise language to explain each step. Vary your sentence structures to keep your readers engaged. Break up your text into short paragraphs. Do not repeat phrases. use varied language. Your tone should be friendly and casual, and you should avoid writing '{heading_text}' in the output. Use bullet points where appropriate to make your content more accessible.\"},\n    {\"role\": \"user\", \"content\": f\"Share your expertise on '{heading_text}' as it relates to '{keyword}'. Use personal stories and experiences to connect with your readers, and keep your writing lively and interesting by avoiding overused phrases. Ask rhetorical questions to help encourage the reader to think more deeply about your topic. Break up your text into short paragraphs to make your text easy to read. Do not repeat phrases. use varied language. Your tone should be friendly and casual. Avoid writing '{heading_text}' in the output.\"},\n    {\"role\": \"user\", \"content\": f\"Provide a fresh perspective on '{keyword}', focusing on '{heading_text}'. Use interesting and thought-provoking language to engage the reader. Do not repeat phrases, use varied language. Break up your text into short paragraphs. Your tone should be friendly and casual. Do not write '{heading_text}' in the output. Use bullet points where appropriate to make your content more accessible.\"},\n    ]\n    \n    # Randomly select one of the prompts for each section\n    messages = [random.choice(prompt_list)]\n    messages.append({\"role\": \"user\", \"content\": ''.join(memory)})\n    model = \"gpt-3.5-turbo\"\n    try:\n    body = openai.ChatCompletion.create(\n    model=model,\n    messages=messages,\n    max_tokens=500,\n    n=1,\n    stop=None,\n    temperature=0.3,\n    top_p=0.2,\n    frequency_penalty=0.5,\n    presence_penalty=0.5,\n    )\n    \n    # Format the generated text\n    message = body['choices'][0]['message']['content'].strip().replace('\\n* ', '\\n&lt;li&gt;')\n    message = message.replace('* ', '&lt;li&gt;')\n    message = message.replace('\\n\\n', '\\n')\n    message = message.replace('\\n', '&lt;/li&gt;\\n')\n    message = f\"&lt;ul&gt;\\n{message}&lt;/ul&gt;\" if '&lt;li&gt;' in message else f\"&lt;div&gt;&lt;p&gt;{message}&lt;/p&gt;&lt;/div&gt;\"\n    \n    # Split the message into paragraphs\n    paragraphs = message.split('\\n\\n')\n    \n    # Join paragraphs into groups of 3 paragraphs each\n    group_size = 3\n    grouped_paragraphs = [paragraphs[i:i+group_size] for i in range(0, len(paragraphs), group_size)]\n    \n    # Join each group of paragraphs into a single string\n    messages = []\n    for group in grouped_paragraphs:\n    message = '\\n\\n'.join(group)\n    # Remove the last character of the last paragraph if it is a full stop\n    if message[-1] == '.':\n    message = message.rstrip('.')\n    messages.append('&lt;p&gt;' + message.strip() + '&lt;/p&gt;\\n')\n    # Join all the messages into a single string\n    message = ''.join(messages)\n    \n    article.append(f\"&lt;h2&gt;{heading_text}&lt;/h2&gt;\\n{message}\")\n    print(f\"Success: Section '{heading_text}' has been written\")\n    \n    except Exception as e:\n    print(f\"Error generating article for '{heading_text}': {e}\")\n    return \"\"\n    \n    return \"\".join(article)","link":"https://www.reddit.com/r/MachineLearning/comments/11m2ayd/d_gpt_35_turbo_issue_any_suggestions/","created":"2023-03-08","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":3}}
{"title":"[D] The Emergent Abilities of Large Language Models","description":"Hey everyone!  \n\n\nLarge Language Models have been shown to gain new abilities (like translation and arithmetic) as they are scaled. Some of these abilities have been recently observed to be **emergent**, meaning that there is an apparent discontinuity in their appearance with scale.  \n\n\nThis article on [**the emergent abilities of large language models**](https://www.assemblyai.com/blog/emergent-abilities-of-large-language-models/) examines this phenomenon, providing necessary background and information on the concept of emergence as a whole.  \n\n\nI'm interested to hear what folks here think about this phenomenon and observation, especially regarding potential explanations as well as real-world implications. Let me know what you think!\n\n&amp;#x200B;\n\nhttps://preview.redd.it/hrh3zuztgcma1.png?width=1316&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=ad511d6d8875cf2765d5f80672d32e49abe28f55","link":"https://www.reddit.com/r/MachineLearning/comments/11l49y5/d_the_emergent_abilities_of_large_language_models/","created":"2023-03-07","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":18}}
{"title":"[R] Prismer: An Open Source Vision-Language Model with An Ensemble of Experts.","description":"Paper here -  [https://arxiv.org/abs/2303.02506](https://arxiv.org/abs/2303.02506)\n\nCode and Models -  [https://github.com/NVlabs/prismer](https://github.com/NVlabs/prismer)","link":"https://www.reddit.com/r/MachineLearning/comments/11lcspc/r_prismer_an_open_source_visionlanguage_model/","created":"2023-03-07","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":5}}
{"title":"[R] PaLM-E: An Embodied Multimodal Language Model - Google 2023 - Exhibits positve transfer learning!","description":"Paper: [https://arxiv.org/abs/2303.03378](https://arxiv.org/abs/2303.03378)\n\nBlog: [https://palm-e.github.io/](https://palm-e.github.io/)\n\nTwitter: [https://twitter.com/DannyDriess/status/1632904675124035585](https://twitter.com/DannyDriess/status/1632904675124035585)\n\nAbstract:\n\n&gt;Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, **exhibits positive transfer**: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. **Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.**       \n\nhttps://preview.redd.it/1z3zc3kte9ma1.jpg?width=1321&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=7ee212c74d468ba5a911e8f3bcfcad520cdd8733\n\nhttps://preview.redd.it/2qapt8kte9ma1.jpg?width=1180&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=30edaa9b99d8c1481b90721e14dae54764999e68\n\nhttps://preview.redd.it/thtfg6kte9ma1.jpg?width=725&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=c430e48e068eab0870e215b743d4a293d97177d2\n\nhttps://preview.redd.it/nffus6kte9ma1.jpg?width=712&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=8234af6ab133385ff96425312ef2d86b95e14d9e\n\nhttps://preview.redd.it/henjo3kte9ma1.jpg?width=710&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=1a36d074839a85a64ee9fc21c10c40234c75cadc","link":"https://www.reddit.com/r/MachineLearning/comments/11krgp4/r_palme_an_embodied_multimodal_language_model/","created":"2023-03-07","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":133}}
{"title":"[N] My first article on GANs, with full Python implementation and replicable results","description":"*I finally did it! Below is a brief intro. I usually don't post my articles here because you need to sign-up or they are in my books, which are not free. But this one is free, no sign-up required, so I decided to post it.*\n\nUsing case studies, I compare generative adversarial networks (GANs) with copulas to synthesize tabular data. I discuss back-end and front-end improvements to help GANs better replicate the correlation structure present in the real data. Likewise, I discuss methods to further improve copulas, including transforms, the use of separate copulas for each population segment, and parametric model-driven copulas compared to a data-driven parameter-free approach. I apply the techniques to real-life datasets, with full Python implementation. In the end, blending both methods leads to better results. Both methods eventually need an iterative gradient-descent technique to find an optimum in the parameter space. For GANs, I provide a detailed discussion of hyperparameters and fine-tuning options.\n\nI show examples where GANs are superior to copulas, and the other way around. My GAN implementation also leads to fully replicable results \u2014 a feature usually absent in other GAN systems. This is particularly important given the high dependency on the initial configuration determined by a seed parameter: it also allows you to find the best synthetic data using multiple runs of GAN in a replicable setting. In the process, I introduce a new matrix correlation distance to evaluate the quality of the synthetic data, taking values between 0 and 1 where 0 is best, and leverage the TableEvaluator library. I also discuss feature clustering to improve the technique, to detect groups of features independent from each other, and apply a different model to each of them. In a medical data example to predict the risk of cancer, I use random forests to classify the real data, and compare the performance with results obtained on the synthetic data.\n\nYou can download the article, access the Python code and check the table of contents, [from here](https://mltblog.com/3F9T3GW).","link":"https://www.reddit.com/r/MachineLearning/comments/11m9enj/n_my_first_article_on_gans_with_full_python/","created":"2023-03-08","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":3}}
{"title":"[R] Reinforcement Learning With C++.","description":"Hello everyone! I've been searching for a long time for a video tutorial that teaches reinforcement learning with C++. Unfortunately, all of the tutorials I've found so far have been just theoretical speeches that don't teach anything about practically implementing AI. I have high experience with C++ but very little experience with reinforcement learning, so I can't enforce AI. I just need to understand the basics of implementing it, maybe one example with explanations.  Not only C++ tho but maybe C# (I don't want python because I have no experience with python and most of the python tutorials have their fancy libraries, and I don't want to learn RL with some libraries but I want to implement the whole thing so I can understand it more deeply). Any recommendations would be greatly appreciated! Thank you!","link":"https://www.reddit.com/r/MachineLearning/comments/11m54z6/r_reinforcement_learning_with_c/","created":"2023-03-08","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":12}}
{"title":"[D] - Have neural networks that modulate their own loss functions been attempted? Is there any active research into this area?","description":" Is it possible to train a neural network that modulates its own loss function, as well as the hyperparameters of its training like momentum?\n\nWould backpropagation still be possible on such a model?","link":"https://www.reddit.com/r/MachineLearning/comments/11l66uj/d_have_neural_networks_that_modulate_their_own/","created":"2023-03-07","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":13}}
{"title":"[D] Tutorial: Run LLaMA on 8gb vram on windows (thanks to bitsandbytes 8bit quantization)","description":"facebookresearch/LLaMA 7b on windows 11 using less than 10GB vram, or LLaMA-13b on less than 24GB.\n\nEfforts are being made to  get the larger LLaMA 30b onto &lt;24GB vram with 4bit quantization by implementing the technique from the paper [GPTQ quantization](https://github.com/oobabooga/text-generation-webui/issues/177) \n\nSince bitsandbytes doesn't officially have windows binaries, the following trick using an older unofficially compiled cuda compatible bitsandbytes binary works for windows.\n\n1. install miniconda, start the miniconda console\n1. create a new dir, for example *C:\\textgen\\* and cd into it\n1. git clone *github.com/oobabooga/text-generation-webui*\n1. follow the installation instructions of text-generation-webui for conda, create the env with the name textgen\n1. Download not the original LLaMA weights, but the [HuggingFace converted](https://rentry.org/llama-tard-v2) weights. The torrent link is on top of this linked article.\n1. copy the llama-7b or -13b folder (or whatever size you want to run) into *C:\\textgen\\text-generation-webui\\models*. The folder should contain the config.json, generation_config.json, pytorch_model.bin, index.json, special_tokens_map.json, tokenizer.model, tokenizer_config.json as well as all the 33 pytorch_model-000xx-of-00033.bin files\n1. put [libbitsandbytes_cuda116.dll](https://github.com/DeXtmL/bitsandbytes-win-prebuilt) in *C:\\Users\\xxx\\miniconda3\\envs\\textgen\\lib\\site-packages\\bitsandbytes\\*\n1. edit *\\bitsandbytes\\cuda_setup\\main.py*:\n  \n  search for:\n  \n  *if not torch.cuda.is_available(): return 'libsbitsandbytes_cpu.so', None, None, None, None*\n  \n  replace with:\n  \n  *if torch.cuda.is_available(): return 'libbitsandbytes_cuda116.dll', None, None, None, None*\n\n  search for this twice:\n  \n  *self.lib = ct.cdll.LoadLibrary(binary_path)*\n  \n  replace with:\n  \n  *self.lib = ct.cdll.LoadLibrary(str(binary_path))*\n\n1. Start text-generation-webui by typing: *python server.py --model LLaMA-7B --load-in-8bit*","link":"https://www.reddit.com/r/MachineLearning/comments/11kwdu9/d_tutorial_run_llama_on_8gb_vram_on_windows/","created":"2023-03-07","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":17}}
{"title":"[D] Can someone explain the discrepancy between the findings of LLaMA and Chinchilla?","description":"Chinchilla states that the model size/dataset ratio should be 1 to 20 and they show it experimentally. LLaMA states their 7B model continued to improve even after 1T tokens. That's 1 to 142. Has anyone figured it out?","link":"https://www.reddit.com/r/MachineLearning/comments/11l3as6/d_can_someone_explain_the_discrepancy_between_the/","created":"2023-03-07","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":16}}
{"title":"[R] Created a Discord server with LLaMA 13B","description":"Installed LLaMA 13B (legitimate download) on a Dual RTX 3090 server and created a discord bot to interact with it.\n\nAs it's quite fast I'm opening it to the public, here is the discord invite. No registration/payments, etc. completely free.\n\nInstructions in comments as I cannot post an invite directly here.","link":"https://www.reddit.com/r/MachineLearning/comments/11kr20f/r_created_a_discord_server_with_llama_13b/","created":"2023-03-07","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":18}}
