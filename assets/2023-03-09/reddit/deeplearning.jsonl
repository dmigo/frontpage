{"title":"PyTorch Faster RCNN Library - Support for transformer detection models.","description":"[https://github.com/sovit-123/fasterrcnn-pytorch-training-pipeline](https://github.com/sovit-123/fasterrcnn-pytorch-training-pipeline)\n\nNow, the library supports Faster RCNN ViTDet and Faster RCNN MobileViT\\_XXS also.\n\nWould love to get feedback/contributions/suggestions.","link":"https://www.reddit.com/r/deeplearning/comments/11mhkpd/pytorch_faster_rcnn_library_support_for/","created":"2023-03-09","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":0}}
{"title":"Can feature engineering avoid overfitting?","description":" Can feature engineering avoid overfitting? If yes, are there any relevant papers that state this?","link":"https://www.reddit.com/r/deeplearning/comments/11mokqu/can_feature_engineering_avoid_overfitting/","created":"2023-03-09","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":8}}
{"title":"Build the BEST Data Science Resume with Quadruple Kaggle Grandmaster","description":"Here's an interview with Chris Deotte, Quadruple Kaggle Grandmaster at NVIDIA. \n\nIn this episode, Chris shares valuable insights on topics such as crafting a strong data science resume, achieving grandmaster status on Kaggle (even quadruple), working at NVIDIA, and how to approach current data science challenges. Learn more about Kaggle, the data science world, and NVIDIA through the fascinating story of Chris Deotte. (and win an RTX 4080 thanks to NVIDIA GTC collaboration!)\n\nListen to this week's episode on your favorite platform: \n\n[https://youtu.be/NjGnnG3evmE](https://youtu.be/NjGnnG3evmE)\n\n[https://podcasts.apple.com/us/podcast/whats-ai-by-louis-bouchard/id1675099708?i=1000603382690](https://podcasts.apple.com/us/podcast/whats-ai-by-louis-bouchard/id1675099708?i=1000603382690)\n\n[https://podcasters.spotify.com/pod/show/louis-bouchard/episodes/How-to-Build-a-strong-Data-Science-Resume--With-Chris-Deotte--Quadruple-Kaggle-Grandmaster-at-NVIDIA-and-win-an-RTX-4080-e203a7t/a-a9f73dt](https://podcasters.spotify.com/pod/show/louis-bouchard/episodes/How-to-Build-a-strong-Data-Science-Resume--With-Chris-Deotte--Quadruple-Kaggle-Grandmaster-at-NVIDIA-and-win-an-RTX-4080-e203a7t/a-a9f73dt)","link":"https://www.reddit.com/r/deeplearning/comments/11mhur7/build_the_best_data_science_resume_with_quadruple/","created":"2023-03-09","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":0}}
{"title":"Parameter values of diffusion models","description":"Hi everyone!\n\nI have a question about diffusion models from the paper \"Denoising Diffusion Probabilistic Models\" by Ho. et al. ([https://arxiv.org/pdf/2006.11239.pdf](https://arxiv.org/pdf/2006.11239.pdf)). They chose not to train \u03a3\\_\u03b8(x\\_t, t) but setting it equal to  \u03c3\\_t\\^2 I, and then they experiment with two different values on \u03c3\\_t\\^2, namely \u03b2\\_t and \\\\tilde{\u03b2}\\_t. The first choice is optimal for x\\_0 \u223c N(0, I), and the second is optimal for x\\_0 deterministically set to one point, why is that? Does anyone have a good explanation and/or derivation of that.","link":"https://www.reddit.com/r/deeplearning/comments/11m0kvf/parameter_values_of_diffusion_models/","created":"2023-03-08","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":1}}
{"title":"AI that plays a video game","description":"How would I make an AI that gathers resources in a game like ark? Thank you, I am new to this","link":"https://www.reddit.com/r/deeplearning/comments/11majq4/ai_that_plays_a_video_game/","created":"2023-03-08","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":3}}
{"title":"Weaviate Vector DB adds support for Product Quantization, Bitmap Filters, Filtered Hybrid Search, Tunable Consistency, and more in the v1.18 release.","description":"Ever since Chat-GPT has hit the masses, the interest in vector search has gone through the roof. Weaviate takes an end2end approach to vector search because it also stores the data object, and builds inverted indexes besides the vector indexes.  \n\n\nYesterday, version `v1.18.0` was released, with the following features that were in high demand by the community:\n\n# Product Quantization\n\nWeaviate v1.18 allows compressing vector embeddings using Product Quantization in combination with HNSW vector indexing (HNSW-PQ). This allows for a lower memory footprint while keeping low latency and high recall\n\n# Bitmap Filtering\n\nWeaviate's inverted index is now built natively on top of roaring bitmaps. This allows for very fast filtered vector search even at the 100M or billion scale. In some extreme cases, search latencies went down from 5s to 5ms.\n\n# Filtered Hybrid Search\n\nWeaviate v1.17 added support for Hybrid (BM25 sparse + Vector Dense) search. However, it did not (yet) allow for setting filters on Hybrid Search queries. This is now possible with v1.18\n\n# BM25 WAND Scoring\n\nWeak-AND (\"WAND\") is a BM25 scoring algorithm that avoids scoring documents that cannot reach a high enough score to be contained in the result set. This speeds up BM25 \u2013\u00a0and in turn \u2013 hybrid search\n\n# Tunable Consistency and Automatic Repairs\n\nA previous Weaviate release added support for High-Availability through Replication. However, the desired level of consistency when reading and writing was set by Weaviate. Now, the user can set these settings according to their preferences. In addition, if Weaviate detects an inconsistency (e.g. after a temporary node failure) it can now be repaired automatically when reading the \"corrupt\" object.\n\n# Cursor API\n\nIn previous Weaviate releases, it was impossible to export all objects from Weaviate because of the increasing cost of each page on pagination. The new cursor API provides a constant-cost way to extract all objects (and their vector embeddings) from Weaviate.\n\n# Azure Backup Module\n\nIn addition to Google Cloud Storage, and Amazon S3, Weaviate now supports Azure Blob storage for seamless backups and restores.\n\n\\---\n\nMore information:\n\n* [Release blog post](https://weaviate.io/blog/weaviate-1-18-release)\n* [Release on GitHub](https://github.com/weaviate/weaviate/releases/tag/v1.18.0)\n\nDisclaimer: I am a co-founder of Weaviate.","link":"https://www.reddit.com/r/deeplearning/comments/11lsal6/weaviate_vector_db_adds_support_for_product/","created":"2023-03-08","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":2}}
{"title":"2048 Q-Learning","description":"Hey, I have a Raspberry pi 4 8gb of RAM and I don\u2019t use it. So I found an idea, it\u2019s to make a 2048 in python with Q-Learning.\nBut I don\u2019t know how to make it.","link":"https://www.reddit.com/r/deeplearning/comments/11mc34a/2048_qlearning/","created":"2023-03-08","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":0}}
{"title":"Powering Anomaly Detection for Industry 4.0: Comet + Anomalib","description":"Smart Manufacturing\u00a0is here to stay. And thankfully, building and tracking production-grade anomaly detection models for Industry 4.0 has never been easier!\n\nIn this article, I explore a new integration between\u00a0Comet\u00a0and Anomalib: an end-to-end solution that includes cutting-edge\u00a0algorithms, visualizations, optimization and inference deployment code with Intel\u2019s OpenVINO toolkit.\n\n[https://medium.com/p/16afd23bd974](https://medium.com/p/16afd23bd974)","link":"https://www.reddit.com/r/deeplearning/comments/11m1mg6/powering_anomaly_detection_for_industry_40_comet/","created":"2023-03-08","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":0}}
{"title":"How can i improve my model in order to get more accuray and less loss?? Thanks","description":"target\\_size=c(200,200)\n\nbatch\\_size=100\n\ntrain\\_data\\_gen=image\\_data\\_generator(rescale = 1./255,horizontal\\_flip = T,vertical\\_flip = T,rotation\\_range = 45,zoom\\_range = 0.25,validation\\_split = 0.2)\n\n&amp;#x200B;\n\n\\# train\n\ntrain\\_image\\_array\\_gen= flow\\_images\\_from\\_directory(directory = \"imagenes/TRAIN/\",shuffle=T,target\\_size =target\\_size,color\\_mode = \"grayscale\", batch\\_size = batch\\_size ,subset = \"training\",  generator = train\\_data\\_gen)\n\n\\# validation\n\nval\\_image\\_array\\_gen= flow\\_images\\_from\\_directory(directory = \"imagenes/TRAIN/\",target\\_size = target\\_size,shuffle = T, color\\_mode = \"grayscale\", batch\\_size = batch\\_size,subset = \"validation\", generator = train\\_data\\_gen)\n\n&amp;#x200B;\n\n&amp;#x200B;\n\ninitializer=initializer\\_random\\_normal(seed = 100)\n\nmodel=keras\\_model\\_sequential(name='simple\\_model')%&gt;%\n\nlayer\\_conv\\_2d(filters = 16,\n\nkernel\\_size = c(3,3),\n\npadding = 'same',\n\nactivation = 'relu',\n\nkernel\\_initializer = initializer,\n\nbias\\_initializer = initializer,\n\ninput\\_shape = c(tama\u00f1o\\_imagen,1)\n\n)%&gt;%\n\nlayer\\_max\\_pooling\\_2d(pool\\_size = c(2,2))%&gt;%\n\nlayer\\_flatten()%&gt;%\n\nlayer\\_dense(units = 16,\n\nactivation = 'relu',\n\nkernel\\_initializer = initializer,\n\nbias\\_initializer = initializer)%&gt;%\n\nlayer\\_dense(units = output\\_n,\n\nactivation = 'sigmoid',\n\nname = 'Output',\n\nkernel\\_initializer = initializer,\n\nbias\\_initializer = initializer)\n\nmodel\n\n&amp;#x200B;\n\n&amp;#x200B;\n\nmodel %&gt;%\n\ncompile(\n\nloss='categorical\\_crossentropy',\n\noptimizer = optimizer\\_adam(learning\\_rate=0.0001),\n\nmetrics = 'accuracy'\n\n)\n\n&amp;#x200B;\n\nhistory=model %&gt;%\n\nfit(train\\_image\\_array\\_gen,steps\\_per\\_epoch=as.integer(train\\_samples/batch\\_size),epochs=40,validation\\_data=val\\_image\\_array\\_gen,validation\\_steps=as.integer(valid\\_samples/batch\\_size)\n\n)\n\n\\*plot(history)----&gt; RESULTS\\*\n\nhttps://preview.redd.it/5ekgkqqk8kma1.png?width=663&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d0b7a81091f377f823f1b52a7bb0ec713c28ca4f\n\n&amp;#x200B;\n\nval\\_data=data.frame(file\\_name=paste0('imagenes/TRAIN/',val\\_image\\_array\\_gen$filenames)) %&gt;%\n\nmutate(class=str\\_extract(file\\_name,'Control|PD'))\n\n&amp;#x200B;\n\nimage\\_prep=function(x){\n\narrays=lapply(x, function(path){\n\nimg=image\\_load(path,target\\_size = c(200,200),grayscale = T)\n\nx=image\\_to\\_array(img)\n\nx=array\\_reshape(x,c(1,dim(x)))\n\nx=x/255 #normalizar los pixeles de la imagen\n\n})\n\n[do.call](https://do.call)(abind::abind,c(arrays,list(along=1)))\n\n}\n\n&amp;#x200B;\n\ntest\\_x=image\\_prep(val\\_data$file\\_name)\n\ndim(test\\_x)\n\n&amp;#x200B;\n\npred\\_test=model %&gt;%\n\npredict(test\\_x)%&gt;%\n\nk\\_argmax()\n\nhead(pred\\_test,10)\n\n&amp;#x200B;\n\ndecode=function(x){\n\ncase\\_when(x==0\\~'Control',\n\nx==1\\~'PD'   )\n\n}\n\npred\\_test=sapply(pred\\_test,decode)\n\nhead(pred\\_test,10)\n\n\\*confusionMatrix(table(as.factor(pred\\_test),as.factor(val\\_data$class)))-----&gt;RESULTS\\*\n\n&amp;#x200B;\n\nhttps://preview.redd.it/fo3d3sdx8kma1.png?width=642&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=c6a76cc9e5dd5ce2aee904e95544286b466214e0\n\n\\*history$metrics$accuracy\\[40\\]----&gt;RESULTS\\*\n\n&amp;#x200B;\n\nhttps://preview.redd.it/21t0vyby8kma1.png?width=254&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5200c07f5af184ec34e3bde5cef828487758320c","link":"https://www.reddit.com/r/deeplearning/comments/11m49hd/how_can_i_improve_my_model_in_order_to_get_more/","created":"2023-03-08","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":6}}
{"title":"FEATURES AND CAPABILITIES OF THE TESLA BOT","description":"The Tesla Bot is equipped with a number of advanced features and capabilities, including:\n\n**Advanced Sensors:** This is a Tesla boat, many sensors have been used including cameras, lidar, and ultrasonic sensors, through these sensors, this Tesla board collects information and reacts based on that information.\n\n**Autopilot:** In this robot, the concept of the Same Tesla self-driving car has been used, it also works on the basis of the information received by the sensor, and it will perform as per the command it will get.\n\n**Humanoid Design:** The biggest factor that has made it popular is the humanoid design, which is required to perform any work like humans, such as a high degree of dexterity and agility.\n\nBecause of this, a person can catch anything very easily, in the same way, how will the Human Knight Tesla Boat work?\n\n**Collaboration with Humans:** It is true that robots can be very useful in assisting humans with repetitive tasks, allowing humans to focus on more complex and higher-level activities. The Tesla robot, which is still in development, is intended to do just that by performing tasks such as fetching groceries, carrying luggage, and performing simple repetitive tasks. \n\nHowever, it is important to note that robots are not perfect and can encounter errors or malfunctions. Additionally, while robots can help reduce the burden of repetitive tasks, they cannot replace human creativity, problem-solving skills, and other uniquely human abilities.","link":"https://www.reddit.com/r/deeplearning/comments/11loo7p/features_and_capabilities_of_the_tesla_bot/","created":"2023-03-08","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":2}}
{"title":"Pytorch training speed slow?","description":"Being a new Pytorch user, I was curious to train the same model with Pytorch that I trained with Tensorflow a few months ago. However, in PyTorch, the training doesn't even seem to pass a single epoch and takes too long.\n\nThe same model, and same dataset, on Tensorflow, took 500 s on avg per epoch, but in PyTorch it is around 3600 s, and the colab memory usage is skyrocketing, thus crashing the server.\n\nIs there something I'm doing wrong? I made the model on GPU and also the data.\n\n**Model used:** EfficientNetB0 with completely unfrozen weights.\n\n**Dataset:** Food101\n\n^(Again, I used the same model and data in TensorFlow too!!)\n\n**Model Code:**\n\n    effnetb1_weights = torchvision.models.EfficientNet_B1_Weights.DEFAULT\n    effnetb1_transforms = effnetb1_weights.transforms()\n    effnetb1 = torchvision.models.efficientnet_b1(weights=effnetb1_weights).to(device) effnetb1.classifier = nn.Sequential(nn.Dropout(p=0.3, inplace=True),             \n                                    nn.Linear(in_features = 1280, out_features=len(class_names))\n    ).to(device)\n\n**Dataset Code:**\n\n    train_data = torchvision.datasets.Food101(root='food101', download=True, \n                                              split='train',         transform=effnetb1_transforms)\n    test_data = torchvision.datasets.Food101(root='food101', download=True, \n                                             split='test', transform=effnetb1_transforms)\n    \n    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False)\n\n**Training Loops:**\n\n    def train_step(model: nn.Module,\n                   dataloader: torch.utils.data.DataLoader,\n                   loss_fn: nn.Module, optimizer: torch.optim.Optimizer,\n                   device: torch.device='cuda' if torch.cuda.is_available() else 'cpu'):\n    \n      train_loss = []\n      train_acc = []\n      \n      model.train()\n      with tqdm(dataloader, unit='batch', ascii=' =', position=0, bar_format='{n_fmt}/{total_fmt} [{bar:30}] - {elapsed_s:.0f}s {rate_fmt} {desc} ') as tbatch:\n        for X, y in tbatch:\n          X, y = X.to(device), y.to(device)\n    \n          preds = model(X)\n    \n          loss = loss_fn(preds, y)\n          acc = accuracy_score(y.cpu(), torch.argmax(torch.softmax(preds, dim=1), dim=1).cpu())\n          tbatch.set_description_str(f\"loss: {torch.mean(torch.Tensor(train_loss)).item():.4f} - accuracy: {torch.mean(torch.Tensor(train_acc)).item():.4f}\")\n    \n          train_loss.append(loss)\n          train_acc.append(acc)\n    \n          optimizer.zero_grad()\n          loss.backward()\n          optimizer.step()\n      return torch.mean(torch.Tensor(train_loss)).item(), torch.mean(torch.Tensor(train_acc)).item()\n\n**Full Notebook:** [https://colab.research.google.com/drive/1VWNMpF4DxOUOqCbPKhvtKthnUQ4Ni7r\\_#scrollTo=qLSlm0b8YO-l](https://colab.research.google.com/drive/1VWNMpF4DxOUOqCbPKhvtKthnUQ4Ni7r_#scrollTo=qLSlm0b8YO-l)","link":"https://www.reddit.com/r/deeplearning/comments/11kyvdm/pytorch_training_speed_slow/","created":"2023-03-07","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":1}}
{"title":"newby here. looking for help on a MLP for speech recognition. any tips or pointers would be appreciated","description":"So I took a class of introduction in AI and machine learning and I have to implement a MLP for speech recognition without any Library that implement the MLP for me. (I.e. I can use numpy) \n\nAny help would be useful!!","link":"https://www.reddit.com/r/deeplearning/comments/11l4mid/newby_here_looking_for_help_on_a_mlp_for_speech/","created":"2023-03-07","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":6}}
{"title":"We tracked mentions of OpenAI, Bing, and Bard across social media to find out who's the most talked about in Silicon Valley","description":"[Posts about OpenAI, Bing, and Bard in the San Francisco Bay Area and Silicon Valley](https://preview.redd.it/tliq31mjecma1.png?width=1286&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b85bd810e79aabae50c0d69f3ef6c47052cc7b5e)\n\nHave you been following the news on the conversational AI race? We used social media data and [geolocation models](https://github.com/1712n/yachay-public/tree/master/conf_geotagging_model) to find posts about OpenAI, Bing, and Bard in the Silicon Valley and San Francisco Bay Area for the last two weeks to see which one received the most mentions.\n\nFirst, we filtered social media data with the keywords \"openai,\" \"bing,\" \"bard,\" and then we predicted coordinates for the social media posts by using our text-based geolocation models. After selecting texts which received a confidence score higher than 0.8, we plotted their coordinates as company logos on a leaflet map using Python and the folium library, restricting the map to the bounding box of the San Francisco Bay Area and Silicon Valley.\n\nWe analyzed over 300 social media posts and found that roughly 54.5% of the time, OpenAI was the most talked about. Bing made second place with around 27.2%, and then Bard came in last with 18.3%.\n\nYou can check out the full map [here](https://1712n.github.io/yachay-public/maps/chatbots/).\n\nOpenAI may be winning the AI race at the moment, but it's not the end yet. Let us know what other AI projects you're following, and we'll check them out.","link":"https://www.reddit.com/r/deeplearning/comments/11l3ofp/we_tracked_mentions_of_openai_bing_and_bard/","created":"2023-03-07","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":1}}
{"title":"How's this workflow for fine tuning SD + Dreambooth + ControlNet with API access? (like the below apps)","description":"I've seen many people that had the idea similar to [deepagency.com](https://deepagency.com/) or [PhotoAI.io](https://photoai.io/) but don't know the workflow. I saw the creators said they use dreambooth with controlnet on [replicate.com](https://replicate.com/)\n\nSo is this the right workflow?\n\n1. Either find a space on hugging face for dreambooth training, or go on google colab or [replicate.com](https://replicate.com/), update your images, play around with the numbers to get what you want in the results\n2. Download the ckpt file, update the file on [replicate.com](https://replicate.com/) and access it via APIs. or train on [replicate.com](https://replicate.com/)? then\n3. Then tweak it further with controlnet\n\nAre these steps correct? if not what do you suggest?\n\nthanks a bunch","link":"https://www.reddit.com/r/deeplearning/comments/11kswr1/hows_this_workflow_for_fine_tuning_sd_dreambooth/","created":"2023-03-07","tags":["ml","reddit","deeplearning"],"meta":{"num_comments":0}}
