{"title":"[Beginner] Any tips/resources on where should I start?","description":"I would like to create a simple chatbot where user would ask a school-related question (e.g., when is the enrollment) and the response will be based on the answer column on the dataset.\n\nWhat I had in mind is to use Question Answering but without need to input the context.  The problem is most of the tutorials I found (HuggingFace) uses with the *'with context'* approach and my Dataset consist only question and answer columns.\n\nAny help or tutorials would greatly help.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11mims7/beginner_any_tipsresources_on_where_should_i_start/","created":"2023-03-09","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":1}}
{"title":"Semantic Search: With Exclusions","description":"I am making a semantic search engine in Python that takes a user input and returns the 5 most similar results from a list of sentences. \n\nThe list of sentences features a list of things not included in the category at the end of a sentence e.g.   \u201cThis category includes: lions, tigers. This category excludes: birds, bees\u201d  \n\nCurrently if I search \u201cbirds\u201d the above example would be returned as strong similarity due to the word matching with \u201ccategory excludes: birds\u201d \n\nDoes anyone know any way to prevent this?\nAny help appreciated!!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11m4niv/semantic_search_with_exclusions/","created":"2023-03-08","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":9}}
{"title":"Encoder-decoder architecture for POS tagging","description":"I understand following about encoder and decoder:\n\n&gt; An encoder is a network that takes the input, and output a feature map/vector/tensor. These feature vector hold the information, the features, that represents the input. The decoder is again a network that takes the feature vector from the encoder, and gives the best closest match to the actual input or intended output.\n\nI want to implement POS tagging with encoder and decoder. I can guess that we can use \"encoder-only\" model to do POS tagging. Can we use \"encoder-decoder\" architecture for POS tagging task? If yes, then how should I design it. Most importantly I am not able to get what input will the decoder get from the encoder.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11m5rzs/encoderdecoder_architecture_for_pos_tagging/","created":"2023-03-08","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":3}}
{"title":"Testing Viterbi Algorithm for Hidden markov model pos tagger","description":"I am implementing the HMM model pos tagger using viterbi algorithm on the brown dataset from nltk. I have separated the data into train and test datasets, now for train dataset, I have calculated the emission and transition probability matrix. I have a few questions though.\n\n1. To calculate the accuracy, do we count the no. of correct tags on words or the no. of correctly tagged sentences? (my guess is it should be words)\n2. For testing data, I have some words which are not in the emission probability matrix, and hence for those sentences viterbi algorithm gives me an error. how do i handle this?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11luju0/testing_viterbi_algorithm_for_hidden_markov_model/","created":"2023-03-08","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":4}}
{"title":"How to get a Phd in NLP for protein/gene design ?","description":"I have a background in Biotechnology and am currently doing a MS in Bioinformatics. My research consists on natural language models like BERT and protein design I'm also working on data/text mining projects with Biomedical data.  I want to do a PHD  with a focus on NLP but I'm worried if I have enough knowhow to apply for them. Any suggestions how I should approach this?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11m5je0/how_to_get_a_phd_in_nlp_for_proteingene_design/","created":"2023-03-08","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":1}}
{"title":"Question about density plots for dimensionality reduced embeddings","description":"I have long form documents that each talk about a variety of topics (10+). The documents are split into paragraph, which is the unit (sub-) topics are talked about. For each paragraph an embedding is created via OpenAI (text-embedding-ada-002). Since the embeddings contain 1536 dimensions, I use UMAP to reduce it to two. \n\nWhat I would like to do is then use bivatiate kde plots via [seaborn](https://seaborn.pydata.org/tutorial/distributions.html) to compare the focus of the documents (each representing an organization) showing differences and commonalities. I don\u2018t have a strong background in mathematics but this part of the [documentation](https://umap-learn.readthedocs.io/en/latest/clustering.html) threw me a little of. While I am not directly clustering, the underlying idea seems similar enough to warrant caution. \n\nDoes anybody know if my idea (umap reduced embedding-&gt; kde plot) reasonably sound or have any pointers to fintune the approach?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11m3jgo/question_about_density_plots_for_dimensionality/","created":"2023-03-08","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Worth learning Python just for NLP if I have good grasp of R?","description":"Wanted to survey what people thought. I have a good amount of experience and feel comfortable with R having used it on various data analytic projects. \n\nApproaching my first NLP project. Do you all feel it is worth learning Python to do this specifically? I know there may be general arguments for learning Python (flexibility, etc.) but was wondering how different it is for NLP application.\n\nThanks!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11l6zu1/worth_learning_python_just_for_nlp_if_i_have_good/","created":"2023-03-07","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":13}}
{"title":"Options for BERT in Python vs. Pyspark","description":"Hi all,  I'm working on a project to improve the **selection of web pages where ads will be placed**. (ex: If the ad is for supplements for women place it on a page about... women's health and wellness. Pretty simple.)\n\nPreviously, this has been done using very basic keyword matching and/or the site's membership in a category that was pre-chosen by the customer. (External service provides categorization of site, customer chooses keywords/category they want to advertise on.) Very basic, context and word sense not considered.\n\nNow I'm trying to bring the system up to a modern approach. \n\n# My approach so far has been the following:\n\n* **Make Corpus Embeddings**\n   * Get the text of a bunch of the pages where an ad can be shown and **do TF-IDF** to find most relevant words.\n   * **Get embeddings** of all the page's words **from bert-base-uncased**\n   * Pull out **just those that are top 10 TD-IDF** and **average** to create a general embedding for that page (Two notes about this: This is actually done a little more efficiently than this but I'm trying to make it clear conceptually that I'm getting the embedding for the word in its original context. I'm adding the extra TF-IDF step because it seems to keep size/computation low and not sacrifice quality.)\n* **Make Example Site Embedding**\n   * **Get an example site from the customer** that they consider ideal to advertise on. Do the above on this site's text also.\n* **Find Pages Similar to Example**\n   * Do **cosine similarity** across the pages in the corpus to **find near neighbors to the example site** and advertise on those highest ranking pages where possible.\n\n# How to get this into pySpark?\n\nSo, this has all been great so far. The results look like we want them to look. But it's just been done on 70k rows of corpus sites, totally in Python. We're going to need to deal with a corpus of \\~10mil sites. That's not going to work in Python. There is a Hadoop cluster available that is accessible by PySpark, though.  \n\nSo we have **options**.\n\n* Put everything in a **UDF**, run same BERT package in UDF (not so efficient and coincidentally also not working at all due to a platform issue I won't explain here but basically **this won't work** so it's ruled out)\n* Switch the **TF-IDF to SparkML**, do the BERT **embeddings in SparkNLP** (this is how we're going about this now but it's still slow, not sure the cause yet)\n* **Forget the TF-IDF** efficiency step and just do BERT embeddings in SparkNLP, go eat cake and watch television!\n* **SOMETHING ELSE MUCH BETTER**\n\n# Can I do this better? How?\n\nThat brings me to my question. What would you do to approach this problem better? What's best for **storage efficiency, computational efficiency**? Would you go about it a totally different way entirely? How can I improve this approach?\n\nThanks for your advice!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11l7vuu/options_for_bert_in_python_vs_pyspark/","created":"2023-03-07","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"[HELP]","description":" I'm trying to build a pos tagger model for my native language and I found out that I need an annotated corpus for my model. my question is should I label each word with its POS as two columns one for the word and the second for the tag or what should I do?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11lgqzf/help/","created":"2023-03-08","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Swahili Translation Tool","description":"Hello all! I am a teacher and I am looking for an app or a website or something you all suggest to translate my assignments and letters home into Swahili and Arabic. If you have anything you would like to suggest, I would really appreciate it! Thank you all.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11l5mlt/swahili_translation_tool/","created":"2023-03-07","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":1}}
{"title":"I made an OpenAI-compatible streaming API (and playground) for your \ud83e\udd17 Transformers-based text generation models","description":"GitHub: [https://github.com/hyperonym/basaran](https://github.com/hyperonym/basaran)\n\nBasaran is an open-source alternative to the [OpenAI text completion API](https://platform.openai.com/docs/api-reference/completions/create). It provides a compatible streaming API for your [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)\\-based [text generation models](https://huggingface.co/models?pipeline_tag=text-generation).\n\nThe open source community will eventually witness the [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release) moment for large language models (LLMs), and Basaran is committed to becoming the [Stable Diffusion web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) for LLMs. Basaran allows you to replace OpenAI's service with the latest open-source model to power your application [without modifying a single line of code](https://github.com/hyperonym/basaran/blob/master/README.md#openai-client-library).","link":"https://www.reddit.com/r/LanguageTechnology/comments/11kjzlh/i_made_an_openaicompatible_streaming_api_and/","created":"2023-03-07","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"We tracked mentions of OpenAI, Bing, and Bard across social media to find out who's the most talked about in Silicon Valley","description":"Have you been following the news on the conversational AI race? We used social media data and [geolocation models](https://github.com/1712n/yachay-public/tree/master/conf_geotagging_model) to find posts about OpenAI, Bing, and Bard in the Silicon Valley and San Francisco Bay Area for the last two weeks to see which one received the most mentions.\n\nFirst, we filtered social media data with the keywords \"openai,\" \"bing,\" \"bard,\" and then we predicted coordinates for the social media posts by using our text-based geolocation models. After selecting texts which received a confidence score higher than 0.8, we plotted their coordinates as company logos on a leaflet [map](https://1712n.github.io/yachay-public/maps/chatbots/) using Python and the folium library, restricting the map to the bounding box of the San Francisco Bay Area and Silicon Valley.\n\nWe analyzed over 300 social media posts and found that roughly 54.5% of the time, OpenAI was the most talked about. Bing made second place with around 27.2%, and then Bard came in last with 18.3%.\n\nOpenAI may be winning the AI race at the moment, but it's not the end yet. Let us know what other AI projects you're following, and we'll check them out.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11l3k5x/we_tracked_mentions_of_openai_bing_and_bard/","created":"2023-03-07","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Simplest Way to Run Jupyter Notebooks on GPUs?","description":"Suggestions for a simple/clear service to run my notebooks on GPUs? I'm comfortable in Jupyter but not command lines, Ubuntu, etc. I just want to be able to run the notebooks that I can't get to execute on my laptop CPU. I'm reluctant to use Google Colab because it's not clear to me that I retain ownership of my data/code/models, and I've tried paperspace and in theory it should be great but I get so many errors/kernel restarts, etc. that it's unusable.\n\nAny suggestions would be welcome.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11kc24y/simplest_way_to_run_jupyter_notebooks_on_gpus/","created":"2023-03-06","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":10}}
{"title":"Recognizing new tokens with Sentence Transformers","description":"I'm currently using the sentence-transformers library to perform semantic parsing on a dataset. The problem is that this data contains a ton of industry jargon and acronyms, and I am not confident in a pretrained transformer's ability to accurately capture those types of tokens.\n\n&amp;#x200B;\n\nIs there a recommended approach for tuning the model to capture these new tokens? Will finetuning the later layers of a pretrained model be sufficient to capture the context around these tokens, or would a better approach be to unlock the initial layer of the transformer to create word embeddings to be used downstream for the sentence embeddings?\n\n&amp;#x200B;\n\nThank you!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11k6svi/recognizing_new_tokens_with_sentence_transformers/","created":"2023-03-06","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":5}}
{"title":"Webhook 401 Error","description":"Can anyone help me please ? I\u2019m doing an assignment for school and we are learning to add web hook fulfillments to dialog-flow. Every time I try to run the agent I always get the 401 Authentication Error. The url doesn\u2019t have typos and there isn\u2019t a password. Can someone tell me what I am doing wrong ?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11kksmx/webhook_401_error/","created":"2023-03-07","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Cicero by Meta AI","description":" \n\nHi everyone,\n\nHere is an attempt to summarise Cicero by Meta AI. It was a difficult read so hope people appreciates this.\n\n**What is Cicero?**\n\nfirst time an AI exceeds at a board game that requires communicating (natural language) with other humans UNDETECTED.\n\nCicero - the AI agent that speaks your language, plans with you, and negotiates like a pro. With human-level performance in Diplomacy (a board game).\n\n**How does it work?**\n\nUsing LLMs and other bits\u2026\n\nif anyone is interested please feel free to check it out.\n\n[https://www.youtube.com/watch?v=Gj5T9m-ZzNA](https://www.youtube.com/watch?v=Gj5T9m-ZzNA)","link":"https://www.reddit.com/r/LanguageTechnology/comments/11kcpic/cicero_by_meta_ai/","created":"2023-03-06","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":0}}
{"title":"Research","description":"Hi ... In your opinion, what are the best research papers in NLP that have come out in the past year?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11jzvd2/research/","created":"2023-03-06","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":4}}
{"title":"Is there a correlation between the scale of the model and the quality of long text generation?","description":"Recently, I conducted a few fine-tuning experiments on a multitask instruction dataset on 1b7 LLM, primarily related to open-ended long story generation. However, it was observed that after generating nearly 300 tokens, the quality of the generated text started to decline, becoming less fluent. \n\nI am curious to know if there exists a relationship between the scale of the model and the quality of long text generation. For example, is it possible to achieve fluent long text after generating a certain number of tokens for models like 1b7?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11k1wzp/is_there_a_correlation_between_the_scale_of_the/","created":"2023-03-06","tags":["languagetechnology","reddit","ml"],"meta":{"num_comments":1}}
