{"title":"[D] Simple Questions Thread","description":"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!","link":"https://www.reddit.com/r/MachineLearning/comments/122oxap/d_simple_questions_thread/","created":"2023-03-26","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":13}}
{"title":"[D] GPT4 and coding problems","description":"[https://medium.com/@enryu9000/gpt4-and-coding-problems-8fbf04fa8134](https://medium.com/@enryu9000/gpt4-and-coding-problems-8fbf04fa8134)  \n\nApparently it cannot solve coding problems which require any amount of thinking. LeetCode examples were most likely data leakage.\n\nSuch drastic gap between MMLU performance and end-to-end coding is somewhat surprising. &lt;sarcasm&gt;Looks like AGI is not here yet.&lt;/sarcasm&gt; Thoughts?","link":"https://www.reddit.com/r/MachineLearning/comments/122ppu0/d_gpt4_and_coding_problems/","created":"2023-03-26","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":160}}
{"title":"[D] Can we train a decompiler?","description":"Looking at how GPT can work with source code mixed with language, I am thinking that similar techniques could perhaps be used to construct a decent decomplier. Consider a language like C. There are plenty of open sources which could be compiled. Then you can use the dataset consisting of (source code, compiled code) pairs to train a generative model to learn the inverse operation from data. Ofc, the model would need to fill in the information lost during compilation (variable names etc) in a human-understandable way, but looking at the recent language models and how they work with source codes, this now seems rather doable. Is anyone working on this already? I would consider such an application to be extremely beneficial.","link":"https://www.reddit.com/r/MachineLearning/comments/123asbg/d_can_we_train_a_decompiler/","created":"2023-03-27","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":12}}
{"title":"[P] SimpleAI : A self-hosted alternative to OpenAI API","description":"Hey everyone,\n\nI wanted to share with you [SimpleAI](https://github.com/lhenault/simpleAI), a self-hosted alternative to OpenAI API.\n\nThe aim of this project is to replicate the (main) endpoints of [OpenAI API](https://platform.openai.com/docs/introduction), and to let you easily and quickly plug in any new model. It basically allows you to deploy your custom model wherever you want and easily, while minimizing the amount of changes both on server and client sides.\n\nIt's compatible with the [OpenAI client](https://github.com/openai/openai-python) so you don't have to change much in your existing code (or can use it to easily query your API).\n\nWether you like or not the AI-as-a-service approach of OpenAI, I think that project could be of interest to many. Even if you are fully satisfied with a paid API, you might be interested in this if:\n\n* You need a model fine tuned on some specific language and don't see any good alternative, or your company data is too sensitive to send it to an external service\n\n* You\u2019ve developped your own awesome model, and want a drop-in replacement to switch to yours, to be able to A/B test the two approaches.\n\n* You're deploying your services in an infrastructure with an unreliable internet connection, so you would rather have your service locally\n\n* You're just another AI enthusiast with a lot of spare time and free GPU\n\nI've personally really enjoyed how open the ML(Ops) community has been in the past years, and seeing how the industry seems to be moving towards paid API and black box systems can be a bit worrying. This project might be useful to expose great, community-based alternatives.\n\n\nIf that sounds interesting, please have a look at the [examples](https://github.com/lhenault/simpleAI/tree/main/examples). I also have a [blogpost](https://louishenault.com/p/replicating-openai-api-for-llama-alpaca-or-any-animal-shaped-llm/) explaining a few more things.\n\n\nThank you!","link":"https://www.reddit.com/r/MachineLearning/comments/122tddh/p_simpleai_a_selfhosted_alternative_to_openai_api/","created":"2023-03-26","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":4}}
{"title":"[D] Favorite tips for staying up to date with AI/Deep Learning research and news?","description":"AI breakthroughs are happening non-stop! What are your approaches to staying up to date?\n\n&amp;#x200B;\n\nNot perfect, but here's what I do at the moment:  \n\n\n1. I create lists for major categories that interest me, collecting books, articles, blog posts, videos, and discussions. (The choice of the tool for list-making is less important than the habit and workflow.)\n2. I capture everything that appears interesting, but defer to reading it later -- I found that it's all about the tricky balance between prioritizing, exploring, and avoiding distractions.\n3. I set weekly goals for myself for consuming selected resources, understanding that not everything captured is a priority. (Usually, I set aside 1 hour in the morning at least)  \n\n4. I use tools and social platforms like Google Scholar alerts, Papers with Code, Twitter, and newsletters to stay updated.  \n\n\n(I just wrote a slightly more lengthy outline of this here: [https://sebastianraschka.com/blog/2023/keeping-up-with-ai.html](https://sebastianraschka.com/blog/2023/keeping-up-with-ai.html))","link":"https://www.reddit.com/r/MachineLearning/comments/122r3sr/d_favorite_tips_for_staying_up_to_date_with/","created":"2023-03-26","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":20}}
{"title":"[D] GPT Question Answering with Reasoning","description":"From what I understand, most people building QA bots with LLMs like GPT3/4/etc, take the approach of creating a vectorstore of their embeddings and when a new question is asked, do some similarity search and include the top X similarity results as part of the prompt into the LLM.\n\n\nHow would you approach getting answers to questions that require more reasoning over your entire set of data?\n\n\nAs an example, consider using several cookbooks as your set of input documents and the question you ask is: \"how many of the recipes in the set use carrots as an ingredient?\" OR \"what is the most common ingredient in all of these recipes?\"\n\n\nUsing the similarity approach, you will likely get all recipes that include carrots, but if you're only taking the top X to send to the LLM, you may not get all of them. The same may be true if you take all results with a similarity score over a certain threshold.\n\n\nAny ideas on how to handle this?","link":"https://www.reddit.com/r/MachineLearning/comments/1234qny/d_gpt_question_answering_with_reasoning/","created":"2023-03-27","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":6}}
{"title":"Have deepfakes become so realistic that they can fool people into thinking they are genuine? [D]","description":"I saw this story of a 50 year old Japanese man who facewapped his face with a young women's face. His followers didn't suspect anything of the photos he posted until he came clean and revealed his identity.\n\nhttps://www.insider.com/man-who-used-faceapp-pretend-woman-more-popular-than-before-2021-5\n\nAnother story I found was of a South Korean youtuber/influencer who became popular, amassed millions of views and then reveled she was deepfaked by a company called dob world.\n\nhttps://www.youtube.com/watch?v=cGycBsawTew\n\nDo you think deepfakes are realistic enough that people can't tell they're looking at a deepfake unless told? The celebrity deepfakes seem obvious since we know the celebrity and can usually know what content they're actually in. But if not told, and looking at an non-famous person, are deepfakes obvious when you see them? Especially if made by a company that has high quality large dataset for both faces\n\nIt makes me wonder how many influencers are deepfaked or edited heavily that they look completely different in person. And I don't just mean photoshopping to look skinny but their face/identity isn't the same in anyway.","link":"https://www.reddit.com/r/MachineLearning/comments/122t1b5/have_deepfakes_become_so_realistic_that_they_can/","created":"2023-03-26","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":14}}
{"title":"ICML: Responding to reviewer after reviewer-author discussion period has passed? [D]","description":"The ICML author-reviewer discussion period officially ended yesterday at 3pm ET, but overnight we received a reply from one of our reviewers that had not replied at all. We are seemingly still able to post comments to OpenReview.\n\nHas anyone else experienced this? Can/should we respond to this reviewer? Would this be violating some rule?","link":"https://www.reddit.com/r/MachineLearning/comments/123gvu2/icml_responding_to_reviewer_after_reviewerauthor/","created":"2023-03-27","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[D] E-Commerce Dataset for Product Recommendation","description":"I want to build a product recommendation system using both product based and user based collaborative filtering. \n\nFor this, I need an e-commerce dataset that includes product views and purchases (user#123 view/add to cart/buy product named XYZ), as well as product and category names (subcategories would be nice) so I can make sure my recommendations make sense.\n\nData from an e-commerce website with a variety of products and a lot of users like Amazon would be great. Bonus points if the products have descriptions.\n\nAll the datasets I found online either doesn't include product view data or product names (generally masked).\n\nI hope you guys can help me find a dataset that satisfy the requirements.","link":"https://www.reddit.com/r/MachineLearning/comments/1233pzh/d_ecommerce_dataset_for_product_recommendation/","created":"2023-03-26","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[D] Best practices for fine-tuning NLP models for prompt-based applications?","description":"I've noticed that the best NLP models are the ones that have been fine-tuned on the data they learned from rather than their size. For example, the LLaMA model has been fine-tuned and achieved a better overall score compared to models with larger parameter counts. (LLaMA's biggest model has 65B parameters, compared to 175B from GPT-3). I'm interested in learning more about the best practices for fine-tuning NLP models, especially technics that experts at Facebook or Stanford uses, with a focus on prompt-based applications.\n\nCan anyone share tips on how to fine-tune NLP models effectively for prompt-based applications? What data should be used for fine-tuning, and how should the data be preprocessed? How can we optimize the hyperparameters during fine-tuning? Are there any particular techniques or tools that work best for fine-tuning NLP models for prompt-based applications?\n\nAdditionally, I'm curious about the format used for the data that is mined for NLP models. What format is best for the data to be in, and how is it typically organized for training and fine-tuning purposes? It's worth noting that my main interest in NLP is prompt-based applications, rather than text completion.","link":"https://www.reddit.com/r/MachineLearning/comments/122mc1c/d_best_practices_for_finetuning_nlp_models_for/","created":"2023-03-26","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[D] Alternatives to double-blind reviewing?","description":"Double blind reviewing has become the norm in ML research. But with anonymity comes a lack of accountability.\n\n\\- Since authors can't flex with their professhorships and institutions, I feel authors are resorting to flexing with overly formal and technical descriptions to intimidate reviewers into accepting. This hurts clarity of presentation and narrows the audience of published papers.\n\n\\- There is little incentive for reviewers to do an honest and good job: 1) they don't get any payment for a good job 2) they have a potential conflict of interest in that they are reviewing work competing for publication in the same venue. So reviewers can be finicikity during reviews, and completely ghost authors during discussion periods.\n\nIs a simple fix to anonymise authors and reviewers during the review process, but deanonymise them after decisions have been reached?","link":"https://www.reddit.com/r/MachineLearning/comments/122vt2h/d_alternatives_to_doubleblind_reviewing/","created":"2023-03-26","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":9}}
{"title":"[D] Build a ChatGPT from zero","description":"I've recently discovered models such as ChatLLaMA that allows you to create a \"ChatGPT\" but you need Meta's LLaMA weights (yes, you can find them in torrents but that's not the point of the question). Similar limitations found in other cases.\n\nTherefore I wanted to try to find an open source: dataset (in addition to hugging face), \"base model\", \"chat model\"  AND that it is feasible to train with a commercial computer with a very good GPU (NVIDIA, etc.). With this get at least decent results.\n\nAlso would be interesting to distinguish between solutions with commercial limitations and those who don't.\n\nThanks!\n\n\u2022 EDIT \u2022\nA first solution I already found is this: https://github.com/databrickslabs/dolly based on this https://huggingface.co/EleutherAI/gpt-j-6B, but looking for some discussion and perhaps other/better solutions.","link":"https://www.reddit.com/r/MachineLearning/comments/12327d1/d_build_a_chatgpt_from_zero/","created":"2023-03-26","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":8}}
{"title":"Is it possible to merge transformers? [D]","description":"In the last few days I had a new thought. I don't know if it is possible or already done somewhere? Is it possible to merge the weights of two transformer models like they do with merging stable diffusion models?\nLike can I merge for example BioBert and LegalBert and get a model that can do both?","link":"https://www.reddit.com/r/MachineLearning/comments/122fj05/is_it_possible_to_merge_transformers_d/","created":"2023-03-26","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":10}}
{"title":"Tools for to solve domain gap between source and target data [D]","description":"Hey guys,  do you know any tools/solutions that help to bridge domain gaps between source and target data? Did you try some that you'd recommend?  Cheers!","link":"https://www.reddit.com/r/MachineLearning/comments/122ooez/tools_for_to_solve_domain_gap_between_source_and/","created":"2023-03-26","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
