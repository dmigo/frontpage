{"title":"[R] \ud83e\udd16\ud83c\udf1f Unlock the Power of Personal AI: Introducing ChatLLaMA, Your Custom Personal Assistant! \ud83d\ude80\ud83d\udcac","description":" \ud83d\ude80 Introducing ChatLLaMA: Your Personal AI Assistant Powered by LoRA! \ud83e\udd16 \n\n&amp;#x200B;\n\nHey AI enthusiasts! \ud83c\udf1f We're excited to announce that you can now create custom personal assistants that run directly on your GPUs!\n\n&amp;#x200B;\n\nChatLLaMA utilizes LoRA, trained on Anthropic's HH dataset, to model seamless conversations between an AI assistant and users.  \n\n&amp;#x200B;\n\nPlus, the RLHF version of LoRA is coming soon! \ud83d\udd25  \n\n&amp;#x200B;\n\n\ud83d\udc49 Get it here: [https://cxn.to/@serpai/lora-weights](https://cxn.to/@serpai/lora-weights)  \n\n&amp;#x200B;\n\n\ud83d\udcda Know any high-quality dialogue-style datasets? Share them with us, and we'll train ChatLLaMA on them!  \n\n&amp;#x200B;\n\n\ud83c\udf10 ChatLLaMA is currently available for 30B and 13B models, with the 7B version coming soon.  \n\n&amp;#x200B;\n\n\ud83d\udd14 Want to stay in the loop for new ChatLLaMA updates? Grab the FREE \\[gumroad link\\]([https://cxn.to/@serpai/lora-weights](https://cxn.to/@serpai/lora-weights)) to sign up and access a collection of links, tutorials, and guides on running the model, merging weights, and more.  (Guides on running and training the model coming soon)\n\n&amp;#x200B;\n\n\ud83e\udd14 Have questions or need help setting up ChatLLaMA? Drop a comment or DM us, and we'll be more than happy to help you out! \ud83d\udcac  \n\n&amp;#x200B;\n\nLet's revolutionize AI-assisted conversations together! \ud83c\udf1f  \n\n&amp;#x200B;\n\n\\*Disclaimer: trained for research, no foundation model weights, and the post was ran through gpt4 to make it more coherent.   \n\n&amp;#x200B;\n\n\ud83d\udc49 Get it here: [https://cxn.to/@serpai/lora-weights](https://cxn.to/@serpai/lora-weights)","link":"https://www.reddit.com/r/MachineLearning/comments/11w03sy/r_unlock_the_power_of_personal_ai_introducing/","created":"2023-03-19","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":128}}
{"title":"[D] Incorporating external data in LSTM models for sales forecasting in e-commerce","description":"**Background:** \n\nI'm working on a project related to sales forecasting in e-commerce and comparing the performance of ARIMAX, lightGBM, and LSTM models across various aggregation levels. I'm also examining the impact of additional features like promotions, inventory levels, and weather on demand forecasting.\n\n&amp;#x200B;\n\n**Question:** \n\nI'm curious about utilizing external data, such as weather information, in LSTM models. My understanding of LSTM is that it calculates the most probable next values based on a certain number of historical values. Can LSTM models use more than one feature to forecast a single target variable? Moreover, is it possible to leverage future features like holidays to improve LSTM forecasts? \n\n&amp;#x200B;\n\nI would appreciate any resources, such as projects, books, or tutorials, that could help me better understand this process. Thank you!","link":"https://www.reddit.com/r/MachineLearning/comments/11weava/d_incorporating_external_data_in_lstm_models_for/","created":"2023-03-20","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":0}}
{"title":"[R] What are the current must-read papers representing the state of the art in machine learning research?","description":"Recently, John Carmack [suggested](https://twitter.com/ID_AA_Carmack/status/1622673143469858816) the creation of a \"canonical list of references from a leading figure,\" referring to a never-released reading list given to him by Ilya Sutskever.\n\nWhile there may be an undue interest in that specific list, MLR is such a big field that it's difficult to know where to start. What are the major papers that are relevant to state of the art work being done in 2023? Perhaps we may crowd-source a list here?","link":"https://www.reddit.com/r/MachineLearning/comments/11vs3oe/r_what_are_the_current_mustread_papers/","created":"2023-03-19","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":11}}
{"title":"[D] For those who have worked 5+ years in the field, what are you up to now?","description":"Hey,I've  been working in ML for the last 5-10 years, almost since deep learning  came up, mostly startups with various successes, sometimes doing more research sometimes doing more engineering tasks.\n\nThis year I was excited to manage a team focused on ML but plans have just changed in my company and this isn't going to happen anytime soon. I am interviewing for another company for a \"senior\"  role but they still ask me to do this basic ML assignment that takes a  few hours to complete. I have just realised how boring it became to me  to the point I don't even want to do it as I literally have done this  for the last 5+ years.\n\nFor those  who have been in the field for at least 5+ years, what are you up to now? Are you still doing ML? Have you moved into management? Have you started your own company? Moved to a different subfield perhaps?\n\nPS: I  have a PhD, in addition to those years of experience i mention. I am  aware this isn't related to ML purely but more like mid-career crisis,  but would be interested to know how people in the field have dealt with  it.","link":"https://www.reddit.com/r/MachineLearning/comments/11vygjb/d_for_those_who_have_worked_5_years_in_the_field/","created":"2023-03-19","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":2}}
{"title":"[Discussion] In which way could Machine Learning be useful for a journaling app?","description":"As per Title, I would like to use Machine Learning to make the journaling expierence better. I got some Questions in that Regard.\n\nFirstly, is it meaningful to host the model on the users device, to keep the Data safe?\n\nWhat do you suggest would be a useful way? A chat that response to the entries? Or meaningful prompts?  \n\nShould the Model learn only from what the user has written in his journal or be pretrained of scientific data or other data to respond to what the user has written accordingly?","link":"https://www.reddit.com/r/MachineLearning/comments/11weeks/discussion_in_which_way_could_machine_learning_be/","created":"2023-03-20","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":0}}
{"title":"[D] IJCAI 2023 Rebuttal Discussion","description":"Title","link":"https://www.reddit.com/r/MachineLearning/comments/11w8x8d/d_ijcai_2023_rebuttal_discussion/","created":"2023-03-20","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":0}}
{"title":"[P] Let's build ChatGPT","description":"Hi all, I just made a tutorial on how to build a basic RLHF system on top of Andrej Karpathy's nanoGPT. I'm grateful to have gotten a thumbs up on Twitter from the legend himself, always a bit nerve wracking making this sort of thing.\n\nI'm sharing this here because I'd love to go deeper into teaching and building this out, if people are interested in watching this sort of thing. Would be very helpful to hear your thoughts.\n\nHere's the code:\n\nhttps://github.com/sanjeevanahilan/nanoChatGPT\n\nThe video: \n\nhttps://m.youtube.com/watch?v=soqTT0o1ZKo&amp;feature=youtu.be","link":"https://www.reddit.com/r/MachineLearning/comments/11v6bvv/p_lets_build_chatgpt/","created":"2023-03-19","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":16}}
{"title":"[D] Best ChatBot that can be run locally?","description":"What do you guys think is currently the best ChatBot that you can download and run offline? After hearing that Alpaca has results similar to GPT-3, I was curious if anything else competes.","link":"https://www.reddit.com/r/MachineLearning/comments/11w8lp2/d_best_chatbot_that_can_be_run_locally/","created":"2023-03-20","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":3}}
{"title":"[Project] What if FastAPI supported NumPy arrays and Pillow images?","description":"When deploying ML models with FastAPI we always had to write our own serialisation code for numpy.ndarray and PIL.Image. Not only have we replaced FastAPI with up to 100x faster C-level library a couple of weeks ago, but we have also recently added support for all the fancy Pythonic types on both client and server sides.  \n\n\n[Check it out on GitHub/Unum-Cloud/UJRPC](https://github.com/unum-cloud/ujrpc#more-functionality-than-fastapi)  \n\n\nhttps://preview.redd.it/3m73l6qodpoa1.png?width=1648&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e50099bce90cb39d5dda0a3890b46d914b11be9c","link":"https://www.reddit.com/r/MachineLearning/comments/11vmgj6/project_what_if_fastapi_supported_numpy_arrays/","created":"2023-03-19","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":0}}
{"title":"[D] Systematised Network Diagrams","description":"Hi all, I've been working on this neural network diagram convention for a few years now and would love to hear your feedback on it. I have personally found it useful, so I thought I would share it in case others would like to adopt it too.\n\nMy motivation was that almost every scientific paper uses a different diagrammatic system to depict their networks. This puts a burden on the reader to get to grips with the unique system, compounded with understanding the novel network displayed.\n\nThe aim of this system is to be modular, minimalistic, and consistent, enabling fast interpretation and universal communication of network architectures in scientific papers and presentations. This outlined key system is designed to represent clear, symbolic placeholders for mathematical functions, much like Feynman diagrams for quantum field theory or logic gates for mathematical logic. My [GitHub](https://github.com/GeorgeBird1/Diagramatic-Neural-Networks) page on this system offers an easy way to centralise, date, and update the convention. No accreditation is required when using this system for any purpose.\n\nIt is easy for hand drawing, with an included shorthand notation, alongside a more technical depiction for use in papers. All symbols are constructable using common flow-chart shapes for easy implementation.\n\nHere are some example networks I've drawn using this system:\n\n[Depiction of various types of neural network architectures using this system.](https://preview.redd.it/2geqegs3cqoa1.png?width=928&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=669c19ece3bc82fd2189f9e325da282a51b70ca5)\n\nHere are the basic key-components, more can be found on my [GitHub](https://github.com/GeorgeBird1/Diagramatic-Neural-Networks) link:\n\n[The basic key system for constructing diagrams. More available on the GitHub](https://preview.redd.it/mlhxhgfbcqoa1.png?width=750&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=5090b1a914dcadd8b491506fb469534e1437531f)\n\nThanks for reading, any feedback gratefully received! :)","link":"https://www.reddit.com/r/MachineLearning/comments/11vv056/d_systematised_network_diagrams/","created":"2023-03-19","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":4}}
{"title":"[D] Modern Topic Modeling/Discovery","description":" I was wondering what are the modern techniques for topic discovery for short and long text. It seems this topic to be slower advancing compared to the rest. I am aware of bertopic but tbh I always have issues finetuning it.\n\nOn a second thought I was thinking to use qna/chat gpt models in order to generate models, so I wanted to ask your opinion on some potential prompts that I could use. Essentially a bit of brainstorming. I will open source all the gathered ideas along with mine and share the link here :)","link":"https://www.reddit.com/r/MachineLearning/comments/11w116z/d_modern_topic_modelingdiscovery/","created":"2023-03-19","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":4}}
{"title":"[R] What do we think about Meta-Interpretive Learning?","description":"Came across this concept, Meta-Interpretive Learning (MIL) developed by Muggleton, Patsantzis, et al.\n\n* [https://arxiv.org/pdf/2101.05050.pdf](https://arxiv.org/pdf/2101.05050.pdf)\n* [https://arxiv.org/pdf/2106.07464.pdf](https://arxiv.org/pdf/2106.07464.pdf)\n* [Presentation](https://www.youtube.com/watch?v=73cBWmjlFLk)\n\nFrom what I understand this is a relatively new approach to ML? Has anyone heard of this? I was hoping to get a general feel for what people in the industry believe for the perspectives of this approach. If you're curious, here's an [implementation](https://github.com/stassa/louise) of MIL.","link":"https://www.reddit.com/r/MachineLearning/comments/11w4kqd/r_what_do_we_think_about_metainterpretive_learning/","created":"2023-03-20","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":0}}
{"title":"[R] Quantitative comparison of ChatGPT and GPT-4 performance on multiple open source datasets","description":"Preliminary results give credence to some of the claims made by OpenAI regarding performance gains achieved by GPT-4 across domains. Unanswered questions remain regarding training data used and possible leakage. Tools used were Langchain and the current API endpoints (chatgpt-3.5-turbo and gpt-4).\n\nhttps://twitter.com/K_Hebenstreit/status/1636789765189308416","link":"https://www.reddit.com/r/MachineLearning/comments/11vl691/r_quantitative_comparison_of_chatgpt_and_gpt4/","created":"2023-03-19","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":0}}
{"title":"[D] Totally Open Alternatives to ChatGPT","description":"I have migrated this to GitHub for easy contribution: https://github.com/nichtdax/awesome-totally-open-chatgpt\n\nBy alternative, I mean projects feature different language model for chat system.\nI do **not** count alternative **frontend** projects because they just call the API from OpenAI. \nI do **not** consider alternative **transformer decoder** to GPT 3.5 either because the training data of them are (mostly) not for chat system.\n\nTags:\n\n-   B: bare (no data, no model's weight, no chat system)\n-   F: full (yes data, yes model's weight, yes chat system including TUI and GUI)\n\n| Project                                                                               | Description                                                                                                                                                                                                                                                                                                                                                                               | Tags |\n| ------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- |\n| [lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch)       | Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM                                                                                                                                                                                                                                                      | B    |\n| [togethercomputer/OpenChatKit](https://github.com/togethercomputer/OpenChatKit)       | OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. [Demo](https://huggingface.co/spaces/togethercomputer/OpenChatKit)                                                                                                                                                                                    | F    |\n| [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) | A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, LLaMA, and Pygmalion.                                                                                                                                                                                                                                                                                    | F    |\n| [KoboldAI/KoboldAI-Client](https://github.com/KoboldAI/KoboldAI-Client)               | This is a browser-based front-end for AI-assisted writing with multiple local &amp; remote AI models. It offers the standard array of tools, including Memory, Author's Note, World Info, Save &amp; Load, adjustable AI settings, formatting options, and the ability to import existing AI Dungeon adventures. You can also turn on Adventure mode and play the game like AI Dungeon Unleashed. | F    |\n| [LAION-AI/Open-Assistant/](https://github.com/LAION-AI/Open-Assistant/)               | OpenAssistant is a chat-based assistant that understands tasks, can interact with third-party systems, and retrieve information dynamically to do so.                                                                                                                                                                                                                                     | F    |","link":"https://www.reddit.com/r/MachineLearning/comments/11uk8ti/d_totally_open_alternatives_to_chatgpt/","created":"2023-03-18","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":74}}
{"title":"[D] \"Glaze\" claims to be able to apply an invisible filter to images to prevent them being useful for training image models. Real tech, or a grift?","description":"This tool \"Glaze\" has been picking up a lot of traction on social media from the anti-AI-image-generator camp, with the general claim being that any image can be \"protected\" from making a useful contribution if included in model training corpora by applying imperceptible filtering.\n\nWithout commenting on the arguments for or against whether this would be a good thing to exist, I am interested in hearing whether or not this capability actually exists, or whether people are once again leaning in hard for a false claim about a system they don't understand. I don't want to go digging through any technical information they've released because the whole conversation makes me want to tear my hair out and is rife with misinfo, but, from what I've actually heard on the technical side, it has sounded more like some sort of adversarial attack dependent on the latent space implementation of Stable Diffusion specifically, which would not \"protect\" their users from inclusion in other models. Even if the approach were to be extensible to incorporate adversarials against other models on the fly, this wouldn't retroactively protect already processed images from being used by future models with different internals. (I guess unless there was some sort of live system built into web image hosts, but we are not there yet...)\n\nAnyway, my gut instinct is that people are being mislead here based on their fear of their images being incorporated into training sets and \"stolen\", but before I make any strong claims to that effect it'd be nice to hear from someone who has more knowledge of the area.","link":"https://www.reddit.com/r/MachineLearning/comments/11v972n/d_glaze_claims_to_be_able_to_apply_an_invisible/","created":"2023-03-19","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":17}}
{"title":"[P] Semantic Feature Embeddings from Hashtags","description":"I want to get semantic feature embeddings given a list of hashtags, to find similar users in social media data (using cosine similarity) or even do zero shot classification. I thought of using a BERT-like pretrained encoder language model, but I guess this is not optimal because grammar and word order do not matter in this case.\n\nDo you know such pretrained embedding model or have any tips, how to train such a model in an unsupervised way( I already have millions of posts containing hashtags)?","link":"https://www.reddit.com/r/MachineLearning/comments/11vqfow/p_semantic_feature_embeddings_from_hashtags/","created":"2023-03-19","tags":["machinelearning","ml","reddit"],"meta":{"num_comments":0}}
