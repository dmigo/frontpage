{"title":"Modern Topic Modeling/Discovery","description":"I was wondering what are the modern techniques for topic discovery for short and long text. It seems this topic to be slower advancing compared to the rest. I am aware of bertopic but tbh I always have issues finetuning it. \n\nOn a second thought I was thinking to use qna/chat gpt models in order to generate models, so I wanted to ask your opinion on some potential prompts that I could use. Essentially  a bit of brainstorming. I will open source all the gathered ideas along with mine and share the link here :)","link":"https://www.reddit.com/r/LanguageTechnology/comments/11vwtn3/modern_topic_modelingdiscovery/","created":"2023-03-19","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":0}}
{"title":"New to Headline Quality subfield -- what are some good models that are readily customizable for domain nomenclature specificity?","description":"Due to need at work, I've been plugged into a project in the Headline Quality NLP subarea despite my complete inexperience in NLP. The project involves assessing headline/title quality of some pharmaceutical reports and estimating semantic divergence between headline and articles. I'm working through some papers like [this one by Omidvar et al.](https://arxiv.org/abs/1911.11139), but don't need state of the art at the moment, just some quick and dirty initial results, and am looking for some input about types of models I should look at for getting started on limited computational resources (I'll only have access to one GPU at most). I'm particularly interested in models I can flexibly modify/hardcode to account for corporate and pharmaceutical nomenclature and terminology. Is this a problem I should still throw transformer models at, even with a dearth of GPU resources, or do any older, simpler models come to mind? Thank you for your time and help!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11w7mh3/new_to_headline_quality_subfield_what_are_some/","created":"2023-03-20","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":0}}
{"title":"multitask learning in classification thesis","description":"Hi! I\u2019m in my senior year and I have two months to write a thesis about multitask learning in nlp classifiers. I\u2019ve read a decent amount of papers on the topic and I would like to share my plan, because I need some feedback!\n\nSo, in my thesis I explore the influence of multitask learning on multilingual models in classification tasks. I\u2019ve decided to use only hard-parameter sharing models \u2014 seems that soft-parameter sharing and other architectural designs (sluice networks and so on) are not SOTA.\n\nModels: I\u2019ve picked XLM-R to investigate encoder + multiple classification heads architecture and mT5 as a SOTA model (and a predecessor of prompt-based models).\n\nDataset: XGLUE, from which I took 6 classification tasks. \n\nAreas of interest: 1. Task relatedness (I found it to be a reaaally ambiguous term. In essence, I would visualise and analyse the embeddings of tasks, but I don\u2019t know how, given they all have two input sentences)\n2. Catastrophic language and task forgetting (my plan is to translate data examples to the target languages and mix languages inside every example \u2014 should it be called \u201clanguage mismatch\u201d?\nThen I would compare it to cross-lingual transfer baselines, where we only train the model on english data)\n3. Shift to multitask pre-training (Small in-house dataset in a foreign language to test models pre-trained on all (or maybe only related) XGLUE tasks\n\nWhat do you think? What should I incorporate? What hypotheses should I test? Are there any recent developments (e.g. adapter layers) that are relevant and open (no GPT I guess)?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11w0i1s/multitask_learning_in_classification_thesis/","created":"2023-03-19","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":1}}
{"title":"Are pre-trained word embeddings (word2vec, glove, fasttext) obsolete now? given wide use of pre-trained languages models like bert etc","description":"","link":"https://www.reddit.com/r/LanguageTechnology/comments/11vav4y/are_pretrained_word_embeddings_word2vec_glove/","created":"2023-03-19","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":20}}
{"title":"Choosing Non-Linear vs Linear","description":"Hello all,\n\nIs there a process for deciding whether to use a Non-Linear or Linear text classifier? From what I have been reading, it seems like people develop scatter plots from their data points to see if their data is linearly separable. \nDo people do this with text data? What does everyone do to evaluate their choice of model?\n\nThanks!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11uyz56/choosing_nonlinear_vs_linear/","created":"2023-03-18","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":0}}
{"title":"Wanna team-up for Quantum NLP projects?","description":"I recently started reading about Quantum NLP. A very experimental and new field in Natural Language Processing. There are only a handful or research papers out there to aid in the knowledge of Quantum NLP, even universities such as MIT, Harvard and Stanford aren't capable or fully understand Quantum NLP yet. Only a few Quantum Computing research labs have the surface-intermediate understanding of Quantum NLP such as Cambridge Quantum.   \n\n\nI have read some of the most recent and important Quantum NLP papers and used **lambeq the only python library capable enough to do Quantum NLP.** Fast forward, I have implemented a basic Quantum NLP project where I classify sentences using Quantum NLP. \n\nI couldn't find many people who are interested in Quantum NLP, that's why, I was looking forward if someone is interested in Quantum NLP in this thread and has previous experience working with NLP itself then we can make a small team and study more advanced topics on Quantum NLP and do cool projects in our pastime. \n\n**GitHub repo link:** [https://github.com/sleepingcat4/Quantum-NLP](https://github.com/sleepingcat4/Quantum-NLP)  \n\n\nIf you're interested in teaming-up, kindly send me a message on **reddit or discord: sleeping\\_cat4#8182**","link":"https://www.reddit.com/r/LanguageTechnology/comments/11uia4r/wanna_teamup_for_quantum_nlp_projects/","created":"2023-03-18","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":17}}
{"title":"An Instruct Version Of GPT-J Using Stanford Alpaca's Dataset","description":"I  just released an instruct version of GPT-J using Stanford Alpaca's  dataset.The result of this experiment is very cool and confirms that,  when fine-tuned on the right data, GPT-J is a very powerful AI model!You  can download the model from the HuggingFace hub: [https://huggingface.co/nlpcloud/instruct-gpt-j-fp16](https://huggingface.co/nlpcloud/instruct-gpt-j-fp16)\n\nHere is an example:\n\n`from transformers import pipeline import torch`\n\n`generator = pipeline(model=\"nlpcloud/instruct-gpt-j-fp16\", torch_dtype=torch.float16, device=0)`\n\n`prompt = \"Correct spelling and grammar from the following text.\\nI do not wan to go\\n\" print(generator(prompt))`\n\nMore details about this experiment here: [https://nlpcloud.com/instruct-version-of-gpt-j-using-stanford-alpaca-dataset.html](https://nlpcloud.com/instruct-version-of-gpt-j-using-stanford-alpaca-dataset.html?utm_source=reddit&amp;utm_campaign=mwu8d596-3816-11ed-a261-0242ac140007)\n\nI hope it will be useful! Please don't hesitate to share some feedbacks!\n\nJulien","link":"https://www.reddit.com/r/LanguageTechnology/comments/11tqqcf/an_instruct_version_of_gptj_using_stanford/","created":"2023-03-17","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":7}}
{"title":"New NLP Game Design potentials","description":"Hello I wanted to share some ideas! I believe some of these ideas to be legit avenues for making games with natural language processing, enabled by the power of GPT-4, and I really want to inspire more people down the line! Here are some apps you could make with the openAI API that leverage a whole new degree of responsiveness:  \n\n\n1. A card game where combat is settled by the names of the cards rather than descriptions or card text, using brief but accurate battle simulations! Pair nouns and adjectives, or even fuse cards to make novel new concepts! Who wins, Saitama or Goku? It takes on a whole new level of fairness and intuition when you let the AI take control!\n2. API calls could be used to procedurally generate enemies or catchable monsters in a roguelike! You could provide an example of a json stat sheet and go from there\n3. Considering json, you could (maybe) create a fighting game with MUGEN that merges calls between openai and an art generator, and create the ultimate platform fighter where players type in the name of their character instead of choosing from a select screen! (although generating move sprites is likely gatekept by a few things still....)  \n\n\nThank you for reading! please considering sharing some of these ideas or trying them out yourself, especially the first one I think it quite accessible. Imagine a deck building game where your card database list is the dictionary :)","link":"https://www.reddit.com/r/LanguageTechnology/comments/11u7lt9/new_nlp_game_design_potentials/","created":"2023-03-17","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":0}}
{"title":"Grinnell vs Reed for eventual PhD","description":"Hello, I am trying to decide between these two schools that I have been accepted to for my undergraduate education.\n\nI plan on pursuing a major in computer science with a concentration in linguistics at Grinnell or an interdisciplinary major with computer science and linguistics at Reed. I was just wondering which of these have the better reputation, if any, in the field.\n\n&amp;#x200B;\n\nThanks!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11tvrf2/grinnell_vs_reed_for_eventual_phd/","created":"2023-03-17","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":6}}
{"title":"Fine-tuning BERT for generating short story, how to do it?","description":"","link":"https://www.reddit.com/r/LanguageTechnology/comments/11tqy4f/finetuning_bert_for_generating_short_story_how_to/","created":"2023-03-17","tags":["ml","reddit","languagetechnology"],"meta":{"num_comments":6}}
