{"title":"[P] GITModel: Dynamically generate high-quality hierarchical topic tree representations of GitHub repositories using customizable GNN message passing layers, chatgpt, and topic modeling.","description":"Decompose Python libraries and generate Coherent hierarchical topic models of the repository.  \n[https://github.com/danielpatrickhug/GitModel](https://github.com/danielpatrickhug/GitModel)\n\nThe ability to bootstrap its own codebase is a powerful feature as it allows for efficient self-improvement and expansion. It means that the codebase is designed in such a way that it can use its own output as an input to improve itself. In the context of GitModel, this feature allows for the efficient improvement and expansion of its own codebase. By using its own output to generate hierarchical topic trees of GitHub repositories, it can analyze and extract insights from its own codebase and other codebases to improve its functionality. This can lead to more efficient and effective code generation, better semantic graph generation, and improved text generation capabilities.\n\n  \nI spent around 10 hours today on a major refactor creating a simple pipeline abstraction and allowing dynamic instantiation from yaml configs. It now also supports multiple GNN heads.\n\nPlease try it out and let me know what you think!\n\nExample:  \n[https://github.com/deepmind/clrs](https://github.com/deepmind/clrs)\n\nhttps://preview.redd.it/ut4fc6c401na1.png?width=1506&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b039242432c1f0526d1d81eadbfe8abc1168d2fd","link":"https://www.reddit.com/r/MachineLearning/comments/11o97on/p_gitmodel_dynamically_generate_highquality/","created":"2023-03-11","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":13}}
{"title":"[P] RWKV 14B is a strong chatbot despite only trained on Pile (16G VRAM for 14B ctx4096 INT8, more optimizations incoming)","description":"The latest CharRWKV v2 has a new chat prompt (works for any topic), and here are some raw user chats with RWKV-4-Pile-14B-20230228-ctx4096-test663 model (topp=0.85, temp=1.0, presence penalty 0.2, frequency penalty 0.5). You are welcome to try ChatRWKV v2:  [https://github.com/BlinkDL/ChatRWKV](https://github.com/BlinkDL/ChatRWKV)\n\nAnd please keep in mind that RWKV is 100% RNN :) Pile v1 date cutoff is year 2020.\n\n[Chat #1](https://preview.redd.it/ripvptomexma1.png?width=438&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=29ed1cd499dc4d693dee32ad7550a7910402d033)\n\n[Chat #2](https://preview.redd.it/8t75njnnexma1.png?width=438&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=d20186f2e423d321817b79e6e559dc74a42bf0b8)\n\nThese are surprisingly good because RWKV is only trained on the Pile (and 100% RNN). No finetuning. No instruct tuning. No RLHF. You are welcome to try it.\n\n1. Update ChatRWKV v2 \\[and rwkv pip package\\] to latest version.\n2. Use  [https://huggingface.co/BlinkDL/rwkv-4-pile-14b/blob/main/RWKV-4-Pile-14B-20230228-ctx4096-test663.pth](https://huggingface.co/BlinkDL/rwkv-4-pile-14b/blob/main/RWKV-4-Pile-14B-20230228-ctx4096-test663.pth)\n3. Run v2/chat.py and enjoy.\n\nChatRWKV v2 supports INT8 now (with my crappy slow quantization, **works for windows, supports any GPU**, 16G VRAM for 14B if you offload final layer to CPU). And you can offload more layers to CPU to run it with 3G VRAM though that will be very slow :) More optimizations are coming.\n\nOr you can try the 7B model (less coherency) and 3B model (not very coherent, but still fun).","link":"https://www.reddit.com/r/MachineLearning/comments/11nre6t/p_rwkv_14b_is_a_strong_chatbot_despite_only/","created":"2023-03-10","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":22}}
{"title":"[D] Development challenges of an autonomous gardening robot using object detection and mapping.","description":"Why do some folk think that this futuristic type of robot can't logically achieve a broad array of stated ML tasks?\n\n[https://youtu.be/EYTiTh7\\_zO4](https://youtu.be/EYTiTh7_zO4)\n\nI see the dev cost of this robot as being 100 times less than a self-driving car: single error fatality risk, unlimited chaotic cities, 90mph compute time limits, make self-driving cars unfeasible compared to multitask garden robots. \n\nFruit-picking is very difficult using AI, but weeding, digging, sowing seeds, irrigation, are fairly easy tasks, and an experienced developer knows that anything is possible with logic.\n\nMillions of acres of farmland are chemically and brutally treated for food that is wrapped in plastic, shipped hundreds of miles, to supermarkets, so as an environmental chemist, rural processes analyst and EE dabbler, I have created an emulator prototype for a garden robot :)","link":"https://www.reddit.com/r/MachineLearning/comments/11oaek2/d_development_challenges_of_an_autonomous/","created":"2023-03-11","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":2}}
{"title":"[D] Is ML a big boys game now?","description":"As much as I enjoy ML as a whole, I am a bit skeptical of the future for individuals. With OpenAI trying to monopolize the market along with Microsoft, which part remains for the small time researchers/developers?\n\nIt seems everything now is just a ChatGPT wrapper, and with GPT-4 around the corner I assume itll be even more prominent.\n\nWhat are your thoughts?","link":"https://www.reddit.com/r/MachineLearning/comments/11njpb9/d_is_ml_a_big_boys_game_now/","created":"2023-03-10","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":125}}
{"title":"[D] Bounding box learning in OCR process","description":" So, I can understand that OCR is a two step process : Text detection + text recognition. Currently, easy OCR/Paddle OCR is giving great text recognition results. For my case, I need to customize the bounding boxes alone for my input data (I played around the parameters but nothing seemed to help me for **borderless tables**). I have manually drawn bounding boxes using labelimg around text and would like to understand whether an object detection model or text detection algorithm should be trained for the same.","link":"https://www.reddit.com/r/MachineLearning/comments/11oag6d/d_bounding_box_learning_in_ocr_process/","created":"2023-03-11","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[D] What's the Time and Space Complexity of Transformer Models Inference?","description":"What's the Big (O) at inference time for transformer models? Is it different for BERT? RoBERTa? T5? DeBERTa?","link":"https://www.reddit.com/r/MachineLearning/comments/11nzinb/d_whats_the_time_and_space_complexity_of/","created":"2023-03-10","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":6}}
{"title":"[D] Development challenges of an autonomous gardening robot using object detection and mapping.","description":"Why do some folk think that this futuristic type of robot can't complete a broad array of stated garden tasks?\n\n[https://youtu.be/EYTiTh7\\_zO4](https://youtu.be/EYTiTh7_zO4)\n\nFruit-picking is very difficult using AI, but weeding, digging, sowing seeds, irrigation, are quite easy tasks, and an experienced developer knows that complexity is possible with logic. I see the dev cost of this robot as being 100 times less than a self-driving car: single error fatality risk, unlimited chaotic cities, 90mph compute time limits, make self driving cars unfeasible compared to multitask garden robots.\n\nMillions of acres of farmland are chemically and brutally treated for food that is wrapped in plastic, shipped hundreds of miles, to supermarkets, so as an environmental chemist, rural processes analyst and EE dabbler, I have created an emulator prototype for a garden robot :)","link":"https://www.reddit.com/r/MachineLearning/comments/11oax7a/d_development_challenges_of_an_autonomous/","created":"2023-03-11","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":6}}
{"title":"[P] Implementing Vision Transformer (ViT) from Scratch using PyTorch","description":"I recently delved into the world of transformers and their application to vision tasks.\n\nAs part of my learning process, I implemented the Vision Transformer (ViT) from scratch using PyTorch. I am sharing my implementation and a step-by-step guide to implementing the model in this post.\n\nI hope you find it helpful.\n\nGithub: [https://github.com/tintn/vision-transformer-from-scratch](https://github.com/tintn/vision-transformer-from-scratch)\n\nPost: [https://medium.com/towards-data-science/implementing-vision-transformer-vit-from-scratch-3e192c6155f0](https://medium.com/towards-data-science/implementing-vision-transformer-vit-from-scratch-3e192c6155f0)","link":"https://www.reddit.com/r/MachineLearning/comments/11nj58o/p_implementing_vision_transformer_vit_from/","created":"2023-03-10","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":2}}
{"title":"[P] Frouros: A Python library for drift detection in Machine Learning problems","description":"Hey everyone!\n\nI want to share with you an open-source library that we've been building for a while. Frouros: A Python library for drift detection in machine learning problems.\n\n[https://github.com/IFCA/frouros](https://github.com/IFCA/frouros)\n\nFrouros implements multiple methods capable of detecting both concept and data drift with a simple, flexible and extendable API. It is intended to be used in conjunction with any machine learning library/framework, therefore is framework-agnostic, although it could also be used for non machine learning problems.\n\nMoreover, Frouros offers the well-known concept of callbacks that is included in libraries like Keras or PyTorch Lightning. This makes it simple to run custom user code at certain points (e.g., on\\_drift\\_detected, on\\_update\\_start, on\\_update\\_end).\n\nWe are currently working on including more examples in the documentation to show what can be done with Frouros.\n\nI would appreciate any feedback you could provide us!","link":"https://www.reddit.com/r/MachineLearning/comments/11nx6ak/p_frouros_a_python_library_for_drift_detection_in/","created":"2023-03-10","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[D] Version 2.1 of the Open Deep Learning Toolkit for Robotics is already available!","description":"The latest version of the  Open Deep Learning Toolkit for Robotics, **Version 2.1 is already available !**\n\n This new version includes the following updates:\n\n**New Features:**\n\n* Added Efficient LiDAR Panoptic Segmentation\n* Added Nanodet 2D Object Detection tool\u00a0\n* Added C API implementations of NanoDet 2D Object Detection tool\n* Added C API implementations of forward pass of DETR 2D Object Detection tool\n* Added C API implementations of forward pass of DeepSORT 2D Object Tracking tool\u00a0\n* Added C API implementations of forward pass of Lightweight OpenPose, Pose Estimator tool\n* Added C API implementations of forward pass of X3D 2D Activity Recognition tool\u00a0\n* Added C API implementations of forward pass of Progressive Spatiotemporal GCN Skeleton-based Action Recognition tool\n* Added Binary High Resolution Analysis tool\n* Added Multi-Object-Search tool\u00a0\n\n***Enhancements***\n\n* Added support in C API for detection target structure and vector of detections\u00a0\n* Added support in C API for tensor structure and vector of tensors\n* Added support in C API for json parser\u00a0 \n\n You can download the toolkit here:  \n\\- GitHub: [https://github.com/opendr-eu/opendr](https://github.com/opendr-eu/opendr)  \n\\- pip: [https://pypi.org/project/opendr-toolkit/](https://pypi.org/project/opendr-toolkit/)  \n\\- Docker Hub: [https://hub.docker.com/r/opendr/opendr-toolkit/tags](https://hub.docker.com/r/opendr/opendr-toolkit/tags) \n\nLooking forward for your comments and suggestions!","link":"https://www.reddit.com/r/MachineLearning/comments/11nrako/d_version_21_of_the_open_deep_learning_toolkit/","created":"2023-03-10","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[Discussion] Are projects like ggml the most realistic way to make custom LLM training, fine-tuning and inference accessible?","description":"I'm not in the ML field (or even CS, tbh), but I've been using it more and more for for some business applications. Recently I even fine-tuned GPT-2 for some classification tasks :) The technology is amazing, but I've been concerned about the prohibitive hardware requirements. Everything I've been doing is on Colab, but ideally, I'd like to be able to run these things locally.\n\nI came across ggml recently, and was able to get Whisper running on my laptop in less than 30 min - - and I thought, wow this is what ML inference -could- be, if more people worked on projects like these. Waiting for top-of-the-line graphics cards to become affordable may be too long a wait, but if a model like GPT-J can run on CPU and RAM, that just might be a way out...?\n\nWhat are your thoughts on this? Are there other projects like GGML with similar goals? CPU+RAM is so much cheaper than the GPU route, if more people made this a focus, it might be a game changer.","link":"https://www.reddit.com/r/MachineLearning/comments/11ocrog/discussion_are_projects_like_ggml_the_most/","created":"2023-03-11","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[N] GPT-4 is coming next week \u2013 and it will be multimodal, says Microsoft Germany - heise online","description":"[https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html](https://www.heise.de/news/GPT-4-is-coming-next-week-and-it-will-be-multimodal-says-Microsoft-Germany-7540972.html)\n\n&gt;**GPT-4 is coming next week**: at an approximately one-hour hybrid information event entitled \"**AI in Focus - Digital Kickoff\" on 9 March 2023**, four Microsoft Germany employees presented Large Language Models (LLM) like GPT series as a disruptive force for companies and their Azure-OpenAI offering in detail. The kickoff event took place in the German language, news outlet Heise was present. **Rather casually, Andreas Braun, CTO Microsoft Germany** and Lead Data &amp; AI STU, **mentioned** what he said was **the imminent release of GPT-4.** The fact that **Microsoft is fine-tuning multimodality with OpenAI should no longer have been a secret since the release of Kosmos-1 at the beginning of March.**\n\n[ Dr. Andreas Braun, CTO Microsoft Germany and Lead Data  &amp; AI STU at the Microsoft Digital Kickoff: \\\\\"KI im Fokus\\\\\" \\(AI in  Focus, Screenshot\\) \\(Bild:\u00a0Microsoft\\) ](https://preview.redd.it/rnst03avarma1.jpg?width=1920&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=c398017ac69b7dda4c95f0d0ee28aa3a37893b90)","link":"https://www.reddit.com/r/MachineLearning/comments/11mzqxu/n_gpt4_is_coming_next_week_and_it_will_be/","created":"2023-03-09","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":75}}
{"title":"[D] What are the Inputs to a Model That Plays Dynamic RTS Games Like StarCraft?","description":"I am familiar with writing networks to play games that have very defined inputs such as Snake or tic tac toe. But what are the inputs for games where units and buildings are constantly being spawned/destroyed? I assume the amount of parameters in the input layer cant be dynamically changing so how do the models handle this? Whats the input difference from a game state with 5 enemies revealed vs. a game state with 100 units revealed?\n\nI assume there is a lot of \"hand waiving\" going on in the input layer and its not getting the position of every unit in the game but Im not sure. Any insight would be great!","link":"https://www.reddit.com/r/MachineLearning/comments/11nyeem/d_what_are_the_inputs_to_a_model_that_plays/","created":"2023-03-10","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[D] One Shot Learning Tasks","description":"From my understanding, a one shot learning task requires us that given a query example, we must classify it correctly out of N different classes (typically N = 5 way or 20 way). The goal however is that we are provided with only one example per class.\n\nSuppose we take an MNIST type dataset. I can map every pixel that makes up the digit onto a cartesian plane where the xy coordinates values is every \"pixel\". Using this cartesian representation, can I just find the simple distance metric between the pairs? For example on a 20 way task, My question is: At each iteration, we are provided with some query example, along with 20 other candidates...if we compute some sort of simple similarity score (that doesnt require neural nets) like (intersection over union) between each candidate to query pair, does this still count as a one shot learning task?\n\nSo leaving aside a neural network approach, if we were to just use a simple distance metric on the coordinates to compute the pairwise similarity between the query and every \"candidate\", does this count as one shot learning?","link":"https://www.reddit.com/r/MachineLearning/comments/11o8tgd/d_one_shot_learning_tasks/","created":"2023-03-11","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"[R] RODIN: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion","description":"We, the team from Microsoft Research, propose a diffusion-based generative model to automatically produces highly detailed 3D digital avatars. The generated avatars can be freely viewed in 360 degrees with unprecedented quality. The model significantly accelerates the traditionally sophisticated 3D modeling process and opens new opportunities for 3D artists. The work has been accepted to CVPR 2022.\n\nProject page: [https://3d-avatar-diffusion.microsoft.com/](https://3d-avatar-diffusion.microsoft.com/)\n\nArxiv paper link: [https://arxiv.org/abs/2212.06135](https://arxiv.org/abs/2212.06135)\n\n[360-degree renderable avatar](https://reddit.com/link/11njhnz/video/3bhbf5x7evma1/player)\n\nOne can use a user-given image or natural language prompt to produce a personalized avatar.\n\n[Text-conditioned avatar generation.](https://preview.redd.it/yp32l7u6fvma1.png?width=2778&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=e8699c81e0750084209c2d2f6b94a7df117fcf78)\n\nWhile this work is validated on 3D avatar generation, as a broader impact, we hope this work paves the way toward building a 3D generative foundation model for general 3D objects.","link":"https://www.reddit.com/r/MachineLearning/comments/11njhnz/r_rodin_a_generative_model_for_sculpting_3d/","created":"2023-03-10","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":3}}
{"title":"[D] What Improvements Accelerate the AI field Multiple orders of magnitude every year?","description":"These are just my perspectives, I am curious to hear how other people see it in the comments.\n\n  \nFrom my perspective there are the following improvements that accelerate AI reserch with multiple orders of magnitude every year:\n\n1.) Low barrier to entrance for researchers as hugging face, kaggle, google colab gives you free resources (CPU,RAM,GPU,TPU) to study\n\n2.) More efficient models: with smaller models reproducing similar results as larger counterpart a good example is Open AI DALL-E vs stable diffusion.\n\n3.) More efficient techniques: Ex changing computation from FP32 -&gt; FP 16 in Nvidia GPUs\n\n4.) Cleaner better labeled data by the community\n\n4.) More efficient underlying programing language optimizations\n\n5.) Rewritten more efficient code\n\n6.) New hardware\n\n7.) Special purpose hardware (while for gaming and other general purpose benchmarks there are 20-30% improvements every year or every 2 years) for AI reserch TENSOR cores (Nvidia GPUs, Google Cloud TPUs) or apple's Neural engines are orders of magnitude of speed improvement for AI models. Or many supercomputers are ARM based (that is not fully related to here but overall great architectural changes).\n\n8.) New hardware types: analog processors might make a comeback soon that helps calculate floating point operations faster for neural nets. (others: Intelligence Processing Unit, Hogel processing unit (HPU) )\n\n9.) Just the number of new professionals/researchers entering different fields of the AI game. University Majors, online courses, jobs ...\n\n10.) Money/funding.\n\n11.) Becoming culturally mainstream, non professionals realizing that they use it every day.","link":"https://www.reddit.com/r/MachineLearning/comments/11o1vjw/d_what_improvements_accelerate_the_ai_field/","created":"2023-03-10","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":5}}
{"title":"[D] Bring back the old arXiv favicon","description":"I used to have friends who would encourage and guide me. They would sit in their house at the top of my pane, smiling so lively as I began to retrain. \n\nI would never evict them, even if they were lame. Because when I struggled with conda, I had a friend to complain. \n\nI liked it that way, we liked it that way. Now everything changed. My friends have been slain, replaced by two, scissoring boomerangs. \n\nNow I sit alone, my smiling friends are no more. Please bring them back, lest what should I live for.","link":"https://www.reddit.com/r/MachineLearning/comments/11o703k/d_bring_back_the_old_arxiv_favicon/","created":"2023-03-11","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":1}}
{"title":"Recent advances in multimodal models: What are your thoughts on chain of thoughts models? [D]","description":"Hi everyone,\n\nI'm interested in learning more about recent advances in multimodal models, particularly chain of thoughts models. I'm curious to know what people working in this field are most excited about and what ideas and papers have inspired them.\n\nSpecifically, I'm interested in learning about:\n\n- The latest research on multimodal models, especially chain of thoughts models\n- The challenges that researchers are currently facing when developing these models\n- How researchers are addressing these challenges\n- What researchers are most excited about when it comes to the potential applications of these models\n\nIf you work on multimodal models, I'd love to hear your thoughts and insights. What papers have been particularly inspiring or influential? What challenges are you currently facing, and how are you addressing them? What are you most excited about when it comes to the future of multimodal models?\n\nThank you in advance for your responses :)","link":"https://www.reddit.com/r/MachineLearning/comments/11nl766/recent_advances_in_multimodal_models_what_are/","created":"2023-03-10","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[P] Counterpoint - a generative model for Fugues and Chorales in the style of J.S. Bach (with samples)","description":"Samples can be found [here](https://soundcloud.com/loua19/sets/bach-ai-chorales) and [here](https://soundcloud.com/loua19/sets/bach-ai-fugues). See how they compare to the original [chorales](https://www.youtube.com/watch?v=rXZBxlVQkjE) and [fugues](https://www.youtube.com/watch?v=w76Bsxs6qvc).\n\nThe model uses a Transformer encoder architecture to complete partially corrupted sequences representations of music. A version of Gibbs sampling is then used to construct new music from scratch. The entire model was trained in under 30 minutes on a single Tesla V100 - really showcasing the efficiency of Transformers in general.\n\nNote that the fugue samples are seeded by the first three bars of an actual Bach fugue. The chorales are generated completely from scratch!\n\nFor more information on how it works - see the [GitHub repo](https://github.com/loua19/counterpoint) or follow me on [Twitter](https://twitter.com/loua42).","link":"https://www.reddit.com/r/MachineLearning/comments/11nrrx6/p_counterpoint_a_generative_model_for_fugues_and/","created":"2023-03-10","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":3}}
{"title":"[R] Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models","description":"","link":"https://www.reddit.com/gallery/11mlwty","created":"2023-03-09","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":30}}
{"title":"\"[Project]\" , \"[Discussion]\" Looking for suggestions for a model to use in an image similarity task","description":"I am currently working on my thesis on a dataset called DISC21. I am trying to achieve good results in the descriptor track (representing each image in a 256 dimensions vector). I tried to finetune ViTl16 (with only unfreezing the last 3 layers) model with only a subset of the training image due to my hardware limitations (I took 1000 original training images and generated 30 augmented images for each of them and started finetuning the model as if it was a classification task. after that I removed the dense layer I added for the 1000 class to extract features)\n\nI believe this training approach is wrong because I am training the model with augmented images without the model actually seeing the original image (the main goal of the dataset is to find the origin of an augmented image)\n\nI am asking for suggestions about how to train the model and also which model should i use. I attached the code I am using below. I would appreciate any suggestion as this thing is very very important to me.\n\n&amp;#x200B;\n\n&amp;#x200B;\n\n    def data_augment(image):\n        p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n        p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n        p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n        p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n        p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    \n        # Flips\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_flip_up_down(image)\n    \n        if p_spatial &gt; .75:\n            image = tf.image.transpose(image)\n    \n        # Rotates\n        if p_rotate &gt; .75:\n            image = tf.image.rot90(image, k=3)  # rotate 270\u00ba\n        elif p_rotate &gt; .5:\n            image = tf.image.rot90(image, k=2)  # rotate 180\u00ba\n        elif p_rotate &gt; .25:\n            image = tf.image.rot90(image, k=1)  # rotate 90\u00ba\n    \n        # Pixel-level transforms\n        if p_pixel_1 &gt;= .4:\n            image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n        if p_pixel_2 &gt;= .4:\n            image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n        if p_pixel_3 &gt;= .4:\n            image = tf.image.random_brightness(image, max_delta=.1)\n    \n        image = vit.preprocess_inputs(image)\n    \n        return image\n    \n    \n    IMAGE_SIZE = 400\n    BATCH_SIZE = 6\n    EPOCHS = 50\n    UNFREEZED_LAYERS = 3\n    IMAGES_DIRECTORY = r\"F:\\Thesis MORE ORGANIZED BGAD\\ImageXAugmentations\\1000x30 Double\"\n    SAVE_MODEL_PATH = r'F:\\Thesis MORE ORGANIZED BGAD\\Experiments\\Experiment 2/'\n    \n    x = datetime.now()\n    \n    vit_model = vit.vit_l16(\n        image_size=IMAGE_SIZE,\n        pretrained=True,\n        include_top=False,\n        pretrained_top=False)\n    \n    vit_model.summary()\n    print('------------------\\n')\n    \n    k = (len(vit_model.layers) - 2) - UNFREEZED_LAYERS\n    i = 1\n    \n    for layer in vit_model.layers[:]:\n        if i &gt; k:\n            i = i + 1\n            print(layer, layer.trainable, layer.name)\n            continue\n        layer.trainable = False\n        print(layer, layer.trainable, layer.name)\n        i = i + 1\n    \n    datagen = tf.keras.preprocessing.image.ImageDataGenerator(validation_split=0.3,\n                                                              preprocessing_function=data_augment)\n    \n    train_gen = datagen.flow_from_directory(IMAGES_DIRECTORY,\n                                            target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE, seed=1,\n                                            color_mode='rgb',\n                                            shuffle=True, class_mode='categorical', subset='training')\n    \n    validation_gen = datagen.flow_from_directory(IMAGES_DIRECTORY,\n                                                 target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=BATCH_SIZE, seed=1,\n                                                 color_mode='rgb',\n                                                 shuffle=False, class_mode='categorical', subset='validation')\n    \n    model = Sequential([vit_model,\n                        keras.layers.Dense(1000, 'softmax')\n                        ])\n    \n    learning_rate = 1e-4\n    optimizer = tfa.optimizers.RectifiedAdam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer,\n                  loss=CategoricalCrossentropy(label_smoothing=0.2),\n                  metrics=['accuracy'])\n    \n    STEP_SIZE_TRAIN = train_gen.n // train_gen.batch_size\n    STEP_SIZE_VAL = validation_gen.n // validation_gen.batch_size\n    \n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy',\n                                                     factor=0.2,\n                                                     patience=1,\n                                                     verbose=1,\n                                                     min_delta=1e-4,\n                                                     min_lr=1e-6,\n                                                     mode='max')\n    \n    earlyStopping = EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=2, mode='max', restore_best_weights=True,\n                                  verbose=1)\n    \n    checkpointer = ModelCheckpoint(filepath=SAVE_MODEL_PATH + '/model ' + str(BATCH_SIZE) + ' ' + str(IMAGE_SIZE)\n                                            + ' ' + 'Layers Unfreezed '\n                                            + str(UNFREEZED_LAYERS) + '.hdf5',\n                                   monitor='val_accuracy',\n                                   verbose=1,\n                                   save_weights_only=False,\n                                   save_best_only=True,\n                                   mode='max')\n    \n    csv_logger = CSVLogger(SAVE_MODEL_PATH + 'log.csv', append=True, separator=';')\n    \n    callbacks = [earlyStopping, checkpointer, reduce_lr, csv_logger]\n    \n    model.summary()\n    \n    model.fit(x=train_gen, steps_per_epoch=STEP_SIZE_TRAIN, validation_data=validation_gen,\n              validation_steps=STEP_SIZE_VAL, epochs=EPOCHS, callbacks=callbacks)\n    \n    model.save(SAVE_MODEL_PATH + 'FullModel.h5')\n    \n    y = datetime.now()\n    \n    send_email()\n    \n    ############################################\n    \n    # Query Phase One\n    \n    import tensorflow as tf\n    import vit_keras.layers\n    import vit_keras.vit\n    import os\n    from datetime import datetime\n    import cv2\n    import numpy as np\n    from keras import Sequential\n    from keras.engine.input_layer import InputLayer\n    from vit_keras import vit, utils\n    from keras.models import Model\n    \n    IMAGE_SIZE = 400\n    \n    tempModel = tf.keras.models.load_model(r\"F:\\Thesis MORE ORGANIZED BGAD\\Experiments\\Experiment 2/FullModel.h5\",\n                                           custom_objects={'ClassToken': vit_keras.layers.ClassToken})\n    tempModel.summary()\n    \n    model = Model(inputs=tempModel.get_layer('vit-l16').layers[0].input,\n                  outputs=tempModel.get_layer('vit-l16').layers[-1].output)\n    \n    model.summary()\n    \n    print('Started at ' + str(datetime.now()))\n    \n    FolderPath = r\"F:\\Thesis MORE ORGANIZED BGAD\\Dataset Samples\\10k Phase One Query Derived From Reference/\"\n    CsvPath = r'F:\\Thesis MORE ORGANIZED BGAD\\Experiments\\Experiment 2/query phase one.csv'\n    \n    # model.summary()\n    \n    \n    res = []\n    \n    count = 0\n    # csvfile = open(csvPath + filename +  str(fileIndex) + '.csv', 'a')\n    \n    for image in os.listdir(FolderPath):\n        try:\n            print('Image: ' + str(count) + '\\n')\n            temp_img = cv2.imread(FolderPath + image, flags=cv2.IMREAD_GRAYSCALE)\n            temp_img = cv2.resize(temp_img, (IMAGE_SIZE, IMAGE_SIZE))\n            temp_img = np.expand_dims(temp_img, -1)\n            temp_img = temp_img.repeat(3, axis=-1)\n    \n            temp_img = vit.preprocess_inputs(temp_img).reshape(1, IMAGE_SIZE, IMAGE_SIZE, 3)\n            # np.savetxt(csvfile, model.predict(temp_img), delimiter=',', fmt='%s')\n    \n            modelres = model.predict(temp_img)\n            res.append(np.append(image[0:-4], modelres))\n            count = count + 1\n        except:\n            print('ex')\n    \n    tempres = np.array(res)\n    tempres = tempres.reshape(10000, 1025)\n    np.savetxt(CsvPath, tempres, delimiter=\",\", fmt=\"%s\")\n    \n    print('Ended at: ' + str(datetime.now()))\n    \n    send_email()\n    \n    #############################\n    \n    # Ref Phase One\n    \n    import tensorflow as tf\n    import vit_keras.layers\n    import vit_keras.vit\n    import os\n    from datetime import datetime\n    import cv2\n    import numpy as np\n    from keras import Sequential\n    from keras.engine.input_layer import InputLayer\n    from vit_keras import vit, utils\n    from keras.models import Model\n    \n    IMAGE_SIZE = 400\n    \n    tempModel = tf.keras.models.load_model(r\"F:\\Thesis MORE ORGANIZED BGAD\\Experiments\\Experiment 2/FullModel.h5\",\n                                           custom_objects={'ClassToken': vit_keras.layers.ClassToken})\n    tempModel.summary()\n    \n    model = Model(inputs=tempModel.get_layer('vit-l16').layers[0].input,\n                  outputs=tempModel.get_layer('vit-l16').layers[-1].output)\n    \n    model.summary()\n    \n    print('Started at ' + str(datetime.now()))\n    \n    FolderPath = r\"F:\\Thesis MORE ORGANIZED BGAD\\Dataset Samples\\10k Reference Converted To Phase 1 10k Query/\"\n    CsvPath = r'F:\\Thesis MORE ORGANIZED BGAD\\Experiments\\Experiment 2/ref phase one.csv'\n    \n    # model.summary()\n    \n    \n    res = []\n    \n    count = 0\n    # csvfile = open(csvPath + filename +  str(fileIndex) + '.csv', 'a')\n    \n    for image in os.listdir(FolderPath):\n        try:\n            print('Image: ' + str(count) + '\\n')\n            temp_img = cv2.imread(FolderPath + image, flags=cv2.IMREAD_GRAYSCALE)\n            temp_img = cv2.resize(temp_img, (IMAGE_SIZE, IMAGE_SIZE))\n            temp_img = np.expand_dims(temp_img, -1)\n            temp_img = temp_img.repeat(3, axis=-1)\n    \n            temp_img = vit.preprocess_inputs(temp_img).reshape(1, IMAGE_SIZE, IMAGE_SIZE, 3)\n            # np.savetxt(csvfile, model.predict(temp_img), delimiter=',', fmt='%s')\n    \n            modelres = model.predict(temp_img)\n            res.append(np.append(image[0:-4], modelres))\n            count = count + 1\n        except:\n            print('ex')\n    \n    tempres = np.array(res)\n    tempres = tempres.reshape(10000, 1025)\n    np.savetxt(CsvPath, tempres, delimiter=\",\", fmt=\"%s\")\n    \n    print('Ended at: ' + str(datetime.now()))\n    \n    send_email()\n    \n    #######################\n    \n    # Query Phase Two\n    \n    import tensorflow as tf\n    import vit_keras.layers\n    import vit_keras.vit\n    import os\n    from datetime import datetime\n    import cv2\n    import numpy as np\n    from keras import Sequential\n    from keras.engine.input_layer import InputLayer\n    from vit_keras import vit, utils\n    from keras.models import Model\n    \n    IMAGE_SIZE = 400\n    \n    tempModel = tf.keras.models.load_model(r\"F:\\Thesis MORE ORGANIZED BGAD\\Experiments\\Experiment 2/FullModel.h5\",\n                                           custom_objects={'ClassToken': vit_keras.layers.ClassToken})\n    tempModel.summary()\n    \n    model = Model(inputs=tempModel.get_layer('vit-l16').layers[0].input,\n                  outputs=tempModel.get_layer('vit-l16').layers[-1].output)\n    \n    model.summary()\n    \n    print('Started at ' + str(datetime.now()))\n    \n    FolderPath = r\"F:\\Thesis MORE ORGANIZED BGAD\\Dataset Samples\\10k Phase Two Query Derived From Reference/\"\n    CsvPath = r'F:\\Thesis MORE ORGANIZED BGAD\\Experiments\\Experiment 2/query phase two.csv'\n    \n    # model.summary()\n    \n    \n    res = []\n    \n    count = 0\n    # csvfile = open(csvPath + filename +  str(fileIndex) + '.csv', 'a')\n    \n    for image in os.listdir(FolderPath):\n        try:\n            print('Image: ' + str(count) + '\\n')\n            temp_img = cv2.imread(FolderPath + image, flags=cv2.IMREAD_GRAYSCALE)\n            temp_img = cv2.resize(temp_img, (IMAGE_SIZE, IMAGE_SIZE))\n            temp_img = np.expand_dims(temp_img, -1)\n            temp_img = temp_img.repeat(3, axis=-1)\n    \n            temp_img = vit.preprocess_inputs(temp_img).reshape(1, IMAGE_SIZE, IMAGE_SIZE, 3)\n            # np.savetxt(csvfile, model.predict(temp_img), delimiter=',', fmt='%s')\n    \n            modelres = model.predict(temp_img)\n            res.append(np.append(image[0:-4], modelres))\n            count = count + 1\n        except:\n            print('ex')\n    \n    tempres = np.array(res)\n    tempres = tempres.reshape(10000, 1025)\n    np.savetxt(CsvPath, tempres, delimiter=\",\", fmt=\"%s\")\n    \n    print('Ended at: ' + str(datetime.now()))\n    \n    send_email()\n    \n    ############################\n    \n    # Ref Phase Two\n    \n    import tensorflow as tf\n    import vit_keras.layers\n    import vit_keras.vit\n    import os\n    from datetime import datetime\n    import cv2\n    import numpy as np\n    from keras import Sequential\n    from keras.engine.input_layer import InputLayer\n    from vit_keras import vit, utils\n    from keras.models import Model\n    \n    IMAGE_SIZE = 400\n    \n    tempModel = tf.keras.models.load_model(r\"F:\\Thesis MORE ORGANIZED BGAD\\Experiments\\Experiment 2/FullModel.h5\",\n                                           custom_objects={'ClassToken': vit_keras.layers.ClassToken})\n    tempModel.summary()\n    \n    model = Model(inputs=tempModel.get_layer('vit-l16').layers[0].input,\n                  outputs=tempModel.get_layer('vit-l16').layers[-1].output)\n    \n    model.summary()\n    \n    print('Started at ' + str(datetime.now()))\n    \n    FolderPath = r\"F:\\Thesis MORE ORGANIZED BGAD\\Dataset Samples\\10k Reference Converted To Phase 2 10k Query/\"\n    CsvPath = r'F:\\Thesis MORE ORGANIZED BGAD\\Experiments\\Experiment 2/ref phase two.csv'\n    \n    # model.summary()\n    \n    \n    res = []\n    \n    count = 0\n    # csvfile = open(csvPath + filename +  str(fileIndex) + '.csv', 'a')\n    \n    for image in os.listdir(FolderPath):\n        try:\n            print('Image: ' + str(count) + '\\n')\n            temp_img = cv2.imread(FolderPath + image, flags=cv2.IMREAD_GRAYSCALE)\n            temp_img = cv2.resize(temp_img, (IMAGE_SIZE, IMAGE_SIZE))\n            temp_img = np.expand_dims(temp_img, -1)\n            temp_img = temp_img.repeat(3, axis=-1)\n    \n            temp_img = vit.preprocess_inputs(temp_img).reshape(1, IMAGE_SIZE, IMAGE_SIZE, 3)\n            # np.savetxt(csvfile, model.predict(temp_img), delimiter=',', fmt='%s')\n    \n            modelres = model.predict(temp_img)\n            res.append(np.append(image[0:-4], modelres))\n            count = count + 1\n        except:\n            print('ex')\n    \n    tempres = np.array(res)\n    tempres = tempres.reshape(10000, 1025)\n    np.savetxt(CsvPath, tempres, delimiter=\",\", fmt=\"%s\")\n    \n    print('Ended at: ' + str(datetime.now()))\n    \n    send_email()\n    \n    #########################\n    \n    #    Roo7 b2a 4eel 2awel column men kol excel feehom 34an ne3mel el 5atwa ely b3daha\n    \n    #########################\n    \n    # bne3mel generate lel h5 file 34an eval metrics PHASE ONE\n    \n    import numpy as np\n    import h5py\n    import pandas as pd\n    \n    f = h5py.File(r\"F:\\Thesis MORE ORGANIZED BGAD\\Experiments\\Experiment 2\" + \"/Submission Phase One.h5\", \"w\")\n    \n    queryDataset = pd.read_csv(\n        r\"F:\\Thesis MORE ORGANIZED BGAD\\Experiments\\Experiment 2\\query phase one.csv\",\n        dtype='float32', header=None).to_numpy()\n    \n    referenceDataset = pd.read_csv(r\"F:\\Thesis MORE ORGANIZED BGAD\\Experiments\\Experiment 2\\ref phase one.csv\",\n                                   dtype='float32', header=None).to_numpy()\n    \n    queryDatasetIDs = pd.read_csv(r\"F:\\Thesis MORE ORGANIZED BGAD\\Dataset CSVs\\10k Query Phase One IDs.csv\",\n                                  header=None).to_numpy()\n    referenceDatasetIDs = pd.read_csv(r\"F:\\Thesis MORE ORGANIZED BGAD\\Dataset CSVs\\10k Reference Phase One IDs.csv\",\n                                      header=None).to_numpy()\n    \n    queryDatasetIDs = queryDatasetIDs.reshape(10000)\n    referenceDatasetIDs = referenceDatasetIDs.reshape(10000)\n    \n    f.create_dataset(\"query\", data=queryDataset)\n    f.create_dataset(\"reference\", data=referenceDataset)\n    f.create_dataset(\"query_id\", data=queryDatasetIDs)\n    f.create_dataset(\"reference_id\", data=referenceDatasetIDs)\n    \n    f.close()\n    \n    #########################\n    \n    # bne3mel generate lel h5 file 34an eval metrics PHASE TWO\n    \n    import numpy as np\n    import h5py\n    import pandas as pd\n    \n    f = h5py.File(r\"F:\\Thesis MORE ORGANIZED BGAD\\Experiments\\Experiment 2\" + \"/Submission Phase Two.h5\", \"w\")\n    \n    queryDataset = pd.read_csv(\n        r\"F:\\Thesis MORE ORGANIZED BGAD\\Experiments\\Experiment 2\\query phase two.csv\",\n        dtype='float32', header=None).to_numpy()\n    \n    referenceDataset = pd.read_csv(r\"F:\\Thesis MORE ORGANIZED BGAD\\Experiments\\Experiment 2\\ref phase two.csv\",\n                                   dtype='float32', header=None).to_numpy()\n    \n    queryDatasetIDs = pd.read_csv(r\"F:\\Thesis MORE ORGANIZED BGAD\\Dataset CSVs\\10k Query Phase Two IDs.csv\",\n                                  header=None).to_numpy()\n    referenceDatasetIDs = pd.read_csv(r\"F:\\Thesis MORE ORGANIZED BGAD\\Dataset CSVs\\10k Reference Phase Two IDs.csv\",\n                                      header=None).to_numpy()\n    \n    queryDatasetIDs = queryDatasetIDs.reshape(10000)\n    referenceDatasetIDs = referenceDatasetIDs.reshape(10000)\n    \n    f.create_dataset(\"query\", data=queryDataset)\n    f.create_dataset(\"reference\", data=referenceDataset)\n    f.create_dataset(\"query_id\", data=queryDatasetIDs)\n    f.create_dataset(\"reference_id\", data=referenceDatasetIDs)\n    \n    f.close()\n    \n    ########################\n    \n    # Roo7 l eval_metrics.py","link":"https://www.reddit.com/r/MachineLearning/comments/11nwtdn/project_discussion_looking_for_suggestions_for_a/","created":"2023-03-10","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[D] Neuron Modeling","description":"Disclaimer : I am just a SWE who only knows some basic concepts of NN and ML, so I might be talking total garbage here.\n\nRecently, I read the news that the organoid made from brain cells can now play a simple game. Since it was made from the real neurons, it was way more efficient in learning.\n\nIf we think about it, our brain is very small and consumes comparably lower power, but still we are pretty smarter than the most of ai models powered by 1000s of gpus.\n\nI was wondering if there are any interesting research papers that actually try to model a human neuron. Btw I am not talking about a neural network itself. I feel like we are over simplifying a neuron as just a number while it can be an object that contains interesting features of our real neurons.\n\nI would really appreciate it if anyone could recommend any related research papers to read!","link":"https://www.reddit.com/r/MachineLearning/comments/11ned6g/d_neuron_modeling/","created":"2023-03-10","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":10}}
{"title":"[D] Subreddit with AI tools only","description":"I created a subreddit where I post a new AI tool every hour. I thought it would be useful to gather them all in one place on Reddit, so they don't get lost among the multitude of AI subreddits and topics: [https://www.reddit.com/r/AItoolsCatalog/](https://www.reddit.com/r/AItoolsCatalog/)\n\nIf you have an amazing project that you'd like to share or if you want to suggest one that in your opinion should be included, feel free to do so.","link":"https://www.reddit.com/r/MachineLearning/comments/11nzhdy/d_subreddit_with_ai_tools_only/","created":"2023-03-10","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":0}}
{"title":"[D] JAX vs PyTorch in 2023","description":"I've recently started my Ph.D. in Multi-Agent RL, and want to learn JAX/Flax and use that for my research, the reason being that DeepMind/Google use it, and I want to land an internship/job there at some point.\n\nI have been using PyTorch for 2.5 years, and in the past few days, I've been struggling to make the switch to JAX/Flax. Although the ideas behind JAX are cool, I feel like they make it unnecessarily complicated, and I would just be better off if I simply kept using PyTorch since I'm very familiar with it.\n\nI had tried to learn JAX 1-2 years ago already, and I came to the same conclusion back then, which makes me think that the usability of JAX hasn't improved much.\n\nDo you think it's worth it to make a serious effort this time to learn JAX, so that I will be able to use it for the rest of my Ph.D., or is there just no point in doing so and I should keep using PyTorch?","link":"https://www.reddit.com/r/MachineLearning/comments/11myoug/d_jax_vs_pytorch_in_2023/","created":"2023-03-09","tags":["reddit","machinelearning","ml"],"meta":{"num_comments":38}}
