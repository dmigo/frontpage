{"title":"Identify custom labels as well as existing labels with Spacy v3","description":"Hi all,\n\nI want to train a model with custom labels, and use it in combination with a pretrained model in Spacy v3.\n\nFor example for this code:\n\n    import spacy\n    import random\n    import json\n    \n    # Load the spaCy NLP model\n    nlp = spacy.load('en_core_web_lg')\n    \n    # Define the training data\n    train_data = [\n        ('These tomatoes are red and tasty.', {'entities': [(6, 14, 'VEGETABLE')]}),\n        ('I like red tomatoes.', {'entities': [(11, 19, 'VEGETABLE')]}),\n       \n        ('These bananas are very green.', {'entities': [(6, 13, 'FRUIT')]}),\n        ('Where are my bananas?', {'entities': [(13, 20, 'FRUIT')]}),\n        ('Are there any bananas near?', {'entities': [(14, 21, 'FRUIT')]}),    \n    ]\n    \n    # Define the new entity labels\n    new_labels = [\"FRUIT\", \"VEGETABLE\"]\n    \n    # Add the new labels to the existing entity recognizer\n    ner = nlp.get_pipe(\"ner\")\n    for label in new_labels:\n        ner.add_label(label)\n    \n    # Set up the optimizer\n    #optimizer = nlp.begin_training()\n    optimizer = nlp.initialize()\n    \n    # Iterate over the training data and update the model\n    for i in range(10):\n        random.shuffle(train_data)\n        for text, annotations in train_data:\n            doc = nlp.make_doc(text)\n            example = spacy.training.Example.from_dict(doc, annotations)\n            nlp.update([example], sgd=optimizer)\n    \n    # Test the model\n    text = \"\"\"What kind of color have bananas &amp; tomatoes in London?\"\"\"\n    doc = nlp(text)\n    for ent in doc.ents:\n        print(ent.text, ent.label_)\n\nThe output is:\n\n    bananas FRUIT\n    tomatoes VEGETABLE\n\nThe custom labels are recognized, but why is \"London\" not recognized as \"GPE\"? How can I achieve it?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11nr3r2/identify_custom_labels_as_well_as_existing_labels/","created":"2023-03-10","tags":["reddit","languagetechnology","ml"],"meta":{"num_comments":0}}
{"title":"How to interpret actions","description":"Hey guys, I would like to be able to extract actions along with their objects. For example, in the sentence \"Paint all the walls red and hide all the doors and windows.\", I would like to extract the verbs \"paint\" and \"hide\", the objects \"walls, doors, windows\", the relationships \"paint-&gt;walls\", \"hide-&gt;doors, windows\", and the adverb relationship \"paint-&gt;red\".\n\nWhat tools/techniques would you suggest? Is deep learning the way to go?\n\n[Spacy](https://spacy.io/usage/rule-based-matching#dependencymatcher) and [Stanza](https://stanfordnlp.github.io/stanza/available\\_models.html) look promising, but I am not sure.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11nozbv/how_to_interpret_actions/","created":"2023-03-10","tags":["reddit","languagetechnology","ml"],"meta":{"num_comments":3}}
{"title":"Training Transformer Networks in Scikit-Learn?!","description":"Have you ever wanted to use handy scikit-learn functionalities with your neural networks, but couldn\u2019t because TensorFlow models are not compatible with the scikit-learn API?\n\nI\u2019m excited to introduce one-line wrappers for TensorFlow/Keras models that enable you to use TensorFlow models within scikit-learn workflows with features like Pipeline, GridSearch, and more.\n\nTransformers are extremely popular for modeling text nowadays with GPT3, ChatGPT, Bard, PaLM, FLAN excelling for conversational AI and other Transformers like T5 &amp; BERT excelling for text classification. Scikit-learn offers a broadly useful suite of features for classifier models, but these are hard to use with Transformers. However not if you use these wrappers we developed, which only require changing one line of code to make your existing Tensorflow/Keras model compatible with scikit-learn\u2019s rich ecosystem!\n\nAll you have to do is swap `keras.Model` \u2192 `KerasWrapperModel`, or `keras.Sequential` \u2192 `KerasSequentialWrapper`. The wrapper objects have all the same methods as their keras counterparts, plus you can use them with tons of awesome scikit-learn methods.\n\nYou can find a demo jupyter notebook and read more about the wrappers here: [https://cleanlab.ai/blog/transformer-sklearn/](https://cleanlab.ai/blog/transformer-sklearn/)","link":"https://www.reddit.com/r/LanguageTechnology/comments/11mzctf/training_transformer_networks_in_scikitlearn/","created":"2023-03-09","tags":["reddit","languagetechnology","ml"],"meta":{"num_comments":0}}
{"title":"Can NLP be used to categorize individuals based on responses?","description":"Hi all,\n\nNew to the field of machine learning and have a dataset with survey responses. I was wondering if NLP can be utilized to categorize individuals based on their responses (approximately 2-3 categories), or is this something better for another domain of machine learning?\n\nIt seems like NLP is more for language generation and interaction. I haven't found much with a couple of quick Google searches around categorization, which makes me think it likely isn't role but just want to check.\n\nThank you!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11n56np/can_nlp_be_used_to_categorize_individuals_based/","created":"2023-03-09","tags":["reddit","languagetechnology","ml"],"meta":{"num_comments":11}}
{"title":"Computational Linguist looking to expand","description":"Hello,\n\nI\u2019m in between jobs right now and looking to expand my career. I\u2019ve held about 4-5 jobs as a computational linguist. It remains my strong suit but I\u2019m also realizing that there are very few jobs for compling. Last role I interviewed for was for an nlp engineer and I realized I\u2019m falling short for anything after building a prototype. I\u2019m looking to get back into \u201cstudying\u201d and considering MLOps or Data Science or MBA as I have held two roles as a product manager too (of language technologies) so may be time to explore that area too. My preference is definitely engineering over product management but I wanted to hear people\u2019s opinion on what/ how to stay relevant to the language technology domain.\n\nThanks for reading!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11mvjhs/computational_linguist_looking_to_expand/","created":"2023-03-09","tags":["reddit","languagetechnology","ml"],"meta":{"num_comments":2}}
{"title":"Wanna Get Training Datasets For Social media spam classifier","description":"I am planning to build social media's spam classifier with Multinomial Naive Bayes model with python, using \\`sklearn\\` and \\`spacy\\` library. And the text feature extraction technique I will use is tf-idf vectorizer.\n\nHowever, I am having problem to find social media datasets with labelled data as SPAM or NOT SPAM. Another criteria with the datasets is that I need the datasets to be balanced (with roughly equal number of SPAM and NOT SPAM data).\n\nDo suggest me some links or source that I could get the data from?\n\nHope for help. Thanks in advance.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11muxkl/wanna_get_training_datasets_for_social_media_spam/","created":"2023-03-09","tags":["reddit","languagetechnology","ml"],"meta":{"num_comments":0}}
{"title":"[Beginner] Any tips/resources on where should I start?","description":"I would like to create a simple chatbot where user would ask a school-related question (e.g., when is the enrollment) and the response will be based on the answer column on the dataset.\n\nWhat I had in mind is to use Question Answering but without need to input the context.  The problem is most of the tutorials I found (HuggingFace) uses with the *'with context'* approach and my Dataset consist only question and answer columns.\n\nAny help or tutorials would greatly help.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11mims7/beginner_any_tipsresources_on_where_should_i_start/","created":"2023-03-09","tags":["reddit","languagetechnology","ml"],"meta":{"num_comments":2}}
{"title":"Semantic Search: With Exclusions","description":"I am making a semantic search engine in Python that takes a user input and returns the 5 most similar results from a list of sentences. \n\nThe list of sentences features a list of things not included in the category at the end of a sentence e.g.   \u201cThis category includes: lions, tigers. This category excludes: birds, bees\u201d  \n\nCurrently if I search \u201cbirds\u201d the above example would be returned as strong similarity due to the word matching with \u201ccategory excludes: birds\u201d \n\nDoes anyone know any way to prevent this?\nAny help appreciated!!","link":"https://www.reddit.com/r/LanguageTechnology/comments/11m4niv/semantic_search_with_exclusions/","created":"2023-03-08","tags":["reddit","languagetechnology","ml"],"meta":{"num_comments":9}}
{"title":"Encoder-decoder architecture for POS tagging","description":"I understand following about encoder and decoder:\n\n&gt; An encoder is a network that takes the input, and output a feature map/vector/tensor. These feature vector hold the information, the features, that represents the input. The decoder is again a network that takes the feature vector from the encoder, and gives the best closest match to the actual input or intended output.\n\nI want to implement POS tagging with encoder and decoder. I can guess that we can use \"encoder-only\" model to do POS tagging. Can we use \"encoder-decoder\" architecture for POS tagging task? If yes, then how should I design it. Most importantly I am not able to get what input will the decoder get from the encoder.","link":"https://www.reddit.com/r/LanguageTechnology/comments/11m5rzs/encoderdecoder_architecture_for_pos_tagging/","created":"2023-03-08","tags":["reddit","languagetechnology","ml"],"meta":{"num_comments":3}}
{"title":"Testing Viterbi Algorithm for Hidden markov model pos tagger","description":"I am implementing the HMM model pos tagger using viterbi algorithm on the brown dataset from nltk. I have separated the data into train and test datasets, now for train dataset, I have calculated the emission and transition probability matrix. I have a few questions though.\n\n1. To calculate the accuracy, do we count the no. of correct tags on words or the no. of correctly tagged sentences? (my guess is it should be words)\n2. For testing data, I have some words which are not in the emission probability matrix, and hence for those sentences viterbi algorithm gives me an error. how do i handle this?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11luju0/testing_viterbi_algorithm_for_hidden_markov_model/","created":"2023-03-08","tags":["reddit","languagetechnology","ml"],"meta":{"num_comments":4}}
{"title":"How to get a Phd in NLP for protein/gene design ?","description":"I have a background in Biotechnology and am currently doing a MS in Bioinformatics. My research consists on natural language models like BERT and protein design I'm also working on data/text mining projects with Biomedical data.  I want to do a PHD  with a focus on NLP but I'm worried if I have enough knowhow to apply for them. Any suggestions how I should approach this?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11m5je0/how_to_get_a_phd_in_nlp_for_proteingene_design/","created":"2023-03-08","tags":["reddit","languagetechnology","ml"],"meta":{"num_comments":1}}
{"title":"Question about density plots for dimensionality reduced embeddings","description":"I have long form documents that each talk about a variety of topics (10+). The documents are split into paragraph, which is the unit (sub-) topics are talked about. For each paragraph an embedding is created via OpenAI (text-embedding-ada-002). Since the embeddings contain 1536 dimensions, I use UMAP to reduce it to two. \n\nWhat I would like to do is then use bivatiate kde plots via [seaborn](https://seaborn.pydata.org/tutorial/distributions.html) to compare the focus of the documents (each representing an organization) showing differences and commonalities. I don\u2018t have a strong background in mathematics but this part of the [documentation](https://umap-learn.readthedocs.io/en/latest/clustering.html) threw me a little of. While I am not directly clustering, the underlying idea seems similar enough to warrant caution. \n\nDoes anybody know if my idea (umap reduced embedding-&gt; kde plot) reasonably sound or have any pointers to fintune the approach?","link":"https://www.reddit.com/r/LanguageTechnology/comments/11m3jgo/question_about_density_plots_for_dimensionality/","created":"2023-03-08","tags":["reddit","languagetechnology","ml"],"meta":{"num_comments":0}}
