{"title":"Diversity-Aware Meta Visual Prompting","description":"We present Diversity-Aware Meta Visual Prompting~(DAM-VP), an efficient and effective prompting method for transferring pre-trained models to downstream tasks with frozen backbone. A challenging issue in visual prompting is that image datasets sometimes have a large data diversity whereas a per-dataset generic prompt can hardly handle the complex distribution shift toward the original pretraining data distribution properly. To address this issue, we propose a dataset Diversity-Aware prompting strategy whose initialization is realized by a Meta-prompt. Specifically, we cluster the downstream dataset into small homogeneity subsets in a diversity-adaptive way, with each subset has its own prompt optimized separately. Such a divide-and-conquer design reduces the optimization difficulty greatly and significantly boosts the prompting performance. Furthermore, all the prompts are initialized with a meta-prompt, which is learned across several datasets. It is a bootstrapped paradigm, with the key observation that the prompting knowledge learned from previous datasets could help the prompt to converge faster and perform better on a new dataset. During inference, we dynamically select a proper prompt for each input, based on the feature distance between the input and each subset. Through extensive experiments, our DAM-VP demonstrates superior efficiency and effectiveness, clearly surpassing previous prompting methods in a series of downstream datasets for different pretraining models. Our code is available at: \\url{https://github.com/shikiw/DAM-VP}.","link":"http://arxiv.org/abs/2303.08138v1","created":"2023-03-14","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"FPUS23: An Ultrasound Fetus Phantom Dataset with Deep Neural Network Evaluations for Fetus Orientations, Fetal Planes, and Anatomical Features","description":"Ultrasound imaging is one of the most prominent technologies to evaluate the growth, progression, and overall health of a fetus during its gestation. However, the interpretation of the data obtained from such studies is best left to expert physicians and technicians who are trained and well-versed in analyzing such images. To improve the clinical workflow and potentially develop an at-home ultrasound-based fetal monitoring platform, we present a novel fetus phantom ultrasound dataset, FPUS23, which can be used to identify (1) the correct diagnostic planes for estimating fetal biometric values, (2) fetus orientation, (3) their anatomical features, and (4) bounding boxes of the fetus phantom anatomies at 23 weeks gestation. The entire dataset is composed of 15,728 images, which are used to train four different Deep Neural Network models, built upon a ResNet34 backbone, for detecting aforementioned fetus features and use-cases. We have also evaluated the models trained using our FPUS23 dataset, to show that the information learned by these models can be used to substantially increase the accuracy on real-world ultrasound fetus datasets. We make the FPUS23 dataset and the pre-trained models publicly accessible at https://github.com/bharathprabakaran/FPUS23, which will further facilitate future research on fetal ultrasound imaging and analysis.","link":"http://arxiv.org/abs/2303.07852v1","created":"2023-03-14","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"Inferential Privacy: From Impossibility to Database Privacy","description":"We investigate the possibility of guaranteeing inferential privacy for mechanisms that release useful information about some data containing sensitive information, denoted by $X$. We describe a general model of utility and privacy in which utility is achieved by disclosing the value of low-entropy features of $X$, while privacy is maintained by keeping high-entropy features of $X$ secret. Adopting this model, we prove that meaningful inferential privacy guarantees can be obtained, even though this is commonly considered to be impossible by the well-known result of Dwork and Naor. Then, we specifically discuss a privacy measure called pointwise maximal leakage (PML) whose guarantees are of the inferential type. We use PML to show that differential privacy admits an inferential formulation: it describes the information leaking about a single entry in a database assuming that every other entry is known, and considering the worst-case distribution on the data. Moreover, we define inferential instance privacy (IIP) as a bound on the (non-conditional) information leaking about a single entry in the database under the worst-case distribution, and show that it is equivalent to free-lunch privacy. Overall, our approach to privacy unifies, formalizes, and explains many existing ideas, e.g., why the informed adversary assumption may lead to underestimating the information leaking about each entry in the database. Furthermore, insights obtained from our results suggest general methods for improving privacy analyses; for example, we argue that smaller privacy parameters can be obtained by excluding low-entropy prior distributions from protection.","link":"http://arxiv.org/abs/2303.07782v1","created":"2023-03-14","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"ForDigitStress: A multi-modal stress dataset employing a digital job interview scenario","description":"We present a multi-modal stress dataset that uses digital job interviews to induce stress. The dataset provides multi-modal data of 40 participants including audio, video (motion capturing, facial recognition, eye tracking) as well as physiological information (photoplethysmography, electrodermal activity). In addition to that, the dataset contains time-continuous annotations for stress and occurred emotions (e.g. shame, anger, anxiety, surprise). In order to establish a baseline, five different machine learning classifiers (Support Vector Machine, K-Nearest Neighbors, Random Forest, Long-Short-Term Memory Network) have been trained and evaluated on the proposed dataset for a binary stress classification task. The best-performing classifier achieved an accuracy of 88.3% and an F1-score of 87.5%.","link":"http://arxiv.org/abs/2303.07742v1","created":"2023-03-14","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"BlinkFlow: A Dataset to Push the Limits of Event-based Optical Flow Estimation","description":"Event cameras provide high temporal precision, low data rates, and high dynamic range visual perception, which are well-suited for optical flow estimation. While data-driven optical flow estimation has obtained great success in RGB cameras, its generalization performance is seriously hindered in event cameras mainly due to the limited and biased training data. In this paper, we present a novel simulator, BlinkSim, for the fast generation of large-scale data for event-based optical flow. BlinkSim consists of a configurable rendering engine and a flexible engine for event data simulation. By leveraging the wealth of current 3D assets, the rendering engine enables us to automatically build up thousands of scenes with different objects, textures, and motion patterns and render very high-frequency images for realistic event data simulation. Based on BlinkSim, we construct a large training dataset and evaluation benchmark BlinkFlow that contains sufficient, diversiform, and challenging event data with optical flow ground truth. Experiments show that BlinkFlow improves the generalization performance of state-of-the-art methods by more than 40% on average and up to 90%. Moreover, we further propose an Event optical Flow transFormer (E-FlowFormer) architecture. Powered by our BlinkFlow, E-FlowFormer outperforms the SOTA methods by up to 91% on MVSEC dataset and 14% on DSEC dataset and presents the best generalization performance.","link":"http://arxiv.org/abs/2303.07716v1","created":"2023-03-14","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"V2V4Real: A Real-world Large-scale Dataset for Vehicle-to-Vehicle Cooperative Perception","description":"Modern perception systems of autonomous vehicles are known to be sensitive to occlusions and lack the capability of long perceiving range. It has been one of the key bottlenecks that prevents Level 5 autonomy. Recent research has demonstrated that the Vehicle-to-Vehicle (V2V) cooperative perception system has great potential to revolutionize the autonomous driving industry. However, the lack of a real-world dataset hinders the progress of this field. To facilitate the development of cooperative perception, we present V2V4Real, the first large-scale real-world multi-modal dataset for V2V perception. The data is collected by two vehicles equipped with multi-modal sensors driving together through diverse scenarios. Our V2V4Real dataset covers a driving area of 410 km, comprising 20K LiDAR frames, 40K RGB frames, 240K annotated 3D bounding boxes for 5 classes, and HDMaps that cover all the driving routes. V2V4Real introduces three perception tasks, including cooperative 3D object detection, cooperative 3D object tracking, and Sim2Real domain adaptation for cooperative perception. We provide comprehensive benchmarks of recent cooperative perception algorithms on three tasks. The V2V4Real dataset and codebase can be found at https://github.com/ucla-mobility/V2V4Real.","link":"http://arxiv.org/abs/2303.07601v1","created":"2023-03-14","tags":["arxiv","dataset"],"meta":{"query":"ti:dataset OR ti:corpus OR ti:database OR abs:\"a new dataset\""}}
{"title":"Half-Day Vulnerabilities: A study of the First Days of CVE Entries","description":"The National Vulnerability Disclosure Database is an invaluable source of information for security professionals and researchers. However, in some cases, a vulnerability report is initially published with incomplete information, a situation that complicates incident response and mitigation. In this paper, we perform an empirical study of vulnerabilities that are initially submitted with an incomplete report, and present key findings related to their frequency, nature, and the time needed to update them. We further present a novel ticketing process that is tailored to addressing the problems related to such vulnerabilities and demonstrate the use of this system with a real-life use case.","link":"http://arxiv.org/abs/2303.07990v1","created":"2023-03-14","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"Practically Solving LPN in High Noise Regimes Faster Using Neural Networks","description":"We conduct a systematic study of solving the learning parity with noise problem (LPN) using neural networks. Our main contribution is designing families of two-layer neural networks that practically outperform classical algorithms in high-noise, low-dimension regimes. We consider three settings where the numbers of LPN samples are abundant, very limited, and in between. In each setting we provide neural network models that solve LPN as fast as possible. For some settings we are also able to provide theories that explain the rationale of the design of our models. Comparing with the previous experiments of Esser, Kubler, and May (CRYPTO 2017), for dimension $n = 26$, noise rate $\\tau = 0.498$, the ''Guess-then-Gaussian-elimination'' algorithm takes 3.12 days on 64 CPU cores, whereas our neural network algorithm takes 66 minutes on 8 GPUs. Our algorithm can also be plugged into the hybrid algorithms for solving middle or large dimension LPN instances.","link":"http://arxiv.org/abs/2303.07987v1","created":"2023-03-14","tags":["arxiv","ml","security"],"meta":{"query":"machine AND learning AND security OR machine AND learning AND secure"}}
{"title":"Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions","description":"ChatGPT is a powerful large language model (LLM) that has made remarkable progress in natural language understanding. Nevertheless, the performance and limitations of the model still need to be extensively evaluated. As ChatGPT covers resources such as Wikipedia and supports natural language question answering, it has garnered attention as a potential replacement for traditional knowledge based question answering (KBQA) models. Complex question answering is a challenge task of KBQA, which comprehensively tests the ability of models in semantic parsing and reasoning. To assess the performance of ChatGPT as a question answering system (QAS) using its own knowledge, we present a framework that evaluates its ability to answer complex questions. Our approach involves categorizing the potential features of complex questions and describing each test question with multiple labels to identify combinatorial reasoning. Following the black-box testing specifications of CheckList proposed by Ribeiro et.al, we develop an evaluation method to measure the functionality and reliability of ChatGPT in reasoning for answering complex questions. We use the proposed framework to evaluate the performance of ChatGPT in question answering on 8 real-world KB-based CQA datasets, including 6 English and 2 multilingual datasets, with a total of approximately 190,000 test cases. We compare the evaluation results of ChatGPT, GPT-3.5, GPT-3, and FLAN-T5 to identify common long-term problems in LLMs. The dataset and code are available at https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT.","link":"http://arxiv.org/abs/2303.07992v1","created":"2023-03-14","tags":["arxiv","ml","prompteng"],"meta":{"query":"abs:\"chatgpt\""}}
{"title":"Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences","description":"As a natural language assistant, ChatGPT is capable of performing various tasks, including but not limited to article generation, code completion, and data analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable level of accuracy and reliability in terms of content evaluation, exhibiting the capability of mimicking human preferences. To further explore ChatGPT's potential in this regard, a study is conducted to assess its ability to rank content. In order to do so, a test set consisting of prompts is created, covering a wide range of use cases, and five models are utilized to generate corresponding responses. ChatGPT is then instructed to rank the responses generated by these models. The results on the test set show that ChatGPT's ranking preferences are consistent with human to a certain extent. This preliminary experimental finding implies that ChatGPT's zero-shot ranking capability could be used to reduce annotation pressure in a number of ranking tasks.","link":"http://arxiv.org/abs/2303.07610v1","created":"2023-03-14","tags":["arxiv","ml","prompteng"],"meta":{"query":"abs:\"chatgpt\""}}
{"title":"LayoutDM: Discrete Diffusion Model for Controllable Layout Generation","description":"Controllable layout generation aims at synthesizing plausible arrangement of element bounding boxes with optional constraints, such as type or position of a specific element. In this work, we try to solve a broad range of layout generation tasks in a single model that is based on discrete state-space diffusion models. Our model, named LayoutDM, naturally handles the structured layout data in the discrete representation and learns to progressively infer a noiseless layout from the initial input, where we model the layout corruption process by modality-wise discrete diffusion. For conditional generation, we propose to inject layout constraints in the form of masking or logit adjustment during inference. We show in the experiments that our LayoutDM successfully generates high-quality layouts and outperforms both task-specific and task-agnostic baselines on several layout tasks.","link":"http://arxiv.org/abs/2303.08137v1","created":"2023-03-14","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"The Random Hivemind: An Ensemble Deep Learner. A Case Study of Application to Solar Energetic Particle Prediction Problem","description":"Deep learning has become a popular trend in recent years in the machine learning community and has even occasionally become synonymous with machine learning itself thanks to its efficiency, malleability, and ability to operate free of human intervention. However, a series of hyperparameters passed to a conventional neural network (CoNN) may be rather arbitrary, especially if there is no surefire way to decide how to program hyperparameters for a given dataset. The random hivemind (RH) alleviates this concern by having multiple neural network estimators make decisions based on random permutations of features. The learning rate and the number of epochs may be boosted or attenuated depending on how all features of a given estimator determine the class that the numerical feature data belong to, but all other hyperparameters remain the same across estimators. This allows one to quickly see whether consistent decisions on a given dataset can be made by multiple neural networks with the same hyperparameters, with random subsets of data chosen to force variation in how data are predicted by each, placing the quality of the data and hyperparameters into focus. The effectiveness of RH is demonstrated through experimentation in the predictions of dangerous solar energetic particle events (SEPs) by comparing it to that of using both CoNN and the traditional approach used by ensemble deep learning in this application. Our results demonstrate that RH outperforms the CoNN and a committee-based approach, and demonstrates promising results with respect to the ``all-clear'' prediction of SEPs.","link":"http://arxiv.org/abs/2303.08092v1","created":"2023-03-14","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"Point Cloud Diffusion Models for Automatic Implant Generation","description":"Advances in 3D printing of biocompatible materials make patient-specific implants increasingly popular. The design of these implants is, however, still a tedious and largely manual process. Existing approaches to automate implant generation are mainly based on 3D U-Net architectures on downsampled or patch-wise data, which can result in a loss of detail or contextual information. Following the recent success of Diffusion Probabilistic Models, we propose a novel approach for implant generation based on a combination of 3D point cloud diffusion models and voxelization networks. Due to the stochastic sampling process in our diffusion model, we can propose an ensemble of different implants per defect, from which the physicians can choose the most suitable one. We evaluate our method on the SkullBreak and SkullFix datasets, generating high-quality implants and achieving competitive evaluation scores.","link":"http://arxiv.org/abs/2303.08061v1","created":"2023-03-14","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"Controllable Mesh Generation Through Sparse Latent Point Diffusion Models","description":"Mesh generation is of great value in various applications involving computer graphics and virtual content, yet designing generative models for meshes is challenging due to their irregular data structure and inconsistent topology of meshes in the same category. In this work, we design a novel sparse latent point diffusion model for mesh generation. Our key insight is to regard point clouds as an intermediate representation of meshes, and model the distribution of point clouds instead. While meshes can be generated from point clouds via techniques like Shape as Points (SAP), the challenges of directly generating meshes can be effectively avoided. To boost the efficiency and controllability of our mesh generation method, we propose to further encode point clouds to a set of sparse latent points with point-wise semantic meaningful features, where two DDPMs are trained in the space of sparse latent points to respectively model the distribution of the latent point positions and features at these latent points. We find that sampling in this latent space is faster than directly sampling dense point clouds. Moreover, the sparse latent points also enable us to explicitly control both the overall structures and local details of the generated meshes. Extensive experiments are conducted on the ShapeNet dataset, where our proposed sparse latent point diffusion model achieves superior performance in terms of generation quality and controllability when compared to existing methods.","link":"http://arxiv.org/abs/2303.07938v1","created":"2023-03-14","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"BLAT: Bootstrapping Language-Audio Pre-training based on AudioSet Tag-guided Synthetic Data","description":"Compared with ample visual-text pre-training research, few works explore audio-text pre-training, mostly due to the lack of sufficient parallel audio-text data. Most existing methods incorporate the visual modality as a pivot for audio-text pre-training, which inevitably induces data noise. In this paper, we propose BLAT: Bootstrapping Language-Audio pre-training based on Tag-guided synthetic data. We utilize audio captioning to generate text directly from audio, without the aid of the visual modality so that potential noise from modality mismatch is eliminated. Furthermore, we propose caption generation under the guidance of AudioSet tags, leading to more accurate captions. With the above two improvements, we curate high-quality, large-scale parallel audio-text data, based on which we perform audio-text pre-training. Evaluation on a series of downstream tasks indicates that BLAT achieves SOTA zero-shot classification performance on most datasets and significant performance improvement when fine-tuned on downstream tasks, suggesting the effectiveness of our synthetic data.","link":"http://arxiv.org/abs/2303.07902v1","created":"2023-03-14","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"Automated Self-Supervised Learning for Recommendation","description":"Graph neural networks (GNNs) have emerged as the state-of-the-art paradigm for collaborative filtering (CF). To improve the representation quality over limited labeled data, contrastive learning has attracted attention in recommendation and benefited graph-based CF model recently. However, the success of most contrastive methods heavily relies on manually generating effective contrastive views for heuristic-based data augmentation. This does not generalize across different datasets and downstream recommendation tasks, which is difficult to be adaptive for data augmentation and robust to noise perturbation. To fill this crucial gap, this work proposes a unified Automated Collaborative Filtering (AutoCF) to automatically perform data augmentation for recommendation. Specifically, we focus on the generative self-supervised learning framework with a learnable augmentation paradigm that benefits the automated distillation of important self-supervised signals. To enhance the representation discrimination ability, our masked graph autoencoder is designed to aggregate global information during the augmentation via reconstructing the masked subgraph structures. Experiments and ablation studies are performed on several public datasets for recommending products, venues, and locations. Results demonstrate the superiority of AutoCF against various baseline methods. We release the model implementation at https://github.com/HKUDS/AutoCF.","link":"http://arxiv.org/abs/2303.07797v1","created":"2023-03-14","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"Pressure study on the interplay between magnetic order and valence-change crossover in EuPd$_2$(Si$_{1-x}$Ge$_x$)$_2$","description":"We present results of the magnetic susceptibility on high-quality single crystals of EuPd$_2$(Si$_{1-x}$Ge$_x$)$_2$ for Ge concentrations 0 $\\leq x \\leq$ 0.105 performed under varying hydrostatic (He-gas) pressure 0 $\\leq p \\leq$ 0.5 GPa. The work extends on recent studies at ambient pressure demonstrating the drastic change in the magnetic response from valence-change-crossover behavior for $x$ = 0 and 0.058, to long-range antiferromagnetic (afm) order below $T_{\\text{N}}$ = 47 K for $x$ = 0.105. The valence-change-crossover temperature $T'_{\\text{V}}$ shows an extraordinarily strong pressure dependence of d$T'_{\\text{V}}$/d$p$ = +(80 $\\pm$ 10) K/GPa. In contrast, a very small pressure dependence of d$T_{\\text{N}}$/d$p \\leq$ +(1 $\\pm$ 0.5) K/GPa is found for the afm order upon pressurizing the $x$ = 0.105 crystal from $p$ = 0 to 0.05 GPa. Remarkably, by further increasing the pressure to 0.1 GPa, a drastic change in the ground state from afm order to valence-change-crossover behavior is observed. Estimates of the electronic entropy, derived from analyzing susceptibility data at varying pressures, indicate that the boundary between afm order and valence-change crossover represents a first-order phase transition. Our results suggest a particular type of second-order critical endpoint of the first-order transition for $x$ = 0.105 at $p_{\\text{cr}} \\approx$ 0.06 GPa and $T_{\\text{cr}} \\approx$ 45 K where intriguing strong-coupling effects between fluctuating charge-, spin- and lattice degrees of freedom can be expected.","link":"http://arxiv.org/abs/2303.07767v1","created":"2023-03-14","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"Image Blending with Osmosis","description":"Image blending is an integral part of many multi-image applications such as panorama stitching or remote image acquisition processes. In such scenarios, multiple images are connected at predefined boundaries to form a larger image. A convincing transition between these boundaries may be challenging, since each image might have been acquired under different conditions or even by different devices.   We propose the first blending approach based on osmosis filters. These drift-diffusion processes define an image evolution with a non-trivial steady state. For our blending purposes, we explore several ways to compose drift vector fields based on the derivatives of our input images. These vector fields guide the evolution such that the steady state yields a convincing blended result. Our method benefits from the well-founded theoretical results for osmosis, which include useful invariances under multiplicative changes of the colour values. Experiments on real-world data show that this yields better quality than traditional gradient domain blending, especially under challenging illumination conditions.","link":"http://arxiv.org/abs/2303.07762v1","created":"2023-03-14","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"Adaptive Policy Learning for Offline-to-Online Reinforcement Learning","description":"Conventional reinforcement learning (RL) needs an environment to collect fresh data, which is impractical when online interactions are costly. Offline RL provides an alternative solution by directly learning from the previously collected dataset. However, it will yield unsatisfactory performance if the quality of the offline datasets is poor. In this paper, we consider an offline-to-online setting where the agent is first learned from the offline dataset and then trained online, and propose a framework called Adaptive Policy Learning for effectively taking advantage of offline and online data. Specifically, we explicitly consider the difference between the online and offline data and apply an adaptive update scheme accordingly, that is, a pessimistic update strategy for the offline dataset and an optimistic/greedy update scheme for the online dataset. Such a simple and effective method provides a way to mix the offline and online RL and achieve the best of both worlds. We further provide two detailed algorithms for implementing the framework through embedding value or policy-based RL algorithms into it. Finally, we conduct extensive experiments on popular continuous control tasks, and results show that our algorithm can learn the expert policy with high sample efficiency even when the quality of offline dataset is poor, e.g., random dataset.","link":"http://arxiv.org/abs/2303.07693v1","created":"2023-03-14","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"Feature-Rich Audio Model Inversion for Data-Free Knowledge Distillation Towards General Sound Classification","description":"Data-Free Knowledge Distillation (DFKD) has recently attracted growing attention in the academic community, especially with major breakthroughs in computer vision. Despite promising results, the technique has not been well applied to audio and signal processing. Due to the variable duration of audio signals, it has its own unique way of modeling. In this work, we propose feature-rich audio model inversion (FRAMI), a data-free knowledge distillation framework for general sound classification tasks. It first generates high-quality and feature-rich Mel-spectrograms through a feature-invariant contrastive loss. Then, the hidden states before and after the statistics pooling layer are reused when knowledge distillation is performed on these feature-rich samples. Experimental results on the Urbansound8k, ESC-50, and audioMNIST datasets demonstrate that FRAMI can generate feature-rich samples. Meanwhile, the accuracy of the student model is further improved by reusing the hidden state and significantly outperforms the baseline method.","link":"http://arxiv.org/abs/2303.07643v1","created":"2023-03-14","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"Global efficiency and network structure of urban traffic flows: A percolation-based empirical analysis","description":"Making the connection between the function and structure of networked systems is one of fundamental issues in complex systems and network science. Urban traffic flows are related to various problems in cities and can be represented as a network of local traffic flows. To identify an empirical relation between the function and network structure of urban traffic flows, we construct a time-varying traffic flow network of a megacity, Seoul, and analyze its global efficiency with a percolation-based approach. Comparing the real-world traffic flow network with its corresponding null-model network having a randomized structure, we show that the real-world network is less efficient than its null-model network during rush hour, yet more efficient during non-rush hour. We observe that in the real-world network, links with the highest betweenness tend to have lower quality during rush hour compared to links with lower betweenness, but higher quality during non-rush hour. Since the top betweenness links tend to traverse the entire network, their congestion has a stronger impact on the network's global efficiency. Our results suggest that urban traffic congestion might arise when such backbone links are severely congested rather than the whole system is slowing down.","link":"http://arxiv.org/abs/2303.07633v1","created":"2023-03-14","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
{"title":"Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI","description":"Heart failure is a serious and life-threatening condition that can lead to elevated pressure in the left ventricle. Pulmonary Arterial Wedge Pressure (PAWP) is an important surrogate marker indicating high pressure in the left ventricle. PAWP is determined by Right Heart Catheterization (RHC) but it is an invasive procedure. A non-invasive method is useful in quickly identifying high-risk patients from a large population. In this work, we develop a tensor learning-based pipeline for identifying PAWP from multimodal cardiac Magnetic Resonance Imaging (MRI). This pipeline extracts spatial and temporal features from high-dimensional scans. For quality control, we incorporate an epistemic uncertainty-based binning strategy to identify poor-quality training samples. To improve the performance, we learn complementary information by integrating features from multimodal data: cardiac MRI with short-axis and four-chamber views, and Electronic Health Records. The experimental analysis on a large cohort of $1346$ subjects who underwent the RHC procedure for PAWP estimation indicates that the proposed pipeline has a diagnostic value and can produce promising performance with significant improvement over the baseline in clinical practice (i.e., $\\Delta$AUC $=0.10$, $\\Delta$Accuracy $=0.06$, and $\\Delta$MCC $=0.39$). The decision curve analysis further confirms the clinical utility of our method.","link":"http://arxiv.org/abs/2303.07540v1","created":"2023-03-14","tags":["arxiv","data-quality"],"meta":{"query":"data AND quality"}}
