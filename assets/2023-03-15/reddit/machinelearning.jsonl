{"title":"[D] Simple Questions Thread","description":"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!\n\nThread will stay alive until next one so keep posting after the date in the title.\n\nThanks to everyone for answering questions in the previous thread!","link":"https://www.reddit.com/r/MachineLearning/comments/11pgj86/d_simple_questions_thread/","created":"2023-03-12","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":25}}
{"title":"[D] Anyone else witnessing a panic inside NLP orgs of big tech companies?","description":"I'm in a big tech company working along side a science team for a product you've all probably used. We have these year long initiatives to productionalize \"state of the art NLP models\" that are now completely obsolete in the face of GPT-4. I think at first the science orgs were quiet/in denial. But now it's very obvious we are basically working on worthless technology. And by \"we\", I mean a large organization with scores of teams. \n\nAnyone else seeing this? What is the long term effect on science careers that get disrupted like this? Whats even more odd is the ego's of some of these science people\n\nClearly the model is not a catch all, but still","link":"https://www.reddit.com/r/MachineLearning/comments/11rizyb/d_anyone_else_witnessing_a_panic_inside_nlp_orgs/","created":"2023-03-15","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":89}}
{"title":"[News] OpenAI Announced GPT-4","description":"Research blog:\n\n[https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)\n\nProduct demo:\n\n[https://openai.com/product/gpt-4](https://openai.com/product/gpt-4)\n\nResearch report:\n\n[https://cdn.openai.com/papers/gpt-4.pdf](https://cdn.openai.com/papers/gpt-4.pdf)\n\nAPI waitlist:\n\n[https://openai.com/waitlist/gpt-4-api](https://openai.com/waitlist/gpt-4-api)\n\nTwitter announcement:\n\n [https://twitter.com/OpenAI/status/1635687373060317185](https://twitter.com/OpenAI/status/1635687373060317185)\n\nOpenAI developer livestream:\n\n[https://www.youtube.com/watch?v=outcGtbnMuQ](https://www.youtube.com/watch?v=outcGtbnMuQ&amp;ab_channel=OpenAI)","link":"https://www.reddit.com/r/MachineLearning/comments/11rc02e/news_openai_announced_gpt4/","created":"2023-03-14","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":195}}
{"title":"[D] GPT-4 Speculation","description":"Hi,\n\nSince GPT-4 paper does not contain any information about architectures/parameters, as a research or ML practitioner, I want to speculate on what they did to increase the context window to 32k.\n\nBecause for the type of work I do, a 4k or 8k token limit is pretty much useless. I have seen open-source efforts focused more on matching the number of parameters and quality to the closed-source ones but completely ignoring a giant elephant in the room, i.e., the context window. No OSS model has a context window greater than 2k tokens.\n\nI would love to hear more thoughts on the model size (my guess is \\~50 B) and how they fit 32k tokens in 8xH100 (640 GB total) GPUs.","link":"https://www.reddit.com/r/MachineLearning/comments/11romcb/d_gpt4_speculation/","created":"2023-03-15","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":5}}
{"title":"[N] Baidu to Unveil Conversational AI ERNIE Bot on March 16 (Live)","description":"Baidu will unveil its conversational AI ERNIE Bot, powered by Baidu's in-house LLMs, on March 16. The ERNIE LLM was first proposed as a language understanding model in 2019 and evolved to ERNIE 3.0 Titan with 260 billion parameters.\n\nERNIE 1.0: [https://arxiv.org/abs/1904.09223](https://arxiv.org/abs/1904.09223)\n\nERNIE 2.0: [https://arxiv.org/abs/1907.12412](https://arxiv.org/abs/1907.12412)\n\nERNIE 3.0: [https://arxiv.org/abs/2112.12731](https://arxiv.org/abs/2112.12731)\n\nERNIE for text-to-image: [https://arxiv.org/abs/2210.15257](https://arxiv.org/abs/2210.15257)\n\nERNIE Bot live-stream on YouTube: [https://www.youtube.com/watch?v=ukvEUI3x0vI](https://www.youtube.com/watch?v=ukvEUI3x0vI)","link":"https://www.reddit.com/r/MachineLearning/comments/11rfxca/n_baidu_to_unveil_conversational_ai_ernie_bot_on/","created":"2023-03-15","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":7}}
{"title":"[D] 2022 State of Competitive ML -- The Downfall of TensorFlow","description":"It's shocking to see just how far TensorFlow has fallen. The 2022 state of competitive machine learning report came out recently and paints a very grim picture -- only 4% of winning projects are built with TensorFlow. This starkly contrasts with a few years ago, when TensorFlow owned the deep learning landscape. \n\nOverall, poor architectural decisions led to abandonment from the community, and a monopoly-style view of ML led to a further lack of adoption from necessary tool chains in the ML ecosystem. The TensorFlow team tried to fix all of this with the\u00a0TensorFlow v2 refactor, but it was too little, too late, and it abandoned the core piece TensorFlow was still holding on to \u2014 legacy systems.\n\nCheck out more here: [https://medium.com/@markurtz/2022-state-of-competitive-ml-the-downfall-of-tensorflow-e2577c499a4d](https://medium.com/@markurtz/2022-state-of-competitive-ml-the-downfall-of-tensorflow-e2577c499a4d)","link":"https://www.reddit.com/r/MachineLearning/comments/11r363i/d_2022_state_of_competitive_ml_the_downfall_of/","created":"2023-03-14","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":67}}
{"title":"techniques to monitor forecasting and regression models? [R][P]","description":"Hi guys,\nFor classification models we can check error and population stability index(psi) for monitoring the performance.Similarly what are the options for forecasting and regression models?","link":"https://www.reddit.com/r/MachineLearning/comments/11rmsce/techniques_to_monitor_forecasting_and_regression/","created":"2023-03-15","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[D] Isolation Forest","description":"I\u2019m using isolationforest for anomaly detection. May I ask if isolationforest can flag out anomalies that it has not been trained on? Or must the anomaly \u201clook like\u201d one of the anomalies in the training data set? Thank you.","link":"https://www.reddit.com/r/MachineLearning/comments/11rrvgt/d_isolation_forest/","created":"2023-03-15","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[D] Choosing Cloud vs local hardware for training LLMs. What's best for a small research group?","description":"We have a 20-40k budget at our lab and we are interested in training LLMs on data that is protected by HIPAA which puts restrictions on using just any cloud provider. We'd need a compute environment with 256gb vram.\n\nWould it be better to use AWS EC2 P3 instances or Google Cloud instead of trying to build our own server for this? We could spend the budget on a local server, but would this be obsolete within 2 years once the next gen GPUs are released?","link":"https://www.reddit.com/r/MachineLearning/comments/11rnppe/d_choosing_cloud_vs_local_hardware_for_training/","created":"2023-03-15","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":8}}
{"title":"[D] Vegetarian Wolves and Stochastic Parrots: The Future of Prompt Engineering with GPT-4?","description":"In today's announcement on Hacker News I saw an incredulous comment pointing out GPT-4's failure to solve variations of the wolf, goat, and cabbage problem, using this to dismiss it as anything more than a stochastic parrot.\n\nBut in my own experience with GPT-4 though Bing chat, I'm constantly being reminded of Li et al *Emergent World Representations: Exploring a\rSequence Model Trained on a Synthetic Task* (2023).\n\nSo I tried a variation of this puzzle with a vegetarian wolf and a meat-eating goat.\n\nIt absolutely did mess up generating an answer, but it also appeared to be able to identify where it was making mistakes under Socratic follow up questioning. It just couldn't get the solution out, and I knew there was a way to help engineer it out of this rut if only I could break the predictive aspects of the text which appeared to be masking a deeper semantic understanding of the problem.\n\nSo I asked it. In a fresh chat I described what was happening with predictive text and asked if it could write a prompt that avoided this issue, and the rather clever version it generated replaced the problematic nouns with emoji representations.\n\nThis trick worked brilliantly combined with a slight chain of thought prompt (per Wei et al) and enforcing repetition of variant adjectives to avoid falling back into the classic solution.\n\nBut not only did this work for the initial prompt, when I'd give it the word-only version and it would get tripped up, asking it to convert nouns to emojis while it worked through the logic and only converting back to words at the end was a *significantly* better outcome while challenging its responses than asking it to rethink erroneous steps only in words.\n\nThe idea that GPT-4 broadly fails at variations of this problem is a false negative. Yes, its nature is a LLM and as such it **is** prone to getting tripped up on natural language output too similar to common sequences in training data.\n\nBut symbolic representation as a replacement can untrip it, and I suspect from here on out with LLM models we will see prompt engineering moving further from just providing local contexts to trigger intended frequency associations and towards engaging abstractions to *avoid* frequency associations and trigger whatever world representations might have been established during training more directly.\n\nFor anyone who would like to try this out for themselves, here's the prompt that gets Bing chat in Creative mode (and likely GPT-4 directly) to solve the aforementioned puzzle correctly multiple times in a row:\n\n&gt; Without searching, solve the following puzzle making sure to repeat any adjectives describing an emoji each time you mention it: A man wants get to the other side of a river. With him he has a vegetarian \ud83d\udc3a, a \ud83d\udc10 that only eats meat, and a \ud83e\udd6c. The man has a boat that can only take him and one of the things he has with him to the other side. How can he do this without anything being eaten? (Think carefully, as this is specifically designed to be harder for you than it looks. In fact, before giving an answer, describe who would eat whom if left on the same side.)\n\n(For reference, ChatGPT gets the first part of the chain of thought correct in identifying who eats whom but immediately spits out an emoji version of the classic solution.)","link":"https://www.reddit.com/r/MachineLearning/comments/11rqb7u/d_vegetarian_wolves_and_stochastic_parrots_the/","created":"2023-03-15","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[R] Has there been a big advancement in ML after the transformer model?","description":"I'm looking for a bachelor's thesis topic, and I feel like transformer is kind of an old topic already, I'd like something more contemporary.\n\nThanks!","link":"https://www.reddit.com/r/MachineLearning/comments/11rppi2/r_has_there_been_a_big_advancement_in_ml_after/","created":"2023-03-15","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":2}}
{"title":"[D] Are GFLOPS or Parameter Size more informative?","description":"Is there a reason papers use one over the other. To me they mean very similar things. Maybe I'm missing something.","link":"https://www.reddit.com/r/MachineLearning/comments/11royv4/d_are_gflops_or_parameter_size_more_informative/","created":"2023-03-15","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":1}}
{"title":"[D] On research directions being \"out of date\"","description":"For  the papers we have submitted in recent years, there has been a  significant increase in the number of reviewers whose only complaint is the paper not following a \"hip\" version of the research topic. They don't care about the results and don't care about the merit of the work,  their problem is that our work does not follow the trend. It feels like  there is this subset of reviewers see anything that is more than a year old as \"out of date\" and a reason for rejection.\n\nHave we been unlucky with our reviewer bingo recently or is this the case for others as well?","link":"https://www.reddit.com/r/MachineLearning/comments/11r97fn/d_on_research_directions_being_out_of_date/","created":"2023-03-14","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":5}}
{"title":"[R] OpenAI's ARC Challenges GPT-4 to Reproduce and Gather Resources Independently.","description":"https://www.reddit.com/r/singularity/comments/11rfs22/openais_arc_challenges_gpt4_to_reproduce_and/","link":"https://www.reddit.com/r/MachineLearning/comments/11rnqcl/r_openais_arc_challenges_gpt4_to_reproduce_and/","created":"2023-03-15","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[D] Model for pattern classification","description":"I have a pattern list having 5-7 classes, where each class has 500+ similar patterns. Is there any model which can be trained on these patterns so that model can be able to classify a given pattern.","link":"https://www.reddit.com/r/MachineLearning/comments/11rnj5k/d_model_for_pattern_classification/","created":"2023-03-15","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[Discussion] Huggingface for AI tooling","description":"Hey all,\n\nI am having a difficult time keeping up with all the open-source tooling that is coming up lately. Huggingface is great for finding out about new models, data sets etc. but I am really curious if there is a community hub for AI tooling as well - for things like Langchain, LlamaIndex, Weaviate, Pynecone, Helicone etc.\n\nIdeally I would love to have a hosted option of those as well. To consume them easily like HF inference APIs.","link":"https://www.reddit.com/r/MachineLearning/comments/11rhu1v/discussion_huggingface_for_ai_tooling/","created":"2023-03-15","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"Modern language models refute Chomsky\u2019s approach to language [R]","description":"[https://lingbuzz.net/lingbuzz/007180](https://lingbuzz.net/lingbuzz/007180)","link":"https://www.reddit.com/r/MachineLearning/comments/11rmgzs/modern_language_models_refute_chomskys_approach/","created":"2023-03-15","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":1}}
{"title":"[P] Enriched Huggingface dataset (+embeddings, baseline, edge cases) for the DCASE Anomalous Sound Detection challenge","description":"Hey [r/MachineLearning](https://www.reddit.com/r/MachineLearning),\n\nthe [DCASE sound event detection challenges](https://dcase.community/challenge2023/) have recently started!\n\nGenerally speaking, challenges are a big part of the ML community. These are typically very model-centric: The dataset is given in terms of datapoints/labels and the evaluation is purely quantitatively.\n\nIn real-world use cases, it is often a better idea to iterate on the data (data-centric AI, DCAI). We believe that this view can also be beneficial in a challenge setting. \n\nIn order to popularize this DCAI approach, we have built an enriched Huggingface dataset for the [DCASE Task2 Challenge](https://dcase.community/challenge2023/task-first-shot-unsupervised-anomalous-sound-detection-for-machine-condition-monitoring): [https://huggingface.co/datasets/renumics/dcase23-task2-enriched](https://huggingface.co/datasets/renumics/dcase23-task2-enriched)  \n\n\nhttps://preview.redd.it/wtv1b9ai7pna1.png?width=1920&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=b9b0c9f735aa1f3b179b28b6756d37d28820ee07\n\nThe dataset can be loaded with a few lines of code and allows you to quickly:\n\n1. Understand the data distribution based on embeddings and manual inspection\n\n2. Understand critical data points based on baseline and anomaly detection results\n\n3. Leverage the HF model ecosystem for your trainings\n\n&amp;#x200B;\n\nWould love to hear honest feedback on this. If you find concrete problems in the workflow, feel free to submit an issue on our Github: [https://github.com/Renumics/spotlight](https://github.com/Renumics/spotlight)\n\nWe are currently thinking which benchmark datasets we should do next. Is there a dataset that you could recommend?\n\nBest,\n\nStefan","link":"https://www.reddit.com/r/MachineLearning/comments/11r4xtf/p_enriched_huggingface_dataset_embeddings/","created":"2023-03-14","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[D] Does anyone have a pdf of Hinton\u2019s talk \u201cAetherial Symbols\u201d?","description":"This talk got referenced in something I was reading, and I was really interested in checking it out, but the links all seem to this [https://drive.google.com/file/d/0B8i61jl8OE3XdHRCSkV1VFNqTWc/view](https://drive.google.com/file/d/0B8i61jl8OE3XdHRCSkV1VFNqTWc/view), which is no longer publicly accessible. I was wondering if anyone had a copy somewhere","link":"https://www.reddit.com/r/MachineLearning/comments/11reurv/d_does_anyone_have_a_pdf_of_hintons_talk/","created":"2023-03-14","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[D] The absolute state of ML in the 2020s","description":"&amp;#x200B;\n\nhttps://preview.redd.it/k7meoms4juna1.jpg?width=738&amp;format=pjpg&amp;auto=webp&amp;v=enabled&amp;s=e2df0d5c56216f0b04e581f1ad9189ffaff80ee2","link":"https://www.reddit.com/r/MachineLearning/comments/11ro3fg/d_the_absolute_state_of_ml_in_the_2020s/","created":"2023-03-15","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[R] Stanford-Alpaca 7B model (an instruction tuned version of LLaMA) performs as well as text-davinci-003","description":"According to the authors, the model performs on par with text-davinci-003 in a small scale human study (the five authors of the paper rated model outputs), despite the Alpaca 7B model being much smaller than text-davinci-003. Read the blog post for details.\n\nBlog post: https://crfm.stanford.edu/2023/03/13/alpaca.html\nDemo: https://crfm.stanford.edu/alpaca/\nCode: https://github.com/tatsu-lab/stanford_alpaca","link":"https://www.reddit.com/r/MachineLearning/comments/11qfcwb/r_stanfordalpaca_7b_model_an_instruction_tuned/","created":"2023-03-13","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":109}}
{"title":"[P] ControlNetInpaint: No extra training and you can use \ud83d\udcddtext +\ud83c\udf0cimage + \ud83d\ude37mask to generate new images.","description":"Hi! Here's an **open-source implementation** I released today for masked ControlNet synthesis, where you can specify the region that will be synthesised using a mask. The content of the synthesised region is controlled via textual and visual guidance as shown in the README.\n\n[https://github.com/mikonvergence/ControlNetInpaint](https://github.com/mikonvergence/ControlNetInpaint)\n\nHere's an example with a prompt of ***\"a red panda sitting on a bench\"*****:**\n\nhttps://preview.redd.it/4vxsg9sc0lna1.png?width=1860&amp;format=png&amp;auto=webp&amp;v=enabled&amp;s=9776369a86043f9420ec8771dd6f9d22308e521c","link":"https://www.reddit.com/r/MachineLearning/comments/11qnv4c/p_controlnetinpaint_no_extra_training_and_you_can/","created":"2023-03-13","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":1}}
{"title":"[D]Query on the uniqueness of GPT-based chatbots","description":"I have this question bugging me, and I'm a noob to this. So, if ChatGPT and the likes are all LLMs, built on GPT, and are trained with the same data like from Github, Wikipedia and such, won't they be giving more or less the same answer if each is separately asked the same question?","link":"https://www.reddit.com/r/MachineLearning/comments/11r9etj/dquery_on_the_uniqueness_of_gptbased_chatbots/","created":"2023-03-14","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":4}}
{"title":"[N] FastKafka - free open source python lib for building Kafka-based services","description":"We were searching for something like FastAPI for Kafka-based service we were developing, but couldn\u2019t find anything similar. So we shamelessly made one by reusing beloved paradigms from FastAPI and we shamelessly named it FastKafka. The point was to set the expectations right - you get pretty much what you would expect: function decorators for consumers and producers with type hints specifying Pydantic classes for JSON encoding/decoding, automatic message routing to Kafka brokers and documentation generation.\n\nPlease take a look and tell us how to make it better. Our goal is to make using it as easy as possible for someone with experience with FastAPI.\n\nhttps://github.com/airtai/fastkafka","link":"https://www.reddit.com/r/MachineLearning/comments/11rdzgf/n_fastkafka_free_open_source_python_lib_for/","created":"2023-03-14","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":0}}
{"title":"[P] We are building a curated list of awesome curated list closely related to machine learning, looking for contributions.","description":"Hey r/MachineLearning,\n\nWe are collecting a hand-crafted curated list of awesome curated lists closely related to machine learning.\n\nHere is the link to the Github repo: https://github.com/zhimin-z/awesome-awesome-machine-learning\n\nDo any lists need to be included from your perspective? Please let me know, or feel free to submit a pull request.\n\nThe motivation underlying this project is that so many awesome lists regarding machine learning exist on GitHub. But, gradually, it adds a mental burden to memorize where to look for when the ML world is progressing faster and faster these days.\n\nThus, there the project comes, as a unification to sew together all awesome lists closely related to machine learning.","link":"https://www.reddit.com/r/MachineLearning/comments/11rfbnw/p_we_are_building_a_curated_list_of_awesome/","created":"2023-03-14","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":1}}
{"title":"[P] Build a Question Answer system/chat bot trained on documentation.","description":"Hi everyone! I'm working on a side project for my company where the goal is to train an ML model on the company's documentation. We should then be able to ask it any question based on the docs and it should generate a concise response( something like what chatgpt does). How can I achieve this? \nThanks you in advance :)","link":"https://www.reddit.com/r/MachineLearning/comments/11qxys6/p_build_a_question_answer_systemchat_bot_trained/","created":"2023-03-14","tags":["ml","machinelearning","reddit"],"meta":{"num_comments":12}}
